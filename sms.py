"""Google Voice SMS Takeout XML Converter.

This script converts Google Voice HTML export files into a standard SMS backup XML format
that can be imported into various SMS backup applications.

The script processes:
- Individual SMS conversations
- Group MMS conversations
- Images and vCard attachments
- Location sharing pins

Author: [Your Name]
Date: [Date]
"""

import argparse
import glob
import inspect
import logging
import hashlib
import mmap
import threading
import os
import re
import sys
import time
from base64 import b64encode
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from datetime import datetime, timedelta
from functools import lru_cache
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional, Tuple, Union
import dateutil.parser
import phonenumbers
from bs4 import BeautifulSoup
from templates import (
    format_index_template, 
    format_conversation_template, 
    SMS_XML_TEMPLATE,
    MMS_XML_TEMPLATE,
    TEXT_PART_TEMPLATE,
    PARTICIPANT_TEMPLATE,
    IMAGE_PART_TEMPLATE,
    VCARD_PART_TEMPLATE
)

# Import new modular components
from conversation_manager import ConversationManager
from phone_lookup import PhoneLookupManager
from sms_processor import SMSProcessor
from utils import (
    is_valid_phone_number, normalize_phone_number, parse_timestamp_from_filename,
    validate_date_range, is_date_in_range, get_memory_usage, log_memory_usage
)
from config import (
    get_config, validate_config, get_output_directory, get_attachments_directory,
    get_index_file_path, DEFAULT_CONFIG
)

# ====================================================================
# CONFIGURATION CONSTANTS
# ====================================================================

# Default file paths and names (can be overridden by command line arguments)
DEFAULT_LOG_FILENAME = "gvoice_converter.log"

# Global variables for paths (set by command line arguments)
PROCESSING_DIRECTORY = Path(
    "../gvoice-convert/"
).resolve()  # Default to ../gvoice-convert/ directory
OUTPUT_DIRECTORY = None  # Will be set based on processing directory
LOG_FILENAME = None  # Will be set based on processing directory

# Global conversation manager
CONVERSATION_MANAGER = None

# Global phone lookup manager
PHONE_LOOKUP_MANAGER = None

# Global filtering configuration
INCLUDE_SERVICE_CODES = False  # Default: filter out service codes
DATE_FILTER_OLDER_THAN = None  # Filter out messages older than this date
DATE_FILTER_NEWER_THAN = None  # Filter out messages newer than this date
FILTER_NUMBERS_WITHOUT_ALIASES = False  # Filter out numbers without aliases
FILTER_NON_PHONE_NUMBERS = False  # Filter out non-phone numbers like shortcodes

# Progress logging configuration
PROGRESS_INTERVAL_PERCENT = 25  # Report progress every 25%
PROGRESS_INTERVAL_COUNT = 50  # Report every N items (used in some loops)
ENABLE_PROGRESS_LOGGING = True
MIN_PROGRESS_INTERVAL = 100

# Performance monitoring configuration
ENABLE_PERFORMANCE_MONITORING = True
PERFORMANCE_LOG_INTERVAL = 1000  # milliseconds or operation count, depending on usage

# Cache size configuration for large datasets (50,000+ SMS, 10,000+ attachments)
# These sizes are optimized for processing large volumes of data efficiently

# Test mode configuration


def set_test_mode(enabled: bool, limit: int = 100):
    """Set test mode configuration."""
    global TEST_MODE, TEST_LIMIT
    TEST_MODE = enabled
    TEST_LIMIT = limit


TEST_MODE = False
TEST_LIMIT = 100

# Performance configuration - Optimized for large datasets by default
ENABLE_BATCH_PROCESSING = True
LARGE_DATASET_THRESHOLD = 5000  # Files (increased for better large dataset handling)
BATCH_SIZE_OPTIMAL = 1000  # Files per batch (increased for better efficiency)
BUFFER_SIZE_OPTIMAL = 32768  # Bytes (32KB - increased for better I/O performance)

# Advanced performance configuration for large datasets (50,000+ files) - High performance defaults
ENABLE_PARALLEL_PROCESSING = True
MAX_WORKERS = min(16, os.cpu_count() or 8)  # Increased to 16 workers max for better parallelization
CHUNK_SIZE_OPTIMAL = 1000  # Files per chunk for parallel processing (optimized for large datasets)
MEMORY_EFFICIENT_THRESHOLD = 10000  # Increased threshold for memory-efficient mode
ENABLE_STREAMING_PARSING = True  # Use streaming for very large files
STREAMING_CHUNK_SIZE = 2 * 1024 * 1024  # 2MB chunks for streaming (increased for better performance)

# File I/O optimization - High performance defaults
FILE_READ_BUFFER_SIZE = 262144  # 256KB buffer for file reading (doubled for better performance)
# Memory mapping enabled by default for better performance on large files
ENABLE_MMAP_FOR_LARGE_FILES = True  # Use memory mapping for files > 5MB
MMAP_THRESHOLD = 5 * 1024 * 1024  # 5MB threshold for mmap

# ====================================================================
# STRICT PARAMETER VALIDATION
# ====================================================================


def validate_function_call(
    func: Callable, args: tuple, kwargs: dict, caller_info: str = ""
) -> None:
    """
    Strict parameter validation for function calls.

    Args:
        func: Function being called
        args: Positional arguments
        kwargs: Keyword arguments
        caller_info: Information about the calling context

    Raises:
        TypeError: If parameter count or types don't match
        ValueError: If required parameters are missing
    """
    try:
        # Get function signature
        sig = inspect.signature(func)

        # Bind arguments to signature
        bound_args = sig.bind(*args, **kwargs)
        bound_args.apply_defaults()

        # Validate parameter types if type hints are available
        for param_name, param_value in bound_args.arguments.items():
            if param_name in sig.parameters:
                param = sig.parameters[param_name]
                if param.annotation != inspect.Parameter.empty:
                    expected_type = param.annotation
                    if expected_type == Union[str, int] and not isinstance(
                        param_value, (str, int)
                    ):
                        raise TypeError(
                            f"Parameter '{param_name}' must be str or int, got {type(param_value).__name__}"
                        )
                    elif expected_type != Any and not isinstance(
                        param_value, expected_type
                    ):
                        raise TypeError(
                            f"Parameter '{param_name}' must be {expected_type.__name__}, got {type(param_value).__name__}"
                        )

        logger.debug(
            f"‚úÖ Function call validated: {func.__name__}({len(args)} args, {len(kwargs)} kwargs) {caller_info}"
        )

    except TypeError as e:
        error_msg = f"Parameter validation failed for {func.__name__}: {e}"
        if caller_info:
            error_msg += f" (called from: {caller_info})"
        logger.error(error_msg)
        raise
    except Exception as e:
        logger.error(
            f"Unexpected error in parameter validation for {func.__name__}: {e}"
        )
        raise


def strict_call(func: Callable, *args, **kwargs) -> Any:
    """
    Wrapper for strict function calls with parameter validation.

    Args:
        func: Function to call
        *args: Positional arguments
        **kwargs: Keyword arguments

    Returns:
        Function result

    Raises:
        TypeError: If parameter validation fails
    """
    # Get caller information for debugging
    caller_frame = inspect.currentframe().f_back
    caller_info = ""
    if caller_frame:
        caller_info = f"{caller_frame.f_code.co_name}:{caller_frame.f_lineno}"

    validate_function_call(func, args, kwargs, caller_info)
    return func(*args, **kwargs)


# ====================================================================
# ENHANCED LOGGING CONFIGURATION
# ====================================================================

# Configure logger reference only; configure handlers/levels in __main__
logger = logging.getLogger(__name__)

# Supported file extensions
SUPPORTED_IMAGE_TYPES = {".jpg", ".jpeg", ".png", ".gif"}
SUPPORTED_VCARD_TYPES = {".vcf"}
SUPPORTED_EXTENSIONS = SUPPORTED_IMAGE_TYPES | SUPPORTED_VCARD_TYPES

# MMS message type constants
MMS_TYPE_SENT = 128
MMS_TYPE_RECEIVED = 132

# Message box constants
MESSAGE_BOX_SENT = 2
MESSAGE_BOX_RECEIVED = 1

# Participant type codes for MMS
PARTICIPANT_TYPE_SENDER = 137
PARTICIPANT_TYPE_RECEIVER = 151

# Error messages
ERROR_NO_MESSAGES = "No messages found in HTML file"
ERROR_NO_PARTICIPANTS = "Could not find participant phone number"
ERROR_NO_SENDER = "Unable to determine sender in MMS with multiple participants"

# Default values and thresholds
DEFAULT_FALLBACK_TIME = 1000  # milliseconds
MIN_PHONE_NUMBER_LENGTH = 7
FILENAME_TRUNCATE_LENGTH = 50

# MMS placeholder messages
MMS_PLACEHOLDER_MESSAGES = {"MMS Sent", "MMS Received"}

# HTML parsing constants
HTML_PARSER = "html.parser"
GROUP_CONVERSATION_MARKER = "Group Conversation"

# Pre-compiled regex patterns for performance
FILENAME_PATTERN = re.compile(r"(?:\((\d+)\))?\.(jpg|gif|png|vcf)$")
CUSTOM_SORT_PATTERN = re.compile(r"(.*?)(?:\((\d+)\))?(\.\w+)?$")
PHONE_NUMBER_PATTERN = re.compile(r"(\+\d{1,3}\s*\d{1,14})")
TEL_HREF_PATTERN = re.compile(r"tel:([+\d\s\-\(\)]+)")

# Additional pre-compiled patterns for better performance
TEXT_TAG_PATTERN = re.compile(r"<text>([^<]*)</text>")
CONTENT_BETWEEN_TAGS_PATTERN = re.compile(r">([^<]+)<")
IMG_TAG_PATTERN = re.compile(r"<img")
VCARD_TAG_PATTERN = re.compile(r'<a[^>]*class=[\'"]vcard[\'"][^>]*>')
AUDIO_TAG_PATTERN = re.compile(r"<audio")
VIDEO_TAG_PATTERN = re.compile(r"<video")

# String translation tables for optimized text processing
HTML_TO_XML_TRANSLATION = str.maketrans(
    {"<": "&lt;", ">": "&gt;", "'": "&apos;", '"': "&quot;"}
)

# Common XML attribute values for string pooling
XML_ATTRIBUTES = {
    'read="1"': 'read="1"',
    'status="1"': 'status="1"',
    'locked="0"': 'locked="0"',
    'type="1"': 'type="1"',
    'type="2"': 'type="2"',
    'm_type="128"': 'm_type="128"',
    'm_type="132"': 'm_type="132"',
    'msg_box="1"': 'msg_box="1"',
    'msg_box="2"': 'msg_box="2"',
    'text_only="0"': 'text_only="0"',
    'text_only="1"': 'text_only="1"',
}

# Legacy functions for backward compatibility with tests


def get_pooled_string(text: str) -> str:
    """Legacy function for backward compatibility with tests."""
    return text


def clear_string_pool() -> None:
    """Legacy function for backward compatibility with tests."""
    pass


def log_performance(
    function_name: str, start_time: float, additional_info: str = ""
) -> None:
    """Legacy function for backward compatibility with tests."""
    pass


class ConversionError(Exception):
    """Custom exception for conversion errors."""

    pass


class FileProcessingError(Exception):
    """Custom exception for file processing errors."""

    pass


class ConfigurationError(Exception):
    """Custom exception for configuration errors."""

    pass


class StringBuilder:
    """Efficient string builder for concatenating multiple strings."""

    def __init__(self, initial_capacity=1024):
        self.parts = []
        self.length = 0
        self.initial_capacity = initial_capacity

    def append(self, text: str):
        """Append text to the builder."""
        if text:
            self.parts.append(text)
            self.length += len(text)

    def append_line(self, text: str = ""):
        """Append text with a newline."""
        self.append(text + "\n")

    def build(self) -> str:
        """Build the final string."""
        if not self.parts:
            return ""
        if len(self.parts) == 1:
            return self.parts[0]
        return "".join(self.parts)

    def clear(self):
        """Clear the builder."""
        self.parts.clear()
        self.length = 0

    def __len__(self):
        return self.length


class ConversationManager:
    """Manages conversation files for different senders/groups."""

    def __init__(
        self,
        output_dir: Path,
        buffer_size: int = 8192,
        batch_size: int = 1000,
        large_dataset: bool = False,
        output_format: str = "xml",
    ):
        # Validate parameters
        if not isinstance(output_dir, Path):
            raise TypeError(
                f"output_dir must be a Path, got {type(output_dir).__name__}"
            )
        if not isinstance(buffer_size, int) or buffer_size <= 0:
            raise ValueError(
                f"buffer_size must be a positive integer, got {buffer_size}"
            )
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError(f"batch_size must be a positive integer, got {batch_size}")
        if not isinstance(output_format, str) or output_format not in ["xml", "html"]:
            raise ValueError(
                f"output_format must be 'xml' or 'html', got {output_format}"
            )

        self.output_dir = output_dir
        # Create output directory if it doesn't exist
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.conversation_files = (
            {}
        )  # Maps conversation_id to file handle and message count
        self.conversation_stats = {}  # Maps conversation_id to statistics
        self.write_buffer_size = (
            buffer_size  # Configurable buffer size for efficient I/O
        )
        self.message_buffer = {}  # Buffer messages before writing to reduce I/O

        # Thread-safety for concurrent writes and file creation
        self._lock = threading.RLock()

        # Pre-allocate common data structures for better performance
        self._empty_list = []
        self._empty_dict = {}

        # Memory-efficient processing for large datasets
        self._batch_size = batch_size if batch_size > 0 else 1000
        self._memory_threshold = (
            100000 if large_dataset else 50000
        )  # Adjust threshold based on dataset size

        self.output_format = output_format

    def get_conversation_id(
        self, participants: List[str], is_group: bool = False
    ) -> str:
        """Generate a unique conversation ID for participants."""
        if is_group:
            # For group conversations, create a filename with concatenated aliases
            if not participants:
                return "group_unknown"

            # Get aliases for all participants
            participant_aliases = []
            for phone in participants:
                # Use the phone lookup manager if available, otherwise use phone number
                try:
                    if PHONE_LOOKUP_MANAGER:
                        alias = PHONE_LOOKUP_MANAGER.get_alias(phone, None)
                    else:
                        alias = phone
                except (AttributeError, NameError):
                    alias = phone
                participant_aliases.append(alias)

            # Remove duplicates while preserving order
            unique_aliases = []
            seen = set()
            for alias in participant_aliases:
                if alias not in seen:
                    unique_aliases.append(alias)
                    seen.add(alias)

            if len(unique_aliases) <= 6:
                # For 6 or fewer participants, use full aliases
                conversation_id = "_".join(unique_aliases)
            else:
                # For more than 6 participants, truncate aliases and add stable hash
                # Take first 5 aliases and truncate them before first underscore
                truncated_aliases = []
                for alias in unique_aliases[:5]:
                    # Truncate before first underscore if it exists
                    truncated = alias.split("_")[0] if "_" in alias else alias
                    truncated_aliases.append(truncated)

                # Create stable hash for remaining participants
                remaining_participants = unique_aliases[5:]
                stable = hashlib.sha1("_".join(remaining_participants).encode("utf-8")).hexdigest()[:6]

                conversation_id = (
                    "_".join(truncated_aliases)
                    + f"_and_{len(remaining_participants)}_more_{stable}"
                )

            # Ensure the filename is not too long (max ~100 characters)
            if len(conversation_id) > 100:
                # Fall back to stable hash-based naming if too long
                participants_str = "_".join(unique_aliases)
                stable = hashlib.sha1(participants_str.encode("utf-8")).hexdigest()[:8]
                conversation_id = f"group_{stable}"

            return conversation_id
        else:
            # For individual conversations, use the alias of the first participant
            if participants:
                phone_number = participants[0]
                # Validate that the phone number is valid
                if not is_valid_phone_number(phone_number):
                    return "unknown"

                # Get alias for the phone number
                try:
                    if PHONE_LOOKUP_MANAGER:
                        alias = PHONE_LOOKUP_MANAGER.get_alias(phone_number, None)
                    else:
                        alias = phone_number
                except (AttributeError, NameError):
                    alias = phone_number
                return alias
            else:
                return "unknown"

    def get_conversation_filename(self, conversation_id: str) -> Path:
        """Get the filename for a conversation."""
        return self.output_dir / f"{conversation_id}.{self.output_format}"

    def write_message(self, conversation_id: str, message_content: str, timestamp: int):
        """Write a message to the conversation file with optimized buffering."""
        with self._lock:
            if conversation_id not in self.conversation_files:
                # Create new conversation file with optimized buffer size
                filename = self.get_conversation_filename(conversation_id)
                file_handle = open(
                    filename, "w", encoding="utf-8", buffering=self.write_buffer_size
                )

                self.conversation_files[conversation_id] = {
                    "file": file_handle,
                    "messages": [],
                    "buffer_size": 0,
                    "max_buffer_size": self.write_buffer_size
                    * 2,  # Allow buffer to grow up to 2x
                }

                # Initialize conversation statistics
                self.conversation_stats[conversation_id] = {
                    "num_sms": 0,
                    "num_img": 0,
                    "num_vcf": 0,
                    "num_calls": 0,
                    "num_voicemails": 0,
                }

            file_info = self.conversation_files[conversation_id]

            # Add message to buffer
            file_info["messages"].append((timestamp, message_content))
            file_info["buffer_size"] += len(message_content)

            # Flush buffer if it gets too large
            if file_info["buffer_size"] > file_info["max_buffer_size"]:
                file_info["file"].flush()
                file_info["buffer_size"] = 0

    def write_message_with_content(
        self,
        conversation_id: str,
        message_text: str,
        attachments: list,
        timestamp: int,
        sender: Optional[str] = None,
    ):
        """Write a message with pre-extracted text and attachment information for HTML output."""
        with self._lock:
            if conversation_id not in self.conversation_files:
                # Create new conversation file with optimized buffer size
                filename = self.get_conversation_filename(conversation_id)
                file_handle = open(
                    filename, "w", encoding="utf-8", buffering=self.write_buffer_size
                )

                self.conversation_files[conversation_id] = {
                    "file": file_handle,
                    "messages": [],
                    "buffer_size": 0,
                    "max_buffer_size": self.write_buffer_size
                    * 2,  # Allow buffer to grow up to 2x
                }

                # Initialize conversation statistics
                self.conversation_stats[conversation_id] = {
                    "num_sms": 0,
                    "num_img": 0,
                    "num_vcf": 0,
                    "num_calls": 0,
                    "num_voicemails": 0,
                }

            file_info = self.conversation_files[conversation_id]

            # Store message with pre-extracted content for HTML output
            message_data = {
                "text": message_text,
                "attachments": attachments,
                "sender": sender or "-",
                "raw_content": None,  # Not needed for HTML output
            }
            file_info["messages"].append((timestamp, message_data))
            file_info["buffer_size"] += len(message_text)

            # Flush buffer if it gets too large
            if file_info["buffer_size"] > file_info["max_buffer_size"]:
                file_info["file"].flush()
                file_info["buffer_size"] = 0

    def finalize_conversation_files(self):
        """Finalize all conversation files by writing headers and closing tags."""
        with self._lock:
            for conversation_id, file_info in self.conversation_files.items():
                try:
                    # Sort messages by timestamp (using tuple unpacking for better performance)
                    sorted_messages = sorted(file_info["messages"], key=lambda x: x[0])

                    if self.output_format == "xml":
                        self._finalize_xml_file(file_info, sorted_messages, conversation_id)
                    elif self.output_format == "html":
                        self._finalize_html_file(
                            file_info, sorted_messages, conversation_id
                        )
                    else:
                        logger.error(f"Unknown output format: {self.output_format}")
                        continue

                except Exception as e:
                    logger.error(
                        f"Failed to finalize conversation file {conversation_id}: {e}"
                    )
                    if "file" in file_info:
                        file_info["file"].close()

            # Clear the conversation files dictionary
            self.conversation_files.clear()

    def generate_index_html(self, stats: Dict[str, int], elapsed_time: float):
        """Generate an index.html file with summary stats and conversation file links."""
        try:
            # Get all conversation files in the output directory
            conversation_files = []
            for file_path in self.output_dir.glob(f"*.{self.output_format}"):
                if file_path.name != "index.html":  # Exclude index.html itself
                    conversation_files.append(file_path)

            # Sort files by name for consistent ordering
            conversation_files.sort(key=lambda x: x.name)

            # Build conversation table rows
            conversation_rows = []

            # Build HTML header
            builder = StringBuilder()
            builder.append_line("<!DOCTYPE html>")
            builder.append_line("<html lang='en'>")
            builder.append_line("<head>")
            builder.append_line("    <meta charset='UTF-8'>")
            builder.append_line(
                "    <meta name='viewport' content='width=device-width, initial-scale=1.0'>"
            )
            builder.append_line("    <title>Google Voice Conversations Index</title>")
            builder.append_line("    <style>")
            builder.append_line(
                "        body { font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }"
            )
            builder.append_line(
                "        .container { max-width: 1200px; margin: 0 auto; background-color: white; padding: 20px; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }"
            )
            builder.append_line(
                "        .header { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; border-radius: 8px; margin-bottom: 30px; text-align: center; }"
            )
            builder.append_line(
                "        .stats-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 20px; margin-bottom: 30px; }"
            )
            builder.append_line(
                "        .stat-card { background-color: #f8f9fa; padding: 20px; border-radius: 8px; text-align: center; border-left: 4px solid #667eea; }"
            )
            builder.append_line(
                "        .stat-number { font-size: 2em; font-weight: bold; color: #667eea; margin-bottom: 5px; }"
            )
            builder.append_line(
                "        .stat-label { color: #666; font-size: 0.9em; }"
            )
            builder.append_line(
                "        .conversations-table { width: 100%; border-collapse: collapse; margin-top: 20px; }"
            )
            builder.append_line(
                "        .conversations-table th, .conversations-table td { padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }"
            )
            builder.append_line(
                "        .conversations-table th { background-color: #667eea; color: white; font-weight: bold; }"
            )
            builder.append_line(
                "        .conversations-table tr:hover { background-color: #f8f9fa; }"
            )
            builder.append_line(
                "        .file-link { color: #667eea; text-decoration: none; font-weight: bold; }"
            )
            builder.append_line(
                "        .file-link:hover { text-decoration: underline; }"
            )
            builder.append_line(
                "        .file-stats { font-size: 0.9em; color: #666; }"
            )
            builder.append_line(
                "        .footer { margin-top: 30px; padding: 20px; text-align: center; color: #666; border-top: 1px solid #eee; }"
            )
            builder.append_line(
                "        .processing-info { background-color: #e8f5e8; padding: 15px; border-radius: 8px; margin-bottom: 20px; border-left: 4px solid #28a745; }"
            )
            builder.append_line("    </style>")
            builder.append_line("</head>")
            builder.append_line("<body>")
            builder.append_line("    <div class='container'>")

            # Build header section
            builder.append_line("        <div class='header'>")
            builder.append_line("            <h1>üì± Google Voice Conversations</h1>")
            builder.append_line(
                "            <p>Complete overview of all processed conversations</p>"
            )
            builder.append_line("        </div>")

            # Build processing info section
            builder.append_line("        <div class='processing-info'>")
            builder.append_line(
                f"            <strong>Processing completed in {elapsed_time:.2f} seconds</strong>"
            )
            builder.append_line(
                f"            <br>Output format: {self.output_format.upper()}"
            )
            builder.append_line(
                f"            <br>Total conversations: {len(conversation_files)}"
            )
            builder.append_line("        </div>")

            # Build summary stats section
            builder.append_line("        <div class='stats-grid'>")
            builder.append_line("            <div class='stat-card'>")
            builder.append_line(
                f"                <div class='stat-number'>{stats.get('num_sms', 0)}</div>"
            )
            builder.append_line(
                "                <div class='stat-label'>SMS Messages</div>"
            )
            builder.append_line("            </div>")
            builder.append_line("            <div class='stat-card'>")
            builder.append_line(
                f"                <div class='stat-number'>{stats.get('num_calls', 0)}</div>"
            )
            builder.append_line(
                "                <div class='stat-label'>Call Logs</div>"
            )
            builder.append_line("            </div>")
            builder.append_line("            <div class='stat-card'>")
            builder.append_line(
                f"                <div class='stat-number'>{stats.get('num_voicemails', 0)}</div>"
            )
            builder.append_line(
                "                <div class='stat-label'>Voicemails</div>"
            )
            builder.append_line("            </div>")
            builder.append_line("            <div class='stat-card'>")
            builder.append_line(
                f"                <div class='stat-number'>{stats.get('num_img', 0)}</div>"
            )
            builder.append_line("                <div class='stat-label'>Images</div>")
            builder.append_line("            </div>")
            builder.append_line("            <div class='stat-card'>")
            builder.append_line(
                f"                <div class='stat-number'>{stats.get('num_vcf', 0)}</div>"
            )
            builder.append_line(
                "                <div class='stat-label'>vCard Contacts</div>"
            )
            builder.append_line("            </div>")
            total_messages = (
                stats.get("num_sms", 0)
                + stats.get("num_calls", 0)
                + stats.get("num_voicemails", 0)
            )
            builder.append_line("            <div class='stat-card'>")
            builder.append_line(
                f"                <div class='stat-number'>{total_messages}</div>"
            )
            builder.append_line(
                "                <div class='stat-label'>Total Items</div>"
            )
            builder.append_line("            </div>")
            builder.append_line("        </div>")

            # Build conversations table
            builder.append_line("        <h2>üìÅ Conversation Files</h2>")
            builder.append_line("        <table class='conversations-table'>")
            builder.append_line("            <thead>")
            builder.append_line("                <tr>")
            builder.append_line("                    <th>Conversation</th>")
            builder.append_line("                    <th>File Type</th>")
            builder.append_line("                    <th>File Size</th>")
            builder.append_line("                    <th>SMS</th>")
            builder.append_line("                    <th>Calls</th>")
            builder.append_line("                    <th>Voicemails</th>")
            builder.append_line("                    <th>Attachments</th>")
            builder.append_line("                    <th>Latest Message</th>")
            builder.append_line("                </tr>")
            builder.append_line("            </thead>")
            builder.append_line("            <tbody>")

            # Build table rows for each conversation file
            for file_path in conversation_files:
                try:
                    # Get file stats
                    stat = file_path.stat()
                    file_size = stat.st_size

                    # Format file size
                    if file_size < 1024:
                        size_str = f"{file_size} B"
                    elif file_size < 1024 * 1024:
                        size_str = f"{file_size / 1024:.1f} KB"
                    else:
                        size_str = f"{file_size / (1024 * 1024):.1f} MB"

                    # Get conversation name (without extension)
                    conversation_name = file_path.stem

                    # Extract conversation statistics and latest message time
                    conv_stats = self._extract_conversation_stats(file_path)

                    # Build table row
                    builder.append_line("                <tr>")
                    builder.append_line(
                        f"                    <td><a href='{file_path.name}' class='file-link'>{conversation_name}</a></td>"
                    )
                    builder.append_line(
                        f"                    <td>{self.output_format.upper()}</td>"
                    )
                    builder.append_line(f"                    <td>{size_str}</td>")
                    builder.append_line(
                        f"                    <td>{conv_stats['sms_count']}</td>"
                    )
                    builder.append_line(
                        f"                    <td>{conv_stats['calls_count']}</td>"
                    )
                    builder.append_line(
                        f"                    <td>{conv_stats['voicemails_count']}</td>"
                    )
                    builder.append_line(
                        f"                    <td>{conv_stats['attachments_count']}</td>"
                    )
                    builder.append_line(
                        f"                    <td>{conv_stats['latest_message_time']}</td>"
                    )
                    builder.append_line("                </tr>")

                except Exception as e:
                    logger.error(f"Failed to get stats for {file_path}: {e}")
                    # Still add the row with basic info
                    conversation_name = file_path.stem
                    builder.append_line("                <tr>")
                    builder.append_line(
                        f"                    <td><a href='{file_path.name}' class='file-link'>{conversation_name}</a></td>"
                    )
                    builder.append_line(
                        f"                    <td>{self.output_format.upper()}</td>"
                    )
                    builder.append_line("                    <td>Unknown</td>")
                    builder.append_line("                    <td>0</td>")
                    builder.append_line("                    <td>0</td>")
                    builder.append_line("                    <td>0</td>")
                    builder.append_line("                    <td>0</td>")
                    builder.append_line("                    <td>Unknown</td>")
                    builder.append_line("                </tr>")

            # Build table footer and HTML footer
            builder.append_line("            </tbody>")
            builder.append_line("        </table>")

            # Build footer
            builder.append_line("        <div class='footer'>")
            builder.append_line(
                "            <p><em>Generated automatically by Google Voice SMS Takeout Converter</em></p>"
            )
            builder.append_line(
                f"            <p>Generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>"
            )
            builder.append_line("        </div>")

            builder.append_line("    </div>")
            builder.append_line("</body>")
            builder.append_line("</html>")

            # Write the index.html file
            index_path = self.output_dir / "index.html"
            with open(index_path, "w", encoding="utf-8") as f:
                f.write(builder.build())

            logger.info(
                f"Generated index.html with {len(conversation_files)} conversation files"
            )

        except Exception as e:
            logger.error(f"Failed to generate index.html: {e}")
            import traceback

            logger.error(f"Traceback: {traceback.format_exc()}")

    def _extract_conversation_stats(
        self, file_path: Path
    ) -> Dict[str, Union[int, str]]:
        """Extract statistics from a conversation file (HTML or XML)."""
        try:
            # Read the file
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()

            # Determine parser based on file extension
            if file_path.suffix.lower() == ".xml":
                # Try to use XML parser for XML files to avoid warnings
                try:
                    soup = BeautifulSoup(content, "xml")
                except Exception:
                    # Fall back to HTML parser if XML parser is not available
                    soup = BeautifulSoup(content, "html.parser")
            else:
                # Use HTML parser for HTML files
                soup = BeautifulSoup(content, "html.parser")

            # Initialize counters
            sms_count = 0
            calls_count = 0
            voicemails_count = 0
            attachments_count = 0
            latest_timestamp = None

            # Find all message rows
            message_rows = soup.find_all("tr")

            for row in message_rows:
                # Check if this row has a message
                message_cell = row.find("td", class_=STRING_POOL.HTML_CLASSES["message"])
                if message_cell:
                    message_text = message_cell.get_text().strip()

                    # Count different message types using string pool
                    if message_text.startswith(STRING_POOL.PATTERNS["call_prefix"]):
                        calls_count += 1
                    elif message_text.startswith(STRING_POOL.PATTERNS["voicemail_prefix"]):
                        voicemails_count += 1
                    elif (
                        message_text
                        and not message_text.startswith(STRING_POOL.PATTERNS["call_prefix"])
                        and not message_text.startswith(STRING_POOL.PATTERNS["voicemail_prefix"])
                    ):
                        sms_count += 1

                    # Check for attachments in the same row
                    attachments_cell = (
                        row.find_all("td")[2] if len(row.find_all("td")) > 2 else None
                    )
                    if attachments_cell:
                        attachments_text = attachments_cell.get_text().strip()
                        if attachments_text != "-" and attachments_text:
                            attachments_count += 1

                    # Get timestamp for latest message
                    timestamp_cell = row.find("td", class_=STRING_POOL.HTML_CLASSES["timestamp"])
                    if timestamp_cell:
                        timestamp_text = timestamp_cell.get_text().strip()
                        if timestamp_text:
                            # Parse timestamp (format: YYYY-MM-DD HH:MM:SS)
                            try:
                                timestamp = datetime.strptime(
                                    timestamp_text, "%Y-%m-%d %H:%M:%S"
                                )
                                if (
                                    latest_timestamp is None
                                    or timestamp > latest_timestamp
                                ):
                                    latest_timestamp = timestamp
                            except ValueError:
                                pass

            # Format latest message time
            if latest_timestamp:
                latest_message_time = latest_timestamp.strftime("%Y-%m-%d %H:%M:%S")
            else:
                latest_message_time = "No messages"

            return {
                "sms_count": sms_count,
                "calls_count": calls_count,
                "voicemails_count": voicemails_count,
                "attachments_count": attachments_count,
                "latest_message_time": latest_message_time,
            }

        except Exception as e:
            logger.error(f"Failed to extract stats from {file_path}: {e}")
            return {
                "sms_count": 0,
                "calls_count": 0,
                "voicemails_count": 0,
                "attachments_count": 0,
                "latest_message_time": "Error",
            }

    def _finalize_xml_file(
        self, file_info: dict, sorted_messages: list, conversation_id: str
    ):
        """Finalize an XML conversation file."""
        # Pre-allocate header lines to avoid repeated string operations
        header_lines = [
            "<?xml version='1.0' encoding='UTF-8' standalone='yes' ?>",
            "<!--Converted from GV Takeout data -->",
            f'<smses count="{len(sorted_messages)}">',
        ]

        # Write header efficiently
        file_info["file"].write("\n".join(header_lines) + "\n")

        # Write sorted messages in optimized batches
        batch_size = self._batch_size
        for i in range(0, len(sorted_messages), batch_size):
            batch = sorted_messages[i:i + batch_size]
            # Use generator expression for memory efficiency
            batch_content = "".join(xml_content for _, xml_content in batch)
            file_info["file"].write(batch_content)

        # Write closing tag
        file_info["file"].write("</smses>")
        file_info["file"].close()

        logger.info(
            f"Finalized XML conversation file: {self.get_conversation_filename(conversation_id)} "
            f"with {len(sorted_messages)} messages"
        )

    def _finalize_html_file(
        self, file_info: dict, sorted_messages: list, conversation_id: str
    ):
        """Finalize an HTML conversation file."""
        # Use StringBuilder for efficient HTML generation
        builder = StringBuilder()

        # Build HTML header efficiently
        builder.append_line("<!DOCTYPE html>")
        builder.append_line("<html lang='en'>")
        builder.append_line("<head>")
        builder.append_line("    <meta charset='UTF-8'>")
        builder.append_line(
            "    <meta name='viewport' content='width=device-width, initial-scale=1.0'>"
        )
        builder.append_line(f"    <title>SMS Conversation - {conversation_id}</title>")
        builder.append_line("    <style>")
        builder.append_line(
            "        body { font-family: Arial, sans-serif; margin: 20px; }"
        )
        builder.append_line(
            "        table { border-collapse: collapse; width: 100%; margin-top: 20px; }"
        )
        builder.append_line(
            "        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }"
        )
        builder.append_line(
            "        th { background-color: #f2f2f2; font-weight: bold; }"
        )
        builder.append_line("        tr:nth-child(even) { background-color: #f9f9f9; }")
        builder.append_line("        .timestamp { font-size: 0.9em; color: #666; }")
        builder.append_line(
            "        .message { max-width: 400px; word-wrap: break-word; }"
        )
        builder.append_line(
            "        .attachment { color: #0066cc; text-decoration: none; }"
        )
        builder.append_line("        .attachment:hover { text-decoration: underline; }")
        builder.append_line(
            "        .header { background-color: #e6f3ff; padding: 15px; border-radius: 5px; margin-bottom: 20px; }"
        )
        builder.append_line("    </style>")
        builder.append_line("</head>")
        builder.append_line("<body>")

        # Build header section
        builder.append_line("    <div class='header'>")
        builder.append_line(f"        <h1>SMS Conversation: {conversation_id}</h1>")
        builder.append_line(f"        <p>Total Messages: {len(sorted_messages)}</p>")
        builder.append_line(
            "        <p><em>Converted from Google Voice Takeout data</em></p>"
        )
        builder.append_line("    </div>")

        # Build table header
        builder.append_line("    <table>")
        builder.append_line("        <thead>")
        builder.append_line("            <tr>")
        builder.append_line("                <th>Timestamp</th>")
        builder.append_line("                <th>Sender</th>")
        builder.append_line("                <th>Message</th>")
        builder.append_line("                <th>Attachments</th>")
        builder.append_line("            </tr>")
        builder.append_line("        </thead>")
        builder.append_line("        <tbody>")

        # Build table rows efficiently
        for timestamp, message_content in sorted_messages:
            # Handle both old format (raw XML content) and new format (pre-extracted content)
            if isinstance(message_content, dict) and "text" in message_content:
                # New format: pre-extracted content
                message_text = message_content["text"]
                # Join attachments and preserve HTML formatting
                if message_content["attachments"]:
                    attachments = " ".join(message_content["attachments"])
                else:
                    attachments = "-"
                sender_display = message_content.get("sender", "-")
            else:
                # Old format: raw XML content - extract message text and attachments
                message_text, attachments = self._extract_message_content(
                    message_content
                )
                # Best-effort sender extraction from raw content
                sender_display = self._extract_sender_from_raw(message_content)

            # Format timestamp
            formatted_time = self._format_timestamp(timestamp)

            # Build table row efficiently
            builder.append_line("            <tr>")
            builder.append_line(
                f"                <td class='timestamp'>{formatted_time}</td>"
            )
            builder.append_line(
                f"                <td class='sender'>{sender_display}</td>"
            )
            builder.append_line(
                f"                <td class='message'>{message_text}</td>"
            )
            builder.append_line(f"                <td>{attachments}</td>")
            builder.append_line("            </tr>")

        # Build HTML footer
        builder.append_line("        </tbody>")
        builder.append_line("    </table>")
        builder.append_line("</body>")
        builder.append_line("</html>")

        # Write the complete HTML content efficiently
        file_info["file"].write(builder.build())
        file_info["file"].close()

        # Clear builder to free memory
        builder.clear()

        logger.info(
            f"Finalized HTML conversation file: {self.get_conversation_filename(conversation_id)} "
            f"with {len(sorted_messages)} messages"
        )

    def _extract_message_content(self, message_content: str) -> tuple[str, str]:
        """Extract message text and attachments from XML content."""
        # Parse the XML content to extract message text and attachments
        try:
            # Simple parsing - look for text content and attachments
            message_text = ""
            attachments = []

            # First try to extract SMS message content from body attribute
            body_match = re.search(r'body="([^"]*)"', message_content)
            if body_match:
                message_text = body_match.group(1)
                # Unescape XML entities
                message_text = (
                    message_text.replace("&apos;", "'")
                    .replace("&quot;", '"')
                    .replace("&lt;", "<")
                    .replace("&gt;", ">")
                    .replace("&amp;", "&")
                )
            else:
                # Try to extract MMS message content from part text
                part_text_match = re.search(r'text="([^"]*)"', message_content)
                if part_text_match:
                    message_text = part_text_match.group(1)
                    # Unescape XML entities
                    message_text = (
                        message_text.replace("&apos;", "'")
                        .replace("&quot;", '"')
                        .replace("&lt;", "<")
                        .replace("&gt;", ">")
                        .replace("&amp;", "&")
                    )
                else:
                    # Fallback: look for content between tags using pre-compiled patterns
                    text_match = TEXT_TAG_PATTERN.search(message_content)
                    if text_match:
                        message_text = text_match.group(1)
                    else:
                        # Last resort: look for content between tags
                        content_match = CONTENT_BETWEEN_TAGS_PATTERN.search(
                            message_content
                        )
                        if content_match:
                            message_text = content_match.group(1)

            # Extract attachments using pre-compiled patterns
            # Check for MMS image parts first (more specific)
            if re.search(r'ct="image/[^"]*"', message_content):
                attachments.append("üì∑ Image")
            # Check for traditional HTML img tags
            elif IMG_TAG_PATTERN.search(message_content):
                attachments.append("üì∑ Image")
            # Check for vCard attachments
            if VCARD_TAG_PATTERN.search(message_content):
                attachments.append("üìá vCard")
            # Check for audio attachments
            if re.search(r'ct="audio/[^"]*"', message_content):
                attachments.append("üéµ Audio")
            elif AUDIO_TAG_PATTERN.search(message_content):
                attachments.append("üéµ Audio")
            # Check for video attachments
            if re.search(r'ct="video/[^"]*"', message_content):
                attachments.append("üé¨ Video")
            elif VIDEO_TAG_PATTERN.search(message_content):
                attachments.append("üé¨ Video")

            # Clean up message text
            message_text = message_text.strip()
            if not message_text:
                message_text = "[No text content]"

            # Join attachments efficiently
            attachments_str = " ".join(attachments) if attachments else "-"

            return message_text, attachments_str

        except Exception as e:
            # Log the specific error and the content that caused it for debugging
            logger.error(f"Failed to extract message content: {e}")
            logger.debug(f"Problematic content: {message_content[:200]}...")
            
            # Try to extract any readable text as a last resort
            try:
                # Strip HTML tags and extract any remaining text
                text_only = re.sub(r'<[^>]+>', '', message_content)
                text_only = re.sub(r'&[^;]+;', ' ', text_only)  # Replace HTML entities
                text_only = re.sub(r'\s+', ' ', text_only).strip()  # Normalize whitespace
                
                if text_only and len(text_only) > 5:
                    return f"[Partial content: {text_only[:100]}{'...' if len(text_only) > 100 else ''}]", "-"
            except Exception:
                pass
            
            return "[Error parsing message]", "-"

    def _extract_sender_from_raw(self, message_content: str) -> str:
        """Best-effort sender extraction from raw XML/HTML content for legacy writes."""
        try:
            # Look for addr entries in MMS XML
            m = re.search(r'<addr[^>]*address="([^"]+)"[^>]*type="137"', message_content)
            if m:
                return m.group(1)
            # Look for tel: links
            m = TEL_HREF_PATTERN.search(message_content)
            if m:
                return m.group(1)
            # Fallback
            return "-"
        except Exception:
            return "-"

    def _format_timestamp(self, timestamp: int) -> str:
        """Format timestamp for display."""
        try:
            from datetime import datetime

            dt = datetime.fromtimestamp(timestamp / 1000)  # Convert from milliseconds
            return dt.strftime("%Y-%m-%d %H:%M:%S")
        except Exception:
            return str(timestamp)

    def get_total_stats(self) -> Dict[str, int]:
        """Get total statistics across all conversations."""
        total_stats = {
            "num_sms": 0,
            "num_img": 0,
            "num_vcf": 0,
            "num_calls": 0,
            "num_voicemails": 0,
        }

        # Count from conversation stats
        for stats in self.conversation_stats.values():
            for key in total_stats:
                total_stats[key] += stats.get(key, 0)

        # Also count from actual message counts if available
        total_messages = 0
        for conversation_id, file_info in self.conversation_files.items():
            if "messages" in file_info:
                total_messages += len(file_info["messages"])

        # If we have message counts but no SMS stats, estimate from messages
        if total_messages > 0 and total_stats["num_sms"] == 0:
            total_stats["num_sms"] = total_messages

        return total_stats

    def update_stats(self, conversation_id: str, stats: Dict[str, int]):
        """Update statistics for a conversation."""
        if conversation_id in self.conversation_stats:
            for key, value in stats.items():
                self.conversation_stats[conversation_id][key] = (
                    self.conversation_stats[conversation_id].get(key, 0) + value
                )


class PhoneLookupManager:
    """Manages phone number to alias mappings with user interaction."""

    def __init__(self, lookup_file: Path, enable_prompts: bool = True):
        # Validate parameters
        if not isinstance(lookup_file, Path):
            raise TypeError(
                f"lookup_file must be a Path, got {type(lookup_file).__name__}"
            )
        if not isinstance(enable_prompts, bool):
            raise TypeError(
                f"enable_prompts must be a boolean, got {type(enable_prompts).__name__}"
            )

        self.lookup_file = lookup_file
        self.enable_prompts = enable_prompts
        self.phone_aliases = {}  # Maps phone numbers to aliases
        self.load_aliases()

    def load_aliases(self):
        """Load existing phone number aliases from file."""
        try:
            if self.lookup_file.exists():
                with open(self.lookup_file, "r", encoding="utf8") as f:
                    for line in f:
                        line = line.strip()
                        if line and not line.startswith("#"):
                            try:
                                phone, alias = line.split("|", 1)
                                self.phone_aliases[phone.strip()] = alias.strip()
                            except ValueError:
                                # Skip malformed lines
                                continue
                logger.info(
                    f"Loaded {len(self.phone_aliases)} phone number aliases from {self.lookup_file}"
                )
            else:
                # Create the file with a header
                self.lookup_file.parent.mkdir(parents=True, exist_ok=True)
                with open(self.lookup_file, "w", encoding="utf8") as f:
                    f.write("# Phone number lookup file\n")
                    f.write("# Format: phone_number|alias\n")
                    f.write("# Lines starting with # are comments\n")
                logger.info(f"Created new phone lookup file: {self.lookup_file}")
        except Exception as e:
            logger.error(f"Failed to load phone aliases: {e}")

    def save_aliases(self):
        """Save phone number aliases to file."""
        try:
            with open(self.lookup_file, "w", encoding="utf8") as f:
                f.write("# Phone number lookup file\n")
                f.write("# Format: phone_number|alias\n")
                f.write("# Lines starting with # are comments\n")
                for phone, alias in sorted(self.phone_aliases.items()):
                    f.write(f"{phone}|{alias}\n")
            logger.info(
                f"Saved {len(self.phone_aliases)} phone number aliases to {self.lookup_file}"
            )
        except Exception as e:
            logger.error(f"Failed to save phone aliases: {e}")
        
    def save_aliases_batched(self, batch_every: int = 100):
        """Save aliases only every N new entries to reduce disk IO."""
        if not hasattr(self, "_unsaved_count"):
            self._unsaved_count = 0
        self._unsaved_count += 1
        if self._unsaved_count >= batch_every:
            self.save_aliases()
            self._unsaved_count = 0

    def sanitize_alias(self, alias: str) -> str:
        """Sanitize alias by removing special characters and replacing spaces with underscores."""
        import re

        # Remove special characters except alphanumeric, spaces, and hyphens
        sanitized = re.sub(r"[^\w\s\-]", "", alias)
        # Replace spaces and hyphens with underscores
        sanitized = re.sub(r"[\s\-]+", "_", sanitized)
        # Remove leading/trailing underscores
        sanitized = sanitized.strip("_")
        # Ensure it's not empty
        if not sanitized:
            sanitized = "unknown"
        return sanitized

    def extract_alias_from_html(
        self, soup: BeautifulSoup, phone_number: str
    ) -> Optional[str]:
        """
        Automatically extract phone alias from HTML content when prompts are disabled.

        Args:
            soup: BeautifulSoup object of the HTML file
            phone_number: Phone number to find alias for

        Returns:
            Optional[str]: Extracted alias or None if not found
        """
        try:
            # Define generic phrases that should not be used as aliases
            generic_phrases = {
                "me",
                "unknown",
                "placed call to",
                "received call from",
                "missed call from",
                "call placed to",
                "call received from",
                "call missed from",
                "voicemail from",
                "voicemail received from",
                "text message from",
                "message from",
            }

            # Look for vCard entries with the phone number
            # Pattern: <a class="tel" href="tel:+1234567890"><span class="fn">Name</span></a>
            tel_links = soup.select(STRING_POOL.ADDITIONAL_SELECTORS["tel_links"])
            for link in tel_links:
                href = link.get("href", "")
                if href.startswith("tel:"):
                    link_phone = href.split(":", 1)[-1]
                    if link_phone == phone_number:
                        # First try to find the name in the fn class
                        fn_element = link.find(["span", "abbr"], class_="fn")
                        if fn_element:
                            name = fn_element.get_text(strip=True)
                            if name and name.lower() not in generic_phrases:
                                return self.sanitize_alias(name)
                        
                        # If no fn class, try to get the name directly from the tel link text
                        # This handles cases like: <a class="tel" href="tel:+1234567890">Name</a>
                        link_text = link.get_text(strip=True)
                        if link_text and link_text.lower() not in generic_phrases:
                            return self.sanitize_alias(link_text)

            # Look for other patterns where phone numbers and names are associated
            # Pattern: <cite class="sender vcard"><a class="tel" href="tel:+1234567890">Name</a></cite>
            cite_elements = soup.find_all("cite", class_=lambda x: x and "sender" in x)
            for cite in cite_elements:
                tel_link = cite.find("a", class_="tel", href=True)
                if tel_link:
                    href = tel_link.get("href", "")
                    if href.startswith("tel:"):
                        link_phone = href.split(":", 1)[-1]
                        if link_phone == phone_number:
                            name = tel_link.get_text(strip=True)
                            if name and name.lower() not in generic_phrases:
                                return self.sanitize_alias(name)

            # Look for general name elements near phone numbers
            # Pattern: <span class="fn">Name</span> or similar
            fn_elements = soup.select(STRING_POOL.ADDITIONAL_SELECTORS["fn_elements"])
            for fn in fn_elements:
                name = fn.get_text(strip=True)
                if name and name.lower() not in generic_phrases:
                    # Check if this name element is near a phone number
                    # Look for tel links in the same container or nearby
                    container = fn.find_parent(["div", "span", "cite"])
                    if container:
                        tel_links = container.select(STRING_POOL.ADDITIONAL_SELECTORS["tel_links"])
                        for tel_link in tel_links:
                            href = tel_link.get("href", "")
                            if href.startswith("tel:"):
                                link_phone = href.split(":", 1)[-1]
                                if link_phone == phone_number:
                                    return self.sanitize_alias(name)

            return None

        except Exception as e:
            logger.debug(f"Failed to extract alias from HTML for {phone_number}: {e}")
            return None

    def get_alias(self, phone_number: str, soup: Optional[BeautifulSoup] = None) -> str:
        """Get alias for a phone number, prompting user if not found and prompts are enabled."""
        if phone_number in self.phone_aliases:
            return self.phone_aliases[phone_number]

        if not self.enable_prompts:
            # Try to automatically extract alias from HTML if provided
            if soup:
                extracted_alias = self.extract_alias_from_html(soup, phone_number)
                if extracted_alias:
                    # Store the automatically extracted alias
                    self.phone_aliases[phone_number] = extracted_alias
                    self.save_aliases_batched()
                    logger.info(
                        f"Automatically extracted alias '{extracted_alias}' for {phone_number}"
                    )
                    return extracted_alias

            return phone_number

        # Prompt user for alias
        try:
            print(f"\nNew phone number found: {phone_number}")
            print(
                "Please provide a name/alias for this number (or press Enter to use the number):"
            )
            alias = input("Alias: ").strip()

            if alias:
                # Sanitize the alias
                sanitized_alias = self.sanitize_alias(alias)
                if sanitized_alias != alias:
                    print(f"Alias sanitized to: {sanitized_alias}")

                # Store the mapping
                self.phone_aliases[phone_number] = sanitized_alias

                # Save batched
                self.save_aliases_batched()

                return sanitized_alias
            else:
                # User chose to use the phone number
                return phone_number

        except (EOFError, KeyboardInterrupt):
            # Handle Ctrl+D or Ctrl+C gracefully
            print("\nUsing phone number as alias.")
            return phone_number
        except Exception as e:
            logger.error(f"Failed to get alias for {phone_number}: {e}")
            return phone_number

    def get_all_aliases(self) -> Dict[str, str]:
        """Get all phone number to alias mappings."""
        return self.phone_aliases.copy()

    def add_alias(self, phone_number: str, alias: str):
        """Manually add a phone number to alias mapping."""
        sanitized_alias = self.sanitize_alias(alias)
        self.phone_aliases[phone_number] = sanitized_alias
        self.save_aliases_batched()
        logger.info(f"Added alias '{sanitized_alias}' for phone number {phone_number}")


@dataclass
class ConversionStats:
    """Statistics for the conversion process."""

    num_sms: int = 0
    num_img: int = 0
    num_vcf: int = 0
    own_number: Optional[str] = None


# XML templates for better maintainability


def validate_configuration():
    """Validate that all required configuration constants are properly set."""
    required_constants = {
        "SUPPORTED_IMAGE_TYPES": SUPPORTED_IMAGE_TYPES,
        "SUPPORTED_VCARD_TYPES": SUPPORTED_VCARD_TYPES,
        "MMS_TYPE_SENT": MMS_TYPE_SENT,
        "MMS_TYPE_RECEIVED": MMS_TYPE_RECEIVED,
        "MESSAGE_BOX_SENT": MESSAGE_BOX_SENT,
        "MESSAGE_BOX_RECEIVED": MESSAGE_BOX_RECEIVED,
        "PARTICIPANT_TYPE_SENDER": PARTICIPANT_TYPE_SENDER,
        "PARTICIPANT_TYPE_RECEIVER": PARTICIPANT_TYPE_RECEIVER,
    }

    for name, value in required_constants.items():
        if value is None:
            raise ConfigurationError(
                f"Required configuration constant {name} is not set"
            )

    logger.info("Configuration validation passed")


def copy_mapped_attachments(src_filename_map: Dict[str, str]) -> None:
    """
    Copy all mapped attachments to the attachments directory with optimized I/O.

    Args:
        src_filename_map: Mapping of src elements to attachment filenames
    """
    logger.info("Starting comprehensive attachment copying...")

    # Get the attachments directory
    attachments_dir = OUTPUT_DIRECTORY / "attachments"
    attachments_dir.mkdir(exist_ok=True, mode=0o755)

    # Get unique filenames from the mapping
    unique_filenames = set()
    for src, filename in src_filename_map.items():
        if filename != "No unused match found":
            unique_filenames.add(filename)

    total_attachments = len(unique_filenames)
    logger.info(f"Found {total_attachments} unique attachments to copy")

    # Use parallel processing for large datasets
    if ENABLE_PARALLEL_PROCESSING and total_attachments > MEMORY_EFFICIENT_THRESHOLD:
        copy_attachments_parallel(unique_filenames, attachments_dir)
    else:
        copy_attachments_sequential(unique_filenames, attachments_dir)


def copy_attachments_sequential(filenames: set, attachments_dir: Path) -> None:
    """Copy attachments sequentially with progress tracking."""
    copied_count = 0
    skipped_count = 0
    error_count = 0

    for filename in filenames:
        try:
            # Source file in Calls directory
            source_file = PROCESSING_DIRECTORY / "Calls" / filename

            if not source_file.exists():
                logger.error(f"Source attachment not found: {source_file}")
                error_count += 1
                continue

            # Destination file in attachments directory
            dest_file = attachments_dir / filename

            if dest_file.exists():
                logger.debug(f"Attachment already exists: {filename}")
                skipped_count += 1
                continue

            # Copy the file with optimized buffer size
            import shutil

            shutil.copy2(source_file, dest_file)
            copied_count += 1

            # Log progress every 100 files
            if copied_count % 100 == 0:
                logger.debug(
                    f"Attachment copying progress: {copied_count}/{len(filenames)} copied"
                )

        except Exception as e:
            logger.error(f"Failed to copy attachment {filename}: {e}")
            error_count += 1

    logger.info(
        f"Attachment copying completed: {copied_count} copied, {skipped_count} skipped, {error_count} errors"
    )
    logger.info(
        f"Total attachments in directory: {len(list(attachments_dir.glob('*')))}"
    )


def copy_attachments_parallel(filenames: set, attachments_dir: Path) -> None:
    """Copy attachments using parallel processing for large datasets."""
    logger.info(
        f"Using parallel processing for {len(filenames)} attachments with {MAX_WORKERS} workers"
    )

    # Convert set to list for indexing
    filename_list = list(filenames)

    # Split into chunks for parallel processing - use generator for memory efficiency
    chunk_size = max(100, len(filename_list) // MAX_WORKERS)
    chunks = list(
        filename_list[i:i + chunk_size]
        for i in range(0, len(filename_list), chunk_size)
    )

    copied_count = 0
    skipped_count = 0
    error_count = 0

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # Submit chunk copying tasks
        future_to_chunk = {
            executor.submit(copy_chunk_parallel, chunk, attachments_dir): chunk
            for chunk in chunks
        }

        # Collect results as they complete
        for future in as_completed(future_to_chunk):
            try:
                chunk_result = future.result()
                copied_count += chunk_result["copied"]
                skipped_count += chunk_result["skipped"]
                error_count += chunk_result["errors"]

                # Log progress
                logger.debug(
                    f"Chunk completed: {chunk_result['copied']} copied, {chunk_result['skipped']} skipped, {chunk_result['errors']} errors"
                )

            except Exception as e:
                logger.error(f"Failed to process chunk: {e}")
                continue

    logger.info(
        f"Parallel attachment copying completed: {copied_count} copied, {skipped_count} skipped, {error_count} errors"
    )
    logger.info(
        f"Total attachments in directory: {len(list(attachments_dir.glob('*')))}"
    )


def copy_chunk_parallel(filenames: List[str], attachments_dir: Path) -> Dict[str, int]:
    """Copy a chunk of attachments for parallel processing."""
    chunk_result = {"copied": 0, "skipped": 0, "errors": 0}

    for filename in filenames:
        try:
            # Source file in Calls directory
            source_file = PROCESSING_DIRECTORY / "Calls" / filename

            if not source_file.exists():
                chunk_result["errors"] += 1
                continue

            # Destination file in attachments directory
            dest_file = attachments_dir / filename

            if dest_file.exists():
                chunk_result["skipped"] += 1
                continue

            # Copy the file with optimized buffer size
            import shutil

            shutil.copy2(source_file, dest_file)
            chunk_result["copied"] += 1

        except Exception as e:
            chunk_result["errors"] += 1
            logger.debug(f"Chunk exception: {e}")
            continue

    return chunk_result


def main():
    """Main conversion function with comprehensive progress logging and performance optimization."""
    start_time = time.time()

    try:
        logger.info("=" * 60)
        logger.info("Starting Google Voice SMS Takeout XML Conversion")
        logger.info("=" * 60)
        logger.info(f"Processing directory: {PROCESSING_DIRECTORY}")
        logger.info(f"Output directory: {OUTPUT_DIRECTORY}")

        # Check and increase system file descriptor limits
        check_and_increase_file_limits()

        # Performance configuration logging
        logger.info("Performance Configuration:")
        logger.info(f"  Parallel processing: {ENABLE_PARALLEL_PROCESSING}")
        logger.info(f"  Max workers: {MAX_WORKERS}")
        logger.info(f"  Memory efficient threshold: {MEMORY_EFFICIENT_THRESHOLD:,}")
        logger.info(f"  Streaming parsing: {ENABLE_STREAMING_PARSING}")
        logger.info(f"  Memory mapping threshold: {MMAP_THRESHOLD // (1024*1024)}MB")

        # Validate configuration
        validate_configuration()

        # Validate date range if both filters are set
        if DATE_FILTER_NEWER_THAN is not None and DATE_FILTER_OLDER_THAN is not None:
            # Quick check: scan a few files to see if any fall within the date range
            logger.info("üìÖ Scanning files to validate date range coverage...")
            sample_files = []
            for root, dirs, files in os.walk(PROCESSING_DIRECTORY):
                for file in files:
                    if file.endswith('.html') and len(sample_files) < 10:
                        sample_files.append(os.path.join(root, file))
                if len(sample_files) >= 10:
                    break
            
            # Check if any sample files fall within the date range
            files_in_range = 0
            for file_path in sample_files:
                try:
                    # Extract timestamp from filename (common pattern: YYYY-MM-DDTHH_MM_SSZ)
                    filename = os.path.basename(file_path)
                    if 'T' in filename and 'Z.html' in filename:
                        # Extract the timestamp part
                        timestamp_str = filename.split('T')[0] + 'T' + filename.split('T')[1].split('Z')[0]
                        timestamp_str = timestamp_str.replace('_', ':')
                        file_date = dateutil.parser.parse(timestamp_str)
                        
                        # Check if file falls within our valid range
                        if DATE_FILTER_OLDER_THAN < file_date < DATE_FILTER_NEWER_THAN:
                            files_in_range += 1
                except Exception:
                    # Skip files we can't parse
                    continue
            
            if files_in_range == 0:
                logger.warning(f"‚ö†Ô∏è  WARNING: No sample files found within the specified date range!")
                logger.warning(f"   Date range: {DATE_FILTER_OLDER_THAN} to {DATE_FILTER_NEWER_THAN}")
                logger.warning(f"   This may indicate no messages will be processed")
                logger.warning(f"   Consider adjusting your date filters or use --full-run to process all files")
                
                # Ask user if they want to continue
                if not args.full_run:
                    logger.error(f"‚ùå REFUSING TO CONTINUE: No files found in date range and not in full-run mode")
                    logger.error(f"   Use --full-run to override this safety check")
                    sys.exit(1)

        # Build attachment mapping
        logger.info("Building attachment mapping...")
        mapping_start = time.time()
        src_filename_map = build_attachment_mapping_with_progress()
        mapping_time = time.time() - mapping_start
        logger.info(
            f"Found {len(src_filename_map)} attachment mappings in {mapping_time:.2f}s"
        )

        # Copy all mapped attachments
        logger.info("Copying mapped attachments...")
        copy_start = time.time()
        copy_mapped_attachments(src_filename_map)
        copy_time = time.time() - copy_start
        logger.info(f"Attachment copying completed in {copy_time:.2f}s")

        # Process HTML files
        logger.info("Processing HTML files...")
        processing_start = time.time()
        stats = process_html_files(src_filename_map)
        processing_time = time.time() - processing_start
        logger.info(
            f"Processed {stats['num_sms']} SMS, {stats['num_img']} images, {stats['num_vcf']} vCards in {processing_time:.2f}s"
        )

        # Finalize conversation files
        logger.info("Finalizing conversation files...")
        finalize_start = time.time()
        CONVERSATION_MANAGER.finalize_conversation_files()
        finalize_time = time.time() - finalize_start

        # Calculate elapsed time and generate index
        elapsed_time = time.time() - start_time
        index_start = time.time()
        CONVERSATION_MANAGER.generate_index_html(stats, elapsed_time)
        index_time = time.time() - index_start

        # Display final results
        display_results(stats, elapsed_time)

        # Performance breakdown
        logger.info("Performance Breakdown:")
        logger.info(
            f"  Attachment mapping: {mapping_time:.2f}s ({(mapping_time/elapsed_time)*100:.1f}%)"
        )
        logger.info(
            f"  Attachment copying: {copy_time:.2f}s ({(copy_time/elapsed_time)*100:.1f}%)"
        )
        logger.info(
            f"  HTML processing: {processing_time:.2f}s ({(processing_time/elapsed_time)*100:.1f}%)"
        )
        logger.info(
            f"  File finalization: {finalize_time:.2f}s ({(finalize_time/elapsed_time)*100:.1f}%)"
        )
        logger.info(
            f"  Index generation: {index_time:.2f}s ({(index_time/elapsed_time)*100:.1f}%)"
        )

        # Throughput metrics
        if "num_sms" in stats and stats["num_sms"] > 0:
            sms_per_second = stats["num_sms"] / elapsed_time
            logger.info(f"Throughput: {sms_per_second:.1f} SMS messages/second")

        if len(src_filename_map) > 0:
            attachments_per_second = (
                len(
                    [
                        f
                        for f in src_filename_map.values()
                        if f != "No unused match found"
                    ]
                )
                / elapsed_time
            )
            logger.info(f"Throughput: {attachments_per_second:.1f} attachments/second")

        logger.info("=" * 60)
        logger.info("Conversion completed successfully!")
        logger.info("=" * 60)

    except Exception as e:
        elapsed_time = time.time() - start_time
        logger.error(f"Conversion failed after {elapsed_time:.2f} seconds: {e}")
        logger.error(f"Exception type: {type(e).__name__}")
        logger.error(f"Exception details: {str(e)}")
        import traceback

        logger.error(f"Traceback: {traceback.format_exc()}")
        raise


def format_elapsed_time(seconds: int) -> str:
    """Format elapsed time in a human-readable format using timedelta."""
    delta = timedelta(seconds=seconds)

    # Use timedelta's built-in attributes for cleaner formatting
    if delta.days > 0:
        return f"{delta.days} days, {delta.seconds // 3600} hours, {(delta.seconds % 3600) // 60} minutes"
    elif delta.seconds >= 3600:
        hours = delta.seconds // 3600
        minutes = (delta.seconds % 3600) // 60
        return f"{hours} hours, {minutes} minutes" if minutes > 0 else f"{hours} hours"
    elif delta.seconds >= 60:
        minutes = delta.seconds // 60
        secs = delta.seconds % 60
        return (
            f"{minutes} minutes, {secs} seconds" if secs > 0 else f"{minutes} minutes"
        )
    else:
        return f"{delta.seconds} seconds"


def display_results(stats: Dict[str, int], elapsed_time: float):
    """
    Display conversion results and performance metrics.

    Args:
        stats: Dictionary containing conversion statistics
        elapsed_time: Total elapsed time in seconds
    """
    logger.info("=" * 60)
    logger.info("CONVERSION RESULTS")
    logger.info("=" * 60)
    logger.info(f"Total SMS messages processed: {stats['num_sms']}")
    logger.info(f"Total images processed: {stats['num_img']}")
    logger.info(f"Total vCard contacts processed: {stats['num_vcf']}")
    logger.info(f"Total calls processed: {stats['num_calls']}")
    logger.info(f"Total voicemails processed: {stats['num_voicemails']}")
    logger.info(f"Total processing time: {elapsed_time:.2f} seconds")

    total_messages = stats["num_sms"] + stats["num_calls"] + stats["num_voicemails"]
    if total_messages > 0:
        messages_per_second = total_messages / elapsed_time
        logger.info(f"Processing rate: {messages_per_second:.2f} messages/second")

    logger.info(f"Output directory: {OUTPUT_DIRECTORY}")
    logger.info("=" * 60)


def extract_own_phone_number(soup: BeautifulSoup) -> Optional[str]:
    """
    Extract the user's own phone number from the HTML.

    Args:
        soup: BeautifulSoup object of the HTML file

    Returns:
        Optional[str]: User's phone number or None if not found
    """
    try:
        # Look for phone number in abbr elements with class 'fn'
        for abbr_tag in soup.find_all("abbr", class_="fn"):
            if abbr_tag.get_text(strip=True) == "Me":
                a_tag = abbr_tag.find_previous("a", class_="tel")
                if a_tag:
                    phone_number = a_tag.get("href").split(":", 1)[-1]
                    try:
                        # Parse and format the phone number
                        parsed_number = phonenumbers.parse(phone_number, None)
                        return format_number(parsed_number)
                    except phonenumbers.phonenumberutil.NumberParseException:
                        return phone_number

        return None

    except Exception as e:
        logger.error(f"Failed to extract own phone number: {e}")
        return None


def remove_problematic_files() -> None:
    """
    Remove problematic files that could interfere with processing.

    Returns:
        None
    """
    # Note: This function may not benefit from simple caching
    # due to file system operations, but the pattern is established
    try:
        # Remove any existing log files
        log_path = Path(LOG_FILENAME)
        if log_path.exists():
            log_path.unlink()
            logger.info("Removed existing log file")

    except Exception as e:
        logger.error(f"Failed to remove problematic files: {e}")


def remove_files_by_pattern(pattern: str, reason: str, regex_pattern: str = "") -> None:
    """
    Remove files matching a pattern with optional regex filtering.

    Args:
        pattern: File pattern to match
        reason: Reason for removal
        regex_pattern: Optional regex pattern for additional filtering

    Returns:
        None
    """
    try:
        # Use glob for pattern matching (more compatible with existing code)
        # Search in the processing directory
        search_pattern = str(PROCESSING_DIRECTORY / pattern)
        files_to_remove = glob.glob(search_pattern)

        if regex_pattern:
            import re

            regex = re.compile(regex_pattern)
            files_to_remove = [f for f in files_to_remove if regex.match(Path(f).name)]

        for file_path in files_to_remove:
            try:
                os.remove(file_path)
                logger.info(f"Removed {file_path} ({reason})")
            except Exception as e:
                logger.error(f"Failed to remove {file_path}: {e}")

    except Exception as e:
        logger.error(f"Failed to remove files by pattern {pattern}: {e}")


@lru_cache(maxsize=50000)
def escape_xml(s: str) -> str:
    """
    Escape special characters for XML output.

    Args:
        s: String to escape

    Returns:
        str: XML-escaped string
    """
    replacements = {
        "&": "&amp;",
        "<": "&lt;",
        ">": "&gt;",
        "'": "&apos;",
        '"': "&quot;",
    }

    for old, new in replacements.items():
        s = s.replace(old, new)
    return s


@lru_cache(maxsize=10000)
def extract_src_cached(html_directory: str) -> List[str]:
    """
    Cached source extraction for performance optimization.

    Args:
        html_directory: Directory to search for HTML files

    Returns:
        list: List of src/href values found in HTML files
    """
    src_list = []

    try:
        for html_file in Path(html_directory).rglob("*.html"):
            try:
                with open(html_file, "r", encoding="utf-8", buffering=FILE_READ_BUFFER_SIZE) as file:
                    soup = BeautifulSoup(file, HTML_PARSER)

                    # Extract image src attributes - use cached selector for performance
                    src_list.extend(
                        img["src"] for img in soup.select(STRING_POOL.ADDITIONAL_SELECTORS["img_src"])
                    )

                    # Extract vCard href attributes - use cached selector for performance
                    src_list.extend(
                        a["href"]
                        for a in soup.select(STRING_POOL.ADDITIONAL_SELECTORS["vcard_links"])
                    )

            except Exception as e:
                # CRITICAL: File processing failures are errors that need attention
                logger.error(f"Failed to process {html_file}: {e}")
                logger.debug(f"Continuing with next file to maintain processing flow")
                continue

    except Exception as e:
        logger.error(f"Failed to extract src from {html_directory}: {e}")

    return src_list


def extract_src(html_directory: str) -> List[str]:
    """
    Extract image src attributes and vCard href attributes from HTML files.

    Args:
        html_directory: Directory to search for HTML files

    Returns:
        list: List of src/href values found in HTML files
    """
    # Use cached version for better performance
    return extract_src_cached(html_directory)


def extract_src_with_progress(html_directory: str = None) -> List[str]:
    """
    Extract image src attributes and vCard href attributes from HTML files with progress logging.

    Args:
        html_directory: Directory to search for HTML files (defaults to PROCESSING_DIRECTORY)

    Returns:
        list: List of src/href values found in HTML files
    """
    if html_directory is None:
        html_directory = str(PROCESSING_DIRECTORY)

    src_list = []

    try:
        # Get total count of HTML files for progress tracking
        html_files = list(Path(html_directory).rglob("*.html"))
        total_files = len(html_files)

        if total_files == 0:
            logger.error(f"No HTML files found in {html_directory}")
            return src_list

        logger.info(f"Starting src extraction from {total_files} HTML files")

        # Apply test mode limit if enabled
        if TEST_MODE and total_files > TEST_LIMIT:
            logger.info(
                f"üß™ TEST MODE: Limiting src extraction to first {TEST_LIMIT} files out of {total_files} total"
            )
            html_files = html_files[:TEST_LIMIT]
            total_files = TEST_LIMIT

        last_reported_progress = 0

        for i, html_file in enumerate(html_files):
            try:
                with open(html_file, "r", encoding="utf-8", buffering=FILE_READ_BUFFER_SIZE) as file:
                    soup = BeautifulSoup(file, HTML_PARSER)

                    # Extract image src attributes - use cached selector for performance
                    img_srcs = [
                        img["src"] for img in soup.select(STRING_POOL.ADDITIONAL_SELECTORS["img_src"])
                        if img.get("src") and is_valid_image_src(img["src"])
                    ]
                    src_list.extend(img_srcs)

                    # Extract vCard href attributes - use cached selector for performance
                    vcard_hrefs = [
                        a["href"]
                        for a in soup.select(STRING_POOL.ADDITIONAL_SELECTORS["vcard_links"])
                        if a.get("href") and is_valid_vcard_href(a["href"])
                    ]
                    src_list.extend(vcard_hrefs)

                # Report progress using utility function
                current_progress = i + 1
                if should_report_progress(
                    current_progress, total_files, last_reported_progress
                ):
                    additional_info = f"Total src elements found: {len(src_list)}"
                    progress_msg = format_progress_message(
                        current_progress, total_files, "Src extraction", additional_info
                    )
                    logger.info(progress_msg)
                    last_reported_progress = current_progress

            except Exception as e:
                # CRITICAL: File processing failures are errors that need attention
                logger.error(f"Failed to process {html_file}: {e}")
                logger.debug(f"Continuing with next file to maintain processing flow")
                continue

        # Log final performance metrics

        logger.info(
            f"Completed src extraction from {total_files} files. Total src elements: {len(src_list)}"
        )

    except Exception as e:
        logger.error(f"Failed to extract src from {html_directory}: {e}")

    return src_list


def is_valid_image_src(src: str) -> bool:
    """
    Validate that a src attribute contains a valid image reference.
    
    Args:
        src: The src attribute value to validate
        
    Returns:
        bool: True if src is a valid image reference, False otherwise
    """
    if not src or not isinstance(src, str):
        return False
    
    # Skip empty or whitespace-only src
    if not src.strip():
        return False
    
    # Skip src that looks like HTML filenames (contains Google Voice patterns)
    if any(pattern in src for pattern in [
        " - Text - ", " - Voicemail - ", " - Received - ", " - Placed - ", " - Missed - "
    ]):
        logger.debug(f"Filtering out HTML filename as invalid image src: {src}")
        return False
    
    # Skip src that contains timestamp patterns (likely HTML filenames)
    if re.search(r'\d{4}-\d{2}-\d{2}T\d{2}_\d{2}_\d{2}Z', src):
        logger.debug(f"Filtering out timestamp pattern as invalid image src: {src}")
        return False
    
    # Skip src that contains phone number patterns (likely HTML filenames)
    if re.search(r'\+\d+', src):
        logger.debug(f"Filtering out phone number pattern as invalid image src: {src}")
        return False
    
    # Valid image src should be a filename with image extension or data URL
    if src.startswith('data:'):
        return True
    
    # Check for common image extensions
    valid_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp', '.svg']
    if any(src.lower().endswith(ext) for ext in valid_extensions):
        return True
    
    # Allow relative paths that might be images
    if src.startswith('./') or src.startswith('../') or src.startswith('/'):
        return True
    
    # Allow simple filenames that might be images
    if '/' not in src and '\\' not in src and '.' in src:
        return True
    
    logger.debug(f"Filtering out potentially invalid image src: {src}")
    return False


def is_valid_vcard_href(href: str) -> bool:
    """
    Validate that an href attribute contains a valid vCard reference.
    
    Args:
        href: The href attribute value to validate
        
    Returns:
        bool: True if href is a valid vCard reference, False otherwise
    """
    if not href or not isinstance(href, str):
        return False
    
    # Skip empty or whitespace-only href
    if not href.strip():
        return False
    
    # Skip href that looks like HTML filenames (contains Google Voice patterns)
    if any(pattern in href for pattern in [
        " - Text - ", " - Voicemail - ", " - Received - ", " - Placed - ", " - Missed - "
    ]):
        logger.debug(f"Filtering out HTML filename as invalid vCard href: {href}")
        return False
    
    # Skip href that contains timestamp patterns (likely HTML filenames)
    if re.search(r'\d{4}-\d{2}-\d{2}T\d{2}_\d{2}_\d{2}Z', href):
        logger.debug(f"Filtering out timestamp pattern as invalid vCard href: {href}")
        return False
    
    # Valid vCard href should be a filename with .vcf extension or data URL
    if href.startswith('data:'):
        return True
    
    # Check for vCard extension
    if href.lower().endswith('.vcf'):
        return True
    
    # Allow relative paths that might be vCards
    if href.startswith('./') or href.startswith('../') or href.startswith('/'):
        return True
    
    # Allow simple filenames that might be vCards
    if '/' not in href and '\\' not in href and '.' in href:
        return True
    
    logger.debug(f"Filtering out potentially invalid vCard href: {href}")
    return False


def extract_src_with_source_files(html_directory: str = None) -> Dict[str, List[str]]:
    """
    Extract image src attributes and vCard href attributes from HTML files with their source file information.

    Args:
        html_directory: Directory to search for HTML files (defaults to PROCESSING_DIRECTORY)

    Returns:
        dict: Mapping from src/href values to list of HTML files that contain them
    """
    if html_directory is None:
        html_directory = str(PROCESSING_DIRECTORY)

    src_to_files = {}

    try:
        # Get total count of HTML files for progress tracking
        html_files = list(Path(html_directory).rglob("*.html"))
        total_files = len(html_files)

        if total_files == 0:
            logger.error(f"No HTML files found in {html_directory}")
            return src_to_files

        logger.info(
            f"Starting src extraction with source tracking from {total_files} HTML files"
        )

        # Apply test mode limit if enabled
        if TEST_MODE and total_files > TEST_LIMIT:
            logger.info(
                f"üß™ TEST MODE: Limiting src extraction to first {TEST_LIMIT} files out of {total_files} total"
            )
            html_files = html_files[:TEST_LIMIT]
            total_files = TEST_LIMIT
        else:
            logger.info(
                f"Scanning all {total_files} HTML files for attachment references to ensure complete coverage"
            )

        last_reported_progress = 0

        for i, html_file in enumerate(html_files):
            try:
                with open(html_file, "r", encoding="utf-8", buffering=FILE_READ_BUFFER_SIZE) as file:
                    soup = BeautifulSoup(file, HTML_PARSER)

                    # Extract image src attributes - use cached selector for performance
                    img_srcs = [
                        img["src"] for img in soup.select(STRING_POOL.ADDITIONAL_SELECTORS["img_src"])
                    ]
                    for src in img_srcs:
                        if src not in src_to_files:
                            src_to_files[src] = []
                        src_to_files[src].append(str(html_file.name))

                    # Extract vCard href attributes - use cached selector for performance
                    vcard_hrefs = [
                        a["href"]
                        for a in soup.select(STRING_POOL.ADDITIONAL_SELECTORS["vcard_links"])
                    ]
                    for src in vcard_hrefs:
                        if src not in src_to_files:
                            src_to_files[src] = []
                        src_to_files[src].append(str(html_file.name))

                # Report progress using utility function
                current_progress = i + 1
                if should_report_progress(
                    current_progress, total_files, last_reported_progress
                ):
                    additional_info = (
                        f"Total unique src elements found: {len(src_to_files)}"
                    )
                    progress_msg = format_progress_message(
                        current_progress,
                        total_files,
                        "Src extraction with source tracking",
                        additional_info,
                    )
                    logger.info(progress_msg)
                    last_reported_progress = current_progress

            except Exception as e:
                logger.warning(f"Failed to process {html_file}: {e}")
                continue

        # Log final performance metrics

        logger.info(
            f"Completed src extraction with source tracking from {total_files} files. Total unique src elements: {len(src_to_files)}"
        )

    except Exception as e:
        logger.error(
            f"Failed to extract src with source tracking from {html_directory}: {e}"
        )

    return src_to_files


@lru_cache(maxsize=100)
def list_att_filenames_cached(directory: str) -> List[str]:
    """
    Cached attachment filename listing for performance optimization.

    Args:
        directory: Directory to search for attachments

    Returns:
        list: List of attachment filenames
    """
    try:
        return [
            str(path.name)
            for path in Path(directory).rglob("*")
            if path.suffix.lower() in SUPPORTED_EXTENSIONS
        ]
    except Exception as e:
        logger.error(f"Failed to list attachment filenames from {directory}: {e}")
        return []


def list_att_filenames(directory: str = None) -> List[str]:
    """
    List all attachment filenames with supported extensions.

    Args:
        directory: Directory to search for attachments (defaults to PROCESSING_DIRECTORY)

    Returns:
        list: List of attachment filenames
    """
    if directory is None:
        directory = str(PROCESSING_DIRECTORY)

    # Use cached version for better performance
    return list_att_filenames_cached(directory)


def list_att_filenames_with_progress(directory: str = None) -> List[str]:
    """
    List all attachment filenames with supported extensions and progress logging.

    Args:
        directory: Directory to search for attachments (defaults to PROCESSING_DIRECTORY)

    Returns:
        list: List of attachment filenames
    """
    if directory is None:
        directory = str(PROCESSING_DIRECTORY)

    start_time = time.time()

    try:
        # Use more efficient file discovery for large directories
        if ENABLE_STREAMING_PARSING:
            # Use generator-based approach to avoid loading all files into memory
            file_generator = Path(directory).rglob("*")
            total_files = 0
            processed_files = 0

            logger.info(f"Starting streaming attachment scan in {directory}")

            attachment_filenames = []
            attachment_count = 0

            for file_path in file_generator:
                total_files += 1
                processed_files += 1

                try:
                    if (
                        file_path.is_file()
                        and file_path.suffix.lower() in SUPPORTED_EXTENSIONS
                    ):
                        attachment_filenames.append(str(file_path.name))
                        attachment_count += 1

                    # Report progress every 1000 files for large datasets
                    if processed_files % 1000 == 0:
                        logger.info(
                            f"Attachment scan progress: {processed_files} files processed, {attachment_count} attachments found"
                        )

                except Exception as e:
                    logger.error(f"Failed to process {file_path}: {e}")
                    continue
        else:
            # Traditional approach for smaller datasets
            all_files = list(Path(directory).rglob("*"))
            total_files = len(all_files)

            if total_files == 0:
                logger.error(f"No files found in {directory}")
                return []

            logger.info(
                f"Starting attachment scan in {directory} - {total_files} total files to examine"
            )

            # Note: We don't apply test mode limit to attachment scanning
            # because we need to see ALL available attachments to properly match them
            # Test mode only limits the HTML files being processed

            last_reported_progress = 0
            attachment_count = 0

            attachment_filenames = []

            for i, file_path in enumerate(all_files):
                if file_path.suffix.lower() in SUPPORTED_EXTENSIONS:
                    attachment_filenames.append(str(file_path.name))
                    attachment_count += 1

                # Report progress
                current_progress = i + 1
                if should_report_progress(
                    current_progress, total_files, last_reported_progress
                ):
                    percentage = (current_progress / total_files) * 100
                    logger.info(
                        f"Attachment scan progress: {current_progress}/{total_files} files examined ({percentage:.1f}%) - "
                        f"Attachments found: {attachment_count}"
                    )
                    last_reported_progress = current_progress

        # Log final performance metrics
        scan_time = time.time() - start_time
        logger.info(
            f"Completed attachment scan in {scan_time:.2f}s. Found {attachment_count} attachments out of {total_files} total files"
        )
        return attachment_filenames

    except Exception as e:
        logger.error(f"Failed to list attachment filenames from {directory}: {e}")
        return []


@lru_cache(maxsize=50000)
def normalize_filename(filename: str) -> str:
    """
    Remove file extension and parenthesized numbers from filename.

    This is used to match filenames back to their respective img_src keys.

    Args:
        filename: Original filename

    Returns:
        str: Normalized filename (max 50 characters)
    """
    # Use pre-compiled regex pattern for better performance
    return FILENAME_PATTERN.sub("", filename)[:FILENAME_TRUNCATE_LENGTH]


def custom_filename_sort(filename: str) -> Tuple[str, int, str]:
    """
    Custom sorting function for filenames with parenthesized numbers.

    Ensures files with parenthesized numbers follow the base filename.

    Args:
        filename: Filename to sort

    Returns:
        tuple: (base_filename, number, extension) for sorting
    """
    # Use pre-compiled regex pattern for better performance
    match = CUSTOM_SORT_PATTERN.match(filename)
    if match:
        base_filename = match.group(1)
        number = int(match.group(2)) if match.group(2) else -1
        extension = match.group(3) if match.group(3) else ""
        return (base_filename, number, extension)
    return (filename, float("inf"), "")


@lru_cache(maxsize=25000)
def src_to_filename_mapping_cached(
    src_elements_str: str, att_filenames_str: str
) -> Dict[str, str]:
    """
    Cached source to filename mapping for performance optimization.

    Args:
        src_elements_str: String representation of src elements list
        att_filenames_str: String representation of attachment filenames list

    Returns:
        dict: Mapping from src to filename
    """
    # Convert strings back to lists
    src_elements = src_elements_str.split(",") if src_elements_str else []
    att_filenames = att_filenames_str.split(",") if att_filenames_str else []

    used_filenames = set()
    mapping = {}

    # Sort filenames before matching to ensure consistent results
    att_filenames.sort(key=custom_filename_sort)

    for src in src_elements:
        if not src.strip():
            continue
        assigned_filename = None
        for filename in att_filenames:
            if not filename.strip():
                continue
            normalized_filename = normalize_filename(filename.strip())
            # Check if normalized filename is in src (with both underscore and hyphen variants)
            if (
                normalized_filename in src
                or normalized_filename.replace("-", "_") in src
                or normalized_filename.replace("_", "-") in src
            ) and filename.strip() not in used_filenames:
                assigned_filename = filename.strip()
                used_filenames.add(filename.strip())
                break
        mapping[src.strip()] = assigned_filename or "No unused match found"

    return mapping


def src_to_filename_mapping(
    src_elements: List[str], att_filenames: List[str]
) -> Dict[str, str]:
    """
    Create a mapping from HTML src elements to attachment filenames.

    Args:
        src_elements: List of src/href values from HTML
        att_filenames: List of attachment filenames

    Returns:
        dict: Mapping from src to filename
    """
    # Use cached version for better performance
    src_elements_str = ",".join(src_elements) if src_elements else ""
    att_filenames_str = ",".join(att_filenames) if att_filenames else ""
    return src_to_filename_mapping_cached(src_elements_str, att_filenames_str)


def src_to_filename_mapping_with_progress(
    src_elements: List[str], att_filenames: List[str]
) -> Dict[str, str]:
    """
    Create a mapping from HTML src elements to attachment filenames with progress logging.

    Args:
        src_elements: List of src/href values from HTML
        att_filenames: List of attachment filenames

    Returns:
        dict: Mapping from src to filename
    """
    used_filenames = set()
    mapping = {}

    total_src_elements = len(src_elements)
    if total_src_elements == 0:
        logger.error("No src elements to map")
        return mapping

    logger.info(
        f"Starting src-to-filename mapping for {total_src_elements} src elements and {len(att_filenames)} attachment files"
    )

    # Sort filenames before matching to ensure consistent results
    att_filenames.sort(key=custom_filename_sort)

    # Calculate progress reporting thresholds
    progress_interval = max(
        1, min(100, total_src_elements // 10)
    )  # Every 10% or 100 elements, whichever is smaller
    last_reported_progress = 0
    mapped_count = 0

    for i, src in enumerate(src_elements):
        if not src.strip():
            continue

        assigned_filename = None
        for filename in att_filenames:
            if not filename.strip():
                continue
            normalized_filename = normalize_filename(filename.strip())
            if normalized_filename in src and filename.strip() not in used_filenames:
                assigned_filename = filename.strip()
                used_filenames.add(filename.strip())
                mapped_count += 1
                break

        mapping[src.strip()] = assigned_filename or "No unused match found"

        # Report progress
        current_progress = i + 1
        if (
            current_progress % progress_interval == 0
            or current_progress == total_src_elements
            or current_progress - last_reported_progress >= 100
        ):

            percentage = (current_progress / total_src_elements) * 100
            logger.info(
                f"Mapping progress: {current_progress}/{total_src_elements} src elements processed ({percentage:.1f}%) - "
                f"Successfully mapped: {mapped_count}"
            )
            last_reported_progress = current_progress

    logger.info(
        f"Completed src-to-filename mapping. Successfully mapped {mapped_count} out of {total_src_elements} src elements"
    )
    return mapping


@lru_cache(maxsize=100)
def build_attachment_mapping_cached() -> Dict[str, str]:
    """
    Cached attachment mapping building for performance optimization.

    Returns:
        dict: Mapping from src elements to attachment filenames
    """
    try:
        # Use the configured processing directory for both scans
        src_elements = extract_src(str(PROCESSING_DIRECTORY))
        att_filenames = list_att_filenames(str(PROCESSING_DIRECTORY))
        return src_to_filename_mapping(src_elements, att_filenames)
    except Exception as e:
        logger.error(f"Failed to build attachment mapping: {e}")
        return {}


def build_attachment_mapping() -> Dict[str, str]:
    """Build mapping from HTML src elements to attachment filenames."""
    # Use cached version for better performance
    return build_attachment_mapping_cached()


def build_attachment_mapping_with_progress() -> Dict[str, str]:
    """Build mapping from src elements to attachment filenames with progress tracking."""

    logger.info("Starting attachment mapping build process...")

    # Step 1: Extract src elements from HTML files with source tracking
    logger.info("Starting src extraction from HTML files with source tracking")
    src_to_files = extract_src_with_source_files()
    src_elements = list(src_to_files.keys())

    # Step 2: Scan for attachment files
    logger.info("Starting attachment scan in processing directory")
    att_filenames = list_att_filenames_with_progress()

    # Step 3: Create mapping
    logger.info(
        f"Starting src-to-filename mapping for {len(src_elements)} src elements and {len(att_filenames)} attachment files"
    )

    # Pre-normalize filenames to avoid repeated processing
    normalized_attachments = {}
    for filename in att_filenames:
        normalized = normalize_filename(filename)
        if normalized not in normalized_attachments:
            normalized_attachments[normalized] = filename

    # Create comprehensive mapping that finds ALL attachments for each HTML file
    mapping = {}

    # First, create a reverse mapping: HTML base filename -> list of all matching attachments
    # Use more efficient data structures for large datasets
    html_to_attachments = {}

    # Pre-process HTML files to avoid repeated string operations
    html_bases = set()
    for html_list in src_to_files.values():
        for html_file in html_list:
            html_base = html_file.replace(".html", "")
            html_bases.add(html_base)

    # Use dictionary comprehension for faster mapping creation
    # Sort attachment filenames for binary search optimization
    att_filenames_sorted = sorted(att_filenames)

    # Create mapping using more efficient algorithm
    for html_base in html_bases:
        # Use binary search approach for finding matching attachments
        matching_attachments = []

        # Find the first attachment that starts with this HTML base
        start_idx = 0
        while start_idx < len(att_filenames_sorted):
            filename = att_filenames_sorted[start_idx]
            if filename.startswith(html_base):
                # Found a match, collect all consecutive matches
                while start_idx < len(att_filenames_sorted) and att_filenames_sorted[
                    start_idx
                ].startswith(html_base):
                    matching_attachments.append(att_filenames_sorted[start_idx])
                    start_idx += 1
                break
            elif filename < html_base:
                start_idx += 1
            else:
                break

        if matching_attachments:
            html_to_attachments[html_base] = matching_attachments
            if logger.isEnabledFor(logging.DEBUG):
                logger.debug(
                    f"HTML '{html_base}' has {len(matching_attachments)} attachments: {matching_attachments[:3]}{'...' if len(matching_attachments) > 3 else ''}"
                )

    # Now create the src -> filename mapping using more efficient algorithms
    # Pre-compute used attachments set for O(1) lookups
    used_attachments = set()

    # Use list comprehension for faster mapping creation
    mapping_items = []

    for src in src_elements:
        if not src.strip():
            continue

        src_normalized = src.strip()
        assigned_filename = None

        if src_normalized in src_to_files:
            # Get the HTML files that contain this src reference
            html_files = src_to_files[src_normalized]

            # Find the first HTML file that has matching attachments
            for html_file in html_files:
                html_base = html_file.replace(".html", "")

                if html_base in html_to_attachments:
                    # Get the first available attachment for this HTML file
                    available_attachments = html_to_attachments[html_base]

                    # Find an attachment that hasn't been used yet
                    for attachment in available_attachments:
                        if attachment not in used_attachments:
                            assigned_filename = attachment
                            used_attachments.add(attachment)
                            break

                    if assigned_filename:
                        break

        # Fallback: Try exact matching if HTML-filename approach didn't work
        if not assigned_filename:
            for normalized, original in normalized_attachments.items():
                filename_without_ext = (
                    original.rsplit(".", 1)[0] if "." in original else original
                )

                if (
                    normalized == src_normalized
                    or normalized.replace("-", "_") == src_normalized
                    or normalized.replace("_", "-") == src_normalized
                    or filename_without_ext == src_normalized
                ):
                    assigned_filename = original
                    break

        mapping_items.append(
            (src_normalized, assigned_filename or "No unused match found")
        )

    # Convert to dictionary more efficiently
    mapping = dict(mapping_items)

    # Log mapping statistics
    total_mappings = len(mapping)
    successful_mappings = len(
        [f for f in mapping.values() if f != "No unused match found"]
    )
    unique_attachments = len(
        set([f for f in mapping.values() if f != "No unused match found"])
    )

    logger.info("Mapping statistics:")
    logger.info(f"  Total src elements: {total_mappings}")
    logger.info(f"  Successful mappings: {successful_mappings}")
    logger.info(f"  Unique attachments mapped: {unique_attachments}")
    logger.info(f"  HTML files with attachments: {len(html_to_attachments)}")

    # Log failed matches for debugging with enhanced corruption analysis
    failed_matches = [
        src for src, filename in mapping.items() if filename == "No unused match found"
    ]
    if failed_matches:
        logger.error(
            f"Found {len(failed_matches)} failed matches out of {total_mappings} total mappings"
        )
        
        # Analyze failed matches for corruption patterns
        corrupted_files = []
        other_failures = []
        
        for src in failed_matches:
            if is_corrupted_filename(src):
                corrupted_files.append(src)
            else:
                other_failures.append(src)
        
        # Report corruption issues
        if corrupted_files:
            logger.error(f"Found {len(corrupted_files)} corrupted filenames:")
            for src in corrupted_files[:5]:  # Show first 5 corrupted files
                logger.error(f"  CORRUPTED: '{src}'")
                # Try to suggest cleaned version
                cleaned = clean_corrupted_filename(src)
                if cleaned != src:
                    logger.error(f"    Suggested clean version: '{cleaned}'")
            if len(corrupted_files) > 5:
                logger.error(f"    ... and {len(corrupted_files) - 5} more corrupted files")
        
        # Report other failures
        if other_failures:
            logger.error(f"Found {len(other_failures)} other failed matches:")
            for src in other_failures[:5]:  # Show first 5 other failures
                logger.error(f"  FAILED: '{src}'")
            if len(other_failures) > 5:
                logger.error(f"    ... and {len(other_failures) - 5} more failed matches")
        
        # Provide guidance
        if corrupted_files:
            logger.error("CORRUPTION DETECTED: Some filenames appear to be corrupted")
            logger.error("This may indicate data export issues or file system corruption")
            logger.error("Consider re-exporting from Google Voice or checking file integrity")
        else:
            logger.error("Failed matches detected - this may indicate data corruption or export issues")

    # Log final performance metrics
    logger.info(f"Completed attachment mapping build. Total mappings: {len(mapping)}")

    return mapping


def is_sms_mms_file(filename: str) -> bool:
    """
    Determine if a file is likely to contain SMS/MMS messages.

    Args:
        filename: HTML filename to check

    Returns:
        bool: True if file likely contains SMS/MMS, False otherwise
    """
    filename_lower = filename.lower()

    # Files that definitely contain SMS/MMS
    if any(keyword in filename_lower for keyword in ["text", "group conversation"]):
        return True

    # Files that definitely don't contain SMS/MMS
    if any(
        keyword in filename_lower
        for keyword in ["placed", "missed", "received", "voicemail"]
    ):
        return False

    # Files with phone numbers might contain SMS/MMS (check for + pattern)
    if re.search(r"\+\d+", filename):
        return True

    # Files with names might contain SMS/MMS
    if re.search(r"[A-Za-z]+\s+[A-Za-z]+", filename):
        return True

    # Default to True for unknown files (process them to be safe)
    return True


def get_file_type(filename: str) -> str:
    """
    Determine the type of file based on filename.

    Args:
        filename: HTML filename to check

    Returns:
        str: File type ('sms', 'mms', 'call', 'voicemail', 'unknown')
    """
    filename_lower = filename.lower()

    if "group conversation" in filename_lower:
        return "mms"
    elif "text" in filename_lower:
        return "sms"
    elif "placed" in filename_lower:
        return "call"
    elif "missed" in filename_lower:
        return "call"
    elif "received" in filename_lower:
        return "call"
    elif "voicemail" in filename_lower:
        return "voicemail"
    else:
        return "unknown"


def process_html_files(src_filename_map: Dict[str, str]) -> Dict[str, int]:
    """Process all HTML files and return statistics."""
    stats = {
        "num_sms": 0,
        "num_img": 0,
        "num_vcf": 0,
        "num_calls": 0,
        "num_voicemails": 0,
    }
    own_number = None

    # Get HTML files from the Calls subdirectory
    calls_directory = PROCESSING_DIRECTORY / "Calls"
    if not calls_directory.exists():
        logger.error(f"Calls directory not found: {calls_directory}")
        return stats

    # Get HTML files as generator for memory efficiency
    html_files_gen = calls_directory.rglob("*.html")

    # Count files first for progress tracking (this is the only time we need the full list)
    html_files_list = list(html_files_gen)
    total_files = len(html_files_list)

    if total_files == 0:
        logger.error(f"No HTML files found in Calls directory: {calls_directory}")
        return stats

    # Process all files, not just SMS/MMS files
    all_files = html_files_list
    filtered_files = len(all_files)

    logger.info(
        f"Found {total_files} HTML files, processing all types including calls and voicemails"
    )

    # Apply test mode limit if enabled
    if TEST_MODE and filtered_files > TEST_LIMIT:
        logger.info(
            f"üß™ TEST MODE: Limiting processing to first {TEST_LIMIT} files out of {filtered_files} total"
        )
        all_files = all_files[:TEST_LIMIT]
        filtered_files = TEST_LIMIT

    # Use batch processing for large datasets
    if filtered_files > LARGE_DATASET_THRESHOLD and ENABLE_BATCH_PROCESSING:
        logger.info(
            f"Using batch processing for large dataset ({filtered_files} files)"
        )
        stats = process_html_files_batch(
            all_files, src_filename_map, batch_size=BATCH_SIZE_OPTIMAL
        )
    else:
        # Process files individually for smaller datasets
        last_reported_progress = 0
        processed_count = 0
        skipped_count = 0

        for i, html_file in enumerate(all_files):
            try:
                file_stats = process_single_html_file(
                    html_file, src_filename_map, own_number
                )

                # Update statistics
                stats["num_sms"] += file_stats["num_sms"]
                stats["num_img"] += file_stats["num_img"]
                stats["num_vcf"] += file_stats["num_vcf"]
                stats["num_calls"] += file_stats["num_calls"]
                stats["num_voicemails"] += file_stats["num_voicemails"]
                processed_count += 1

                # Extract own number from first file if not already found
                if own_number is None:
                    own_number = file_stats.get("own_number")

                # Report progress using utility function
                current_progress = i + 1
                if should_report_progress(
                    current_progress, filtered_files, last_reported_progress
                ):
                    additional_info = f"SMS: {stats['num_sms']}, Images: {stats['num_img']}, vCards: {stats['num_vcf']}, Calls: {stats['num_calls']}, Voicemails: {stats['num_voicemails']}"
                    progress_msg = format_progress_message(
                        current_progress,
                        filtered_files,
                        "File processing",
                        additional_info,
                    )
                    logger.info(progress_msg)
                    last_reported_progress = current_progress

            except Exception as e:
                logger.error(f"Failed to process {html_file}: {e}")
                skipped_count += 1
                continue

    logger.info(
        f"Completed processing {filtered_files} files. "
        f"Final stats - SMS: {stats['num_sms']}, Images: {stats['num_img']}, vCards: {stats['num_vcf']}, Calls: {stats['num_calls']}, Voicemails: {stats['num_voicemails']}"
    )
    return stats


def process_single_html_file(
    html_file: Path, src_filename_map: Dict[str, str], own_number: Optional[str]
) -> Dict[str, Union[int, str]]:
    """Process a single HTML file and return file-specific statistics."""
    
    # CHECK IF FILE SHOULD BE SKIPPED BEFORE PROCESSING
    if should_skip_file(html_file.name):
        logger.debug(f"Skipping corrupted or invalid file: {html_file.name}")
        return {
            "num_sms": 0,
            "num_img": 0,
            "num_vcf": 0,
            "num_calls": 0,
            "num_voicemails": 0,
        }
    
    file_type = get_file_type(html_file.name)
    logger.debug(f"Processing {html_file.name} (Type: {file_type})")

    # Parse HTML file
    soup = parse_html_file(html_file)

    # Extract own phone number if not already found
    if own_number is None:
        own_number = extract_own_phone_number(soup)

    # Handle different file types
    if file_type == "call":
        return process_call_file(html_file, soup, own_number, src_filename_map)
    elif file_type == "voicemail":
        return process_voicemail_file(html_file, soup, own_number, src_filename_map)
    else:
        # Process SMS/MMS files as before
        return process_sms_mms_file(html_file, soup, own_number, src_filename_map)


def process_sms_mms_file(
    html_file: Path,
    soup: BeautifulSoup,
    own_number: Optional[str],
    src_filename_map: Dict[str, str],
) -> Dict[str, Union[int, str]]:
    """Process SMS/MMS files and return statistics."""
    # Strategy 1: Use cached CSS selector for better performance
    messages_raw = soup.select(STRING_POOL.CSS_SELECTORS["message"])
    
    # Strategy 2: If no messages found, try alternative selectors
    if not messages_raw:
        logger.debug(f"Primary message selector failed for {html_file.name}, trying alternatives")
        # Try alternative selectors for different HTML structures
        alternative_selectors = [
            "div[class*='message']",
            "tr[class*='message']", 
            ".conversation div",
            "div[class*='sms']",
            "div[class*='text']",
            "tr[class*='sms']",
            "tr[class*='text']"
        ]
        
        for selector in alternative_selectors:
            messages_raw = soup.select(selector)
            if messages_raw:
                logger.debug(f"Found {len(messages_raw)} messages using selector: {selector}")
                break
    
    # Strategy 3: If still no messages, look for any divs with message-like content
    if not messages_raw:
        logger.debug(f"Alternative selectors failed for {html_file.name}, searching for message-like content")
        all_divs = soup.find_all("div")
        messages_raw = []
        for div in all_divs:
            # Check if div contains message-like content (timestamp, text, etc.)
            if (div.find(class_="dt") or div.find(abbr=True) or 
                div.find("q") or div.find(class_="sender") or
                div.get_text().strip()):
                messages_raw.append(div)
        
        if messages_raw:
            logger.debug(f"Found {len(messages_raw)} message-like divs in {html_file.name}")
    
    # Strategy 4: ENHANCED - Look for older Google Voice message structures
    if not messages_raw:
        logger.debug(f"Standard selectors failed for {html_file.name}, searching for older Google Voice structures")
        # Look for older Google Voice HTML structures that might not have modern CSS classes
        older_selectors = [
            "div:has(.dt)",  # Divs containing timestamp elements
            "div:has(abbr[title])",  # Divs containing timestamp abbr elements
            "div:has(.sender)",  # Divs containing sender information
            "div:has(q)",  # Divs containing quoted text (messages)
            "tr:has(.dt)",  # Table rows containing timestamp elements
            "tr:has(abbr[title])",  # Table rows containing timestamp abbr elements
            "tr:has(.sender)",  # Table rows containing sender information
            "tr:has(q)",  # Table rows containing quoted text (messages)
        ]
        
        for selector in older_selectors:
            try:
                messages_raw = soup.select(selector)
                if messages_raw:
                    logger.debug(f"Found {len(messages_raw)} older-style messages using selector: {selector}")
                    break
            except Exception as e:
                logger.debug(f"Selector '{selector}' failed: {e}")
                continue
    
    # Strategy 5: FINAL FALLBACK - Look for any elements that might contain messages
    if not messages_raw:
        logger.debug(f"All selectors failed for {html_file.name}, using comprehensive fallback")
        # Look for any element that contains both timestamp and text content
        potential_messages = []
        
        # Look for elements with timestamp indicators
        timestamp_elements = soup.find_all(["abbr", "span", "div", "td"], 
                                         attrs={"title": True})
        
        for elem in timestamp_elements:
            # Check if this element or its parent contains message content
            parent = elem.find_parent(["div", "tr", "td"])
            if parent:
                # Check if parent contains text content that looks like a message
                text_content = parent.get_text().strip()
                if (len(text_content) > 10 and  # Reasonable length for a message
                    not text_content.isdigit() and  # Not just numbers
                    any(word in text_content.lower() for word in ["text", "message", "sms", "call"])):
                    potential_messages.append(parent)
        
        if potential_messages:
            messages_raw = potential_messages
            logger.debug(f"Found {len(messages_raw)} potential messages using comprehensive fallback")

    if not messages_raw:
        logger.error(f"No messages found in SMS/MMS file: {html_file.name}")
        # Log HTML structure for debugging
        logger.debug(f"HTML structure preview for {html_file.name}: {str(soup)[:1000]}...")
        return {
            "num_sms": 0,
            "num_img": 0,
            "num_vcf": 0,
            "num_calls": 0,
            "num_voicemails": 0,
            "own_number": own_number,
        }
    
    # ENHANCED LOGGING: Log message count and structure for debugging
    if len(messages_raw) < 5:  # Log details for files with very few messages
        logger.info(f"File {html_file.name} has only {len(messages_raw)} messages - this might indicate missing older messages")
        for i, msg in enumerate(messages_raw):
            msg_text = msg.get_text().strip()[:100]  # First 100 chars
            logger.debug(f"  Message {i+1}: {msg_text}...")
            # Log HTML structure of this message element
            logger.debug(f"  Message {i+1} HTML: {str(msg)[:200]}...")

    # Use cached CSS selector for participants
    participants_raw = soup.select(STRING_POOL.CSS_SELECTORS["participants"])

    # Process messages more efficiently
    sms_count = 0
    mms_count = 0

    # Use cached selectors for better performance
    img_selector = STRING_POOL.ADDITIONAL_SELECTORS["img_src"]
    vcard_selector = STRING_POOL.ADDITIONAL_SELECTORS["vcard_links"]

    for message in messages_raw:
        # Check for attachments more efficiently
        has_images = bool(message.select_one(img_selector))
        has_vcards = bool(message.select_one(vcard_selector))

        if has_images or has_vcards:
            mms_count += 1
        else:
            sms_count += 1

    # Process all messages in a single pass. SMS will be written directly,
    # MMS will be forwarded by write_sms_messages to write_mms_messages.
    logger.debug(
        f"Processing {len(messages_raw)} messages (SMS and MMS) from {html_file.name}"
    )
    write_sms_messages(
        html_file.name,
        messages_raw,
        own_number,
        src_filename_map,
        page_participants_raw=participants_raw,
        soup=soup,
    )

    # NOTE: MMS messages with attachments are forwarded from write_sms_messages
    # to write_mms_messages to avoid double-processing.

    # Count attachments more efficiently
    img_count = len(soup.select(img_selector))
    vcf_count = len(soup.select(vcard_selector))

    return {
        "num_sms": sms_count,
        "num_img": img_count,
        "num_vcf": vcf_count,
        "num_calls": 0,
        "num_voicemails": 0,
        "own_number": own_number,
    }


@lru_cache(maxsize=100)
def parse_html_file_cached(html_file_str: str) -> BeautifulSoup:
    """
    Cached HTML file parsing for performance optimization.

    Args:
        html_file_str: String representation of HTML file path

    Returns:
        BeautifulSoup: Parsed HTML content
    """
    return parse_html_file(Path(html_file_str))


def parse_html_file(html_file: Path) -> BeautifulSoup:
    """Parse HTML file and return BeautifulSoup object with optimized I/O."""
    try:
        file_size = html_file.stat().st_size

        # Use memory mapping for large files to improve performance
        if ENABLE_MMAP_FOR_LARGE_FILES and file_size > MMAP_THRESHOLD:
            with open(html_file, "rb") as f:
                with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:
                    content = mm.read().decode("utf-8")
        else:
            # Use optimized buffer size for smaller files
            with open(
                html_file, "r", encoding="utf-8", buffering=FILE_READ_BUFFER_SIZE
            ) as f:
                content = f.read()

        return BeautifulSoup(content, HTML_PARSER)
    except Exception as e:
        raise FileProcessingError(f"Failed to parse HTML file {html_file}: {e}")


@lru_cache(maxsize=15000)
def count_attachments_in_file_cached(html_file_str: str, extensions_str: str) -> int:
    """
    Cached attachment counting for performance optimization.

    Args:
        html_file_str: String representation of HTML file path
        extensions_str: String representation of extensions set

    Returns:
        int: Number of attachments found
    """
    # Convert strings back to original types
    html_file = Path(html_file_str)
    extensions = set(extensions_str.split(","))

    try:
        # Cache directory listings to avoid repeated iterdir() calls
        dir_path = str(html_file.parent)
        if not hasattr(count_attachments_in_file_cached, "_dir_cache"):
            count_attachments_in_file_cached._dir_cache = {}

        if dir_path not in count_attachments_in_file_cached._dir_cache:
            # Use more efficient directory scanning with set comprehension
            count_attachments_in_file_cached._dir_cache[dir_path] = {
                path.suffix.lower()
                for path in html_file.parent.iterdir()
                if path.is_file() and path.suffix.lower() in extensions
            }

        file_extensions = count_attachments_in_file_cached._dir_cache[dir_path]
        # Use set intersection for faster matching
        return len(file_extensions & extensions)
    except Exception:
        return 0


def count_attachments_in_file(html_file: Path, extensions: set) -> int:
    """
    Count attachments of specific types in a file's directory.

    Args:
        html_file: Path to HTML file
        extensions: Set of file extensions to count

    Returns:
        int: Number of attachments found
    """
    # Use cached version for better performance
    html_file_str = str(html_file)
    extensions_str = ",".join(sorted(extensions))
    return count_attachments_in_file_cached(html_file_str, extensions_str)


def write_sms_messages(
    file: str,
    messages_raw: List,
    own_number: Optional[str],
    src_filename_map: Dict[str, str],
    page_participants_raw: Optional[List] = None,
    soup: Optional[BeautifulSoup] = None,
):
    """
    Write SMS messages to conversation files.

    Args:
        file: HTML filename being processed
        messages_raw: List of message elements from HTML
        own_number: User's phone number
        src_filename_map: Mapping of src elements to filenames
    """
    try:
        fallback_number = extract_fallback_number(file)

        # Check if file should be skipped based on filename patterns
        if should_skip_file(file):
            logger.info(f"Skipping file with invalid phone number pattern: {file}")
            return

        # Get the primary phone number for this conversation
        phone_number, participant_raw = get_first_phone_number(
            messages_raw, fallback_number
        )

        # Search for fallback numbers in similarly named files if needed
        if phone_number == 0:
            phone_number = search_fallback_numbers(file, fallback_number)

        # Skip processing if we still can't get a valid phone number
        if not is_valid_phone_number(phone_number):
            # Fallback 1: try to derive from page-level participants
            if page_participants_raw:
                try:
                    participants, _aliases = get_participant_phone_numbers_and_aliases(
                        page_participants_raw
                    )
                    # Prefer a participant that is not own_number
                    for p in participants:
                        if not own_number or p != own_number:
                            phone_number = p
                            participant_raw = create_dummy_participant(p)
                            break
                except Exception:
                    pass
            
            # Fallback 2: parse the source HTML file for any tel: numbers
            if not is_valid_phone_number(phone_number):
                try:
                    html_path = PROCESSING_DIRECTORY / "Calls" / file
                    if html_path.exists():
                        with open(html_path, "r", encoding="utf-8") as f:
                            soup_all = BeautifulSoup(f.read(), HTML_PARSER)
                        for link in soup_all.find_all("a", href=True):
                            href = link.get("href", "")
                            if href.startswith("tel:"):
                                m = TEL_HREF_PATTERN.search(href)
                                candidate = m.group(1) if m else ""
                                if candidate:
                                    try:
                                        formatted = format_number(
                                            phonenumbers.parse(candidate, None)
                                        )
                                        if not own_number or formatted != own_number:
                                            phone_number = formatted
                                            participant_raw = create_dummy_participant(
                                                phone_number
                                            )
                                            break
                                    except Exception:
                                        continue
                except Exception:
                    pass
            
            # Fallback 3: Extract phone number from filename if it contains one
            if not is_valid_phone_number(phone_number):
                try:
                    phone_match = re.search(r"(\+\d{1,3}\s?\d{1,14})", file)
                    if phone_match:
                        phone_number = format_number(
                            phonenumbers.parse(phone_match.group(1), None)
                        )
                        participant_raw = create_dummy_participant(phone_number)
                        logger.debug(f"Extracted phone number from filename: {phone_number}")
                except Exception as e:
                    logger.debug(f"Failed to extract phone number from filename: {e}")
            
            # Fallback 4: Look for any phone numbers in the entire HTML content
            if not is_valid_phone_number(phone_number):
                try:
                    # If we have soup parameter, use it directly
                    if soup is not None:
                        tel_links = soup.find_all("a", href=True)
                        for link in tel_links:
                            href = link.get("href", "")
                            if href.startswith("tel:"):
                                match = TEL_HREF_PATTERN.search(href)
                                if match:
                                    try:
                                        phone_number = format_number(
                                            phonenumbers.parse(match.group(1), None)
                                        )
                                        if not own_number or phone_number != own_number:
                                            participant_raw = create_dummy_participant(phone_number)
                                            logger.debug(f"Extracted phone number from soup: {phone_number}")
                                            break
                                    except Exception as e:
                                        logger.debug(f"Failed to parse phone number from soup: {e}")
                                        continue
                    else:
                        # Fallback to scanning HTML files
                        html_files = list(PROCESSING_DIRECTORY.rglob("*.html"))
                        for html_file in html_files:
                            if html_file.name == file:
                                with open(html_file, "r", encoding="utf-8") as f:
                                    soup_content = BeautifulSoup(f.read(), HTML_PARSER)
                                
                                # Look for any tel: links in the entire document
                                tel_links = soup_content.find_all("a", href=True)
                                for link in tel_links:
                                    href = link.get("href", "")
                                    if href.startswith("tel:"):
                                        match = TEL_HREF_PATTERN.search(href)
                                        if match:
                                            try:
                                                phone_number = format_number(
                                                    phonenumbers.parse(match.group(1), None)
                                                )
                                                if not own_number or phone_number != own_number:
                                                    participant_raw = create_dummy_participant(phone_number)
                                                    logger.debug(f"Extracted phone number from HTML content: {phone_number}")
                                                    break
                                            except Exception as e:
                                                logger.debug(f"Failed to parse phone number from HTML: {e}")
                                                continue
                                break
                except Exception as e:
                    logger.debug(f"Failed to scan HTML content for phone numbers: {e}")
            
                    # Fallback 5: Use a default conversation ID for unknown numbers
        if not is_valid_phone_number(phone_number):
            logger.error(
                f"Could not determine valid phone number for {file}, using fallback conversation ID"
            )
            logger.debug(f"Fallback conversation ID will be created from filename hash")
            logger.debug(f"Phone number that failed validation: '{phone_number}'")
            # Create a unique conversation ID based on filename
            phone_number = f"unknown_{hash(file) % 1000000}"
            participant_raw = create_dummy_participant(phone_number)
        
        # Final validation check with enhanced debugging
        if not is_valid_phone_number(phone_number):
            logger.error(
                f"Could not determine valid phone number for {file}, skipping SMS processing"
            )
            logger.debug(f"Final phone number that failed validation: '{phone_number}'")
            logger.debug(f"File type: {get_file_type(file)}")
            logger.debug(f"Fallback number used: {fallback_number}")
            return

        # Get total message count for progress tracking
        total_messages = len(messages_raw)
        if total_messages == 0:
            logger.error(f"No messages to process in {file}")
            return

        logger.info(f"Processing {total_messages} SMS messages from {file}")
        
        # ENHANCED: Warn about potentially large files that might take time
        if total_messages > 1000:
            logger.info(f"Large file detected ({total_messages} messages) - this may take significant time to process")
        elif total_messages > 100:
            logger.info(f"Medium file detected ({total_messages} messages) - processing in progress")

        # Calculate progress reporting thresholds
        progress_interval = max(
            1, min(50, total_messages // 10)
        )  # Every 10% or 50 messages, whichever is smaller
        last_reported_progress = 0
        processed_count = 0
        skipped_count = 0

        # Process SMS messages and write to conversation files
        for i, message in enumerate(messages_raw):
            try:
                # Check if message contains images or vCards (treat as MMS)
                if message.find_all("img") or message.find_all("a", class_="vcard"):
                    # Process as MMS instead of SMS. Prefer page-level participants
                    # to capture full group membership; fall back to per-message cite.
                    participants_context = (
                        page_participants_raw if page_participants_raw else [[participant_raw]]
                    )
                    write_mms_messages(
                        file,
                        participants_context,
                        [message],
                        own_number,
                        src_filename_map,
                        soup=None,  # No soup available in SMS context
                    )
                    processed_count += 1  # Count as processed, not skipped
                    continue

                # Skip MMS placeholder messages
                message_content = get_message_text(message)
                if message_content in MMS_PLACEHOLDER_MESSAGES:
                    skipped_count += 1
                    continue

                # DATE FILTERING: Skip messages outside the specified date range
                message_timestamp = get_time_unix(message, file)
                if should_skip_message_by_date(message_timestamp):
                    skipped_count += 1
                    continue

                # PHONE FILTERING: Skip numbers without aliases if filtering is enabled
                if FILTER_NUMBERS_WITHOUT_ALIASES and PHONE_LOOKUP_MANAGER:
                    if not PHONE_LOOKUP_MANAGER.has_alias(str(phone_number)):
                        logger.debug(f"Skipping message from {phone_number} - no alias found and filtering enabled")
                        skipped_count += 1
                        continue
                    
                    # Also check if the number is explicitly excluded
                    if PHONE_LOOKUP_MANAGER.is_excluded(str(phone_number)):
                        exclusion_reason = PHONE_LOOKUP_MANAGER.get_exclusion_reason(str(phone_number))
                        logger.debug(f"Skipping message from {phone_number} - explicitly excluded: {exclusion_reason}")
                        skipped_count += 1
                        continue

                # NON-PHONE FILTERING: Skip toll-free and non-US numbers if filtering is enabled
                if FILTER_NON_PHONE_NUMBERS:
                    if not is_valid_phone_number(str(phone_number), filter_non_phone=True):
                        logger.debug(f"Skipping message from {phone_number} - toll-free or non-US number filtered out")
                        skipped_count += 1
                        continue

                # Get alias for the phone number, with filename-based fallback
                alias = PHONE_LOOKUP_MANAGER.get_alias(str(phone_number), None)
                
                # If no alias found, try to extract from filename
                if not alias and file and " - Text -" in file:
                    name_part = file.split(" - Text -")[0]
                    # Use the name part as alias if it looks like a person's name
                    if len(name_part.strip()) > 2 and not name_part.strip().isdigit():
                        alias = name_part.strip()
                        logger.debug(f"Using filename-based alias for SMS: {alias}")

                # Prepare SMS message data
                sms_values = {
                    "alias": alias,
                    "type": get_message_type(message),
                    "message": message_content,
                    "time": get_time_unix(message, file),
                }

                # Format SMS XML
                sms_text = format_sms_xml(sms_values)

                # Write to conversation file
                conversation_id = CONVERSATION_MANAGER.get_conversation_id(
                    [str(phone_number)], False
                )
                if CONVERSATION_MANAGER.output_format == "html":
                    # For HTML output, extract text and attachments directly
                    message_text = sms_values.get("message", "")
                    if not message_text or message_text.strip() == "":
                        message_text = "[Empty message]"
                    attachments = []
                    # Determine sender display for SMS
                    sender_display = "Me" if sms_values.get("type") == 2 else alias
                    CONVERSATION_MANAGER.write_message_with_content(
                        conversation_id,
                        message_text,
                        attachments,
                        sms_values["time"],
                        sender=sender_display,
                    )
                else:
                    # For XML output, use the XML format
                    CONVERSATION_MANAGER.write_message(
                        conversation_id, sms_text, sms_values["time"]
                    )

                processed_count += 1

                # Report progress
                current_progress = i + 1
                if (
                    current_progress % progress_interval == 0
                    or current_progress == total_messages
                    or current_progress - last_reported_progress >= 50
                ):

                    percentage = (current_progress / total_messages) * 100
                    logger.info(
                        f"SMS processing progress in {file}: {current_progress}/{total_messages} messages processed ({percentage:.1f}%) - "
                        f"Processed: {processed_count}, Skipped: {skipped_count}"
                    )
                    last_reported_progress = current_progress

            except Exception as e:
                logger.error(f"Failed to process SMS message: {e}")
                skipped_count += 1
                continue

        logger.info(
            f"Completed SMS processing for {file}. Processed: {processed_count}, Skipped: {skipped_count}"
        )

    except Exception as e:
        logger.error(f"Failed to write SMS messages for {file}: {e}")


@lru_cache(maxsize=25000)
def extract_fallback_number_cached(filename: str) -> Union[str, int]:
    """
    Cached fallback number extraction for performance optimization.

    Args:
        filename: Filename to extract number from

    Returns:
        Union[str, int]: Extracted number or 0 if not found
    """
    import re
    
    # PERFORMANCE OPTIMIZED STRATEGY ORDER: Most likely to succeed first
    
    # Strategy 1: Extract numeric service codes from start of filename (MOST COMMON)
    # Pattern: "262966 - Text - ..." or "12345 - Text - ..."
    numeric_code_match = re.match(r"^(\d{4,7})\s*-\s*", filename)
    if numeric_code_match:
        return int(numeric_code_match.group(1))

    # Strategy 2: Extract phone number from filename (international format)
    match = PHONE_NUMBER_PATTERN.search(filename)
    if match:
        # Remove any non-digit characters and convert to int
        phone_number = (
            match.group(1)
            .replace("+", "")
            .replace(" ", "")
            .replace("-", "")
            .replace("(", "")
            .replace(")", "")
        )
        # Only return if it's a reasonable length (at least 7 digits for international)
        if len(phone_number) >= 7:
            return int(phone_number)
        # If it's too short, continue to next strategy

    # Strategy 3: Extract any number from parentheses in the filename
    paren_match = re.search(r"\((\d+)\)", filename)
    if paren_match:
        return int(paren_match.group(1))
    
    # Strategy 4: Generate a hash-based fallback number for files without numbers
    # This ensures we can still process files like "Susan Nowak Tang - Text - ..."
    if " - Text - " in filename or " - Voicemail - " in filename:
        # Create a consistent hash-based number for the same name
        name_part = filename.split(" - ")[0]
        hash_value = abs(hash(name_part)) % 100000000  # 8-digit number, ensure positive
        # Ensure it's at least 8 digits by padding with zeros if needed
        if hash_value < 10000000:  # Less than 8 digits
            hash_value += 10000000  # Add 10 million to ensure 8 digits
        return hash_value

    # Strategy 5: Extract any sequence of digits from filename (last resort)
    # But only if it's not part of a phone number pattern
    if not PHONE_NUMBER_PATTERN.search(filename):
        digit_match = re.search(r"(\d{4,})", filename)
        if digit_match:
            return int(digit_match.group(1))

    return 0


# is_valid_phone_number function moved to utils.py


def is_legitimate_google_voice_export(filename: str) -> bool:
    """
    Check if a filename is a legitimate Google Voice export with file parts.
    
    Args:
        filename: Filename to check
        
    Returns:
        bool: True if filename is a legitimate Google Voice export, False otherwise
    """
    import re
    
    # Check for legitimate Google Voice export patterns with file parts
    for pattern in [" - Text - ", " - Voicemail - ", " - Received - ", " - Placed - ", " - Missed - "]:
        if pattern in filename:
            after_pattern = filename.split(pattern)[1]
            
            # Check for legitimate pattern: YYYY-MM-DDTHH_MM_SSZ-N-M
            # Where N and M are typically single digits (0-9) representing file parts/versions
            if re.search(r'^[0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}_[0-9]{2}_[0-9]{2}Z-[0-9]-[0-9]\.html$', after_pattern):
                return True
            # Also check without .html extension
            if re.search(r'^[0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}_[0-9]{2}_[0-9]{2}Z-[0-9]-[0-9]$', after_pattern):
                return True
            
            # ENHANCED: Check for legitimate pattern with any single-digit file parts
            # This handles cases like "PhilipLICW Abramovitz - LI Clean Water - Text - 2024-07-29T16_10_03Z-6-1"
            if re.search(r'^[0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}_[0-9]{2}_[0-9]{2}Z-[0-9]-[0-9]$', after_pattern):
                return True
            if re.search(r'^[0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}_[0-9]{2}_[0-9]{2}Z-[0-9]-[0-9]\.html$', after_pattern):
                return True
    
    return False


def should_skip_file(filename: str) -> bool:
    """
    Determine if a file should be skipped based on filename patterns and filtering settings.

    Args:
        filename: Filename to check

    Returns:
        bool: True if file should be skipped, False otherwise
    """
    # Skip files that start with " -" (no phone number)
    if filename.startswith(" -"):
        return True

    # Skip files that start with just a dash or space
    if filename.startswith("-") or filename.startswith(" "):
        return True

    # Skip only if filename contains invalid characters that suggest file corruption
    import re
    if re.search(r'[<>:"|?*]', filename):
        return True
    
    # Skip if filename is empty or just whitespace/punctuation
    if not filename.strip() or filename.strip() in ['-', '.', '_']:
        return True

    # ENHANCED CORRUPTION DETECTION: Check for malformed Google Voice filename patterns
    # Valid patterns: 
    #   "Name - Text - YYYY-MM-DDTHH_MM_SSZ.html"
    #   "Name - Voicemail - YYYY-MM-DDTHH_MM_SSZ.html"
    #   "Name - Received/Placed/Missed - YYYY-MM-DDTHH_MM_SSZ.html"
    # Corrupted patterns have extra parts like "-6-1" at the end
    
    # Check for empty or malformed name parts
    if filename.startswith(" - ") or filename.startswith("- "):
        logger.debug(f"Skipping filename with empty/malformed name part: {filename}")
        return True
    
    # Check if filename follows the expected Google Voice pattern
    for pattern in [" - Text - ", " - Voicemail - ", " - Received - ", " - Placed - ", " - Missed - "]:
        if pattern in filename:
            # Extract the part after the pattern
            after_pattern = filename.split(pattern)[1]
            
            # Check if the timestamp part is corrupted (has extra dashes/parts)
            if after_pattern.count("-") > 2:  # More than expected dashes in timestamp
                # But allow legitimate Google Voice export patterns
                if is_legitimate_google_voice_export(filename):
                    logger.debug(f"Allowing legitimate Google Voice export with file parts: {filename}")
                    break  # This is legitimate, continue processing
                else:
                    logger.debug(f"Skipping corrupted filename with extra parts: {filename}")
                    return True
            
            # Check if the timestamp part has unexpected characters or patterns
            # Allow both .html and no extension for legitimate patterns
            has_html_extension = after_pattern.endswith(".html")
            if has_html_extension:
                timestamp_part = after_pattern[:-5]  # Remove .html
            else:
                # Check if this is a legitimate Google Voice export without .html extension
                if is_legitimate_google_voice_export(filename):
                    timestamp_part = after_pattern  # No extension, but legitimate
                    logger.debug(f"Allowing legitimate Google Voice export without .html: {filename}")
                else:
                    # Missing .html extension and not a legitimate pattern
                    logger.debug(f"Skipping filename without .html extension: {filename}")
                    return True
            
            # Check for corrupted timestamp patterns (extra numbers/parts after timestamp)
            # Valid: "2024-07-29T16_10_03Z"
            # BUT allow legitimate Google Voice export patterns like "2024-07-29T16_10_03Z-6-1"
            if re.search(r'[0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}_[0-9]{2}_[0-9]{2}Z.*[0-9-]+', timestamp_part):
                # Check if this is a legitimate Google Voice export with file parts
                if is_legitimate_google_voice_export(filename):
                    logger.debug(f"Allowing legitimate Google Voice export with file parts: {filename}")
                    break  # This is legitimate, continue processing
                else:
                    logger.debug(f"Skipping corrupted filename with malformed timestamp: {filename}")
                    return True
            
            break  # Found a valid pattern, no need to check others

    # SERVICE CODE FILTERING: Skip numeric service codes unless explicitly enabled
    if not INCLUDE_SERVICE_CODES:
        # Pattern: "262966 - Text - ..." or "12345 - Text - ..."
        numeric_code_match = re.match(r"^(\d{4,7})\s*-\s*", filename)
        if numeric_code_match:
            number = numeric_code_match.group(1)
            # Skip if it's a service code (4-7 digits)
            if 4 <= len(number) <= 7:
                return True

    # Don't skip files that start with names, numbers (when enabled), or "Group Conversation"
    # These are legitimate conversation files that should be processed
    return False


def clean_corrupted_filename(filename: str) -> str:
    """
    Attempt to clean corrupted Google Voice filenames by removing extra parts.
    
    Args:
        filename: Potentially corrupted filename
        
    Returns:
        str: Cleaned filename or original if cleaning failed
    """
    import re
    
    # Check if this is a corrupted filename that can be cleaned
    for pattern in [" - Text - ", " - Voicemail - ", " - Received - ", " - Placed - ", " - Missed - "]:
        if pattern in filename:
            # Extract the parts
            parts = filename.split(pattern)
            if len(parts) == 2:
                name_part = parts[0]
                timestamp_part = parts[1]
                
                # Skip if name part is empty or just whitespace/dashes
                if not name_part.strip() or name_part.strip() in ['-', '.', '_']:
                    logger.debug(f"Cannot clean filename with empty name part: {filename}")
                    return filename
                
                # Remove .html extension if present
                has_html_extension = timestamp_part.endswith(".html")
                if has_html_extension:
                    timestamp_part = timestamp_part[:-5]
                
                # Look for the valid timestamp pattern and preserve legitimate Google Voice export patterns
                # Valid pattern: YYYY-MM-DDTHH_MM_SSZ
                valid_timestamp_match = re.search(r'[0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}_[0-9]{2}_[0-9]{2}Z', timestamp_part)
                if valid_timestamp_match:
                    clean_timestamp = valid_timestamp_match.group(0)
                    
                    # Check if this is a legitimate Google Voice export with file parts (e.g., "-6-1")
                    # Only preserve single-digit patterns like "-6-1", not multi-digit like "-123-456"
                    file_parts_match = re.search(r'[0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}_[0-9]{2}_[0-9]{2}Z(-[0-9]-[0-9])', timestamp_part)
                    if file_parts_match:
                        # Preserve the legitimate file parts (single digits only)
                        file_parts = file_parts_match.group(1)
                        clean_timestamp += file_parts
                    
                    # Add .html extension if the original had it
                    extension = ".html" if has_html_extension else ""
                    cleaned_filename = f"{name_part}{pattern}{clean_timestamp}{extension}"
                    logger.debug(f"Cleaned corrupted filename: '{filename}' -> '{cleaned_filename}'")
                    return cleaned_filename
    
    # Return original if cleaning failed
    return filename


def is_corrupted_filename(filename: str) -> bool:
    """
    Check if a filename appears to be corrupted based on Google Voice export patterns.
    
    Args:
        filename: Filename to check
        
    Returns:
        bool: True if filename appears corrupted, False otherwise
    """
    import re
    
    # Check for empty or malformed name parts
    if filename.startswith(" - ") or filename.startswith("- "):
        return True
    
    # Check for common corruption patterns
    for pattern in [" - Text - ", " - Voicemail - ", " - Received - ", " - Placed - ", " - Missed - "]:
        if pattern in filename:
            after_pattern = filename.split(pattern)[1]
            
            # First check if this is a legitimate Google Voice export with file parts
            if is_legitimate_google_voice_export(filename):
                return False
            
            # Check for extra dashes in timestamp part (but allow legitimate file parts)
            if after_pattern.count("-") > 2:
                # Allow legitimate patterns like "2024-07-29T16_10_03Z-6-1"
                if not re.search(r'[0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}_[0-9]{2}_[0-9]{2}Z-\d+-\d+', after_pattern):
                    return True
            
            # Check for corrupted timestamp with extra parts
            if re.search(r'[0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}_[0-9]{2}_[0-9]{2}Z.*[0-9-]+', after_pattern):
                # If it doesn't match legitimate patterns, it's corrupted
                return True
            
            # Check for missing .html extension
            if not after_pattern.endswith(".html"):
                return True
            
            break  # Found a valid pattern, no need to check others
    
    return False


def should_skip_message_by_date(message_timestamp: int) -> bool:
    """
    Determine if a message should be skipped based on date filtering settings.

    Args:
        message_timestamp: Unix timestamp in milliseconds

    Returns:
        bool: True if message should be skipped due to date filtering, False otherwise
    """
    if DATE_FILTER_OLDER_THAN is None and DATE_FILTER_NEWER_THAN is None:
        return False  # No date filtering enabled
    
    # Convert timestamp to datetime for comparison
    try:
        message_date = datetime.fromtimestamp(message_timestamp / 1000.0)
        
        # Check older-than filter
        if DATE_FILTER_OLDER_THAN and message_date < DATE_FILTER_OLDER_THAN:
            return True
        
        # Check newer-than filter
        if DATE_FILTER_NEWER_THAN and message_date > DATE_FILTER_NEWER_THAN:
            return True
        
        return False
    except Exception as e:
        logger.error(f"Failed to parse message timestamp {message_timestamp} for date filtering: {e}")
        logger.debug(f"Message will not be filtered by date - treating as valid")
        return False  # Don't skip if we can't parse the timestamp


def extract_fallback_number(file: str) -> Union[str, int]:
    """
    Extract fallback phone number from filename.

    Args:
        file: Filename to extract number from

    Returns:
        Union[str, int]: Extracted number or 0 if not found
    """
    # Use cached version for better performance
    return extract_fallback_number_cached(file)


@lru_cache(maxsize=50000)
def format_sms_xml_cached(alias: str, time: int, type_val: int, message: str) -> str:
    """
    Cached SMS XML formatting for performance optimization.

    Args:
        alias: Phone number alias
        time: Timestamp
        type_val: Message type
        message: Message text

    Returns:
        str: Formatted SMS XML
    """
    return SMS_XML_TEMPLATE.format(
        alias=alias, time=time, type=type_val, message=message
    )


def format_sms_xml(sms_values: Dict[str, Union[str, int]]) -> str:
    """
    Format SMS message values into XML.

    Args:
        sms_values: Dictionary containing SMS message data

    Returns:
        str: Formatted SMS XML
    """
    # Use cached version for better performance
    return format_sms_xml_cached(
        str(sms_values["alias"]),
        int(sms_values["time"]),
        int(sms_values["type"]),
        str(sms_values["message"]),
    )


@lru_cache(maxsize=25000)
def search_fallback_numbers_cached(file: str, fallback_number: str) -> Union[str, int]:
    """
    Cached fallback number search for performance optimization.

    Args:
        file: Filename to search for fallback numbers
        fallback_number: Fallback number to search for

    Returns:
        Union[str, int]: Found fallback number or 0
    """
    # This function is a placeholder for future caching optimization
    # For now, we'll use the non-cached version
    return fallback_number


def search_fallback_numbers(
    file: str, fallback_number: Union[str, int]
) -> Union[str, int]:
    """
    Search for fallback numbers in similarly named files.

    Args:
        file: Filename to search for fallback numbers
        fallback_number: Fallback number to search for

    Returns:
        Union[str, int]: Found fallback number or 0
    """
    try:
        # Extract base filename without extension
        base_filename = Path(file).stem

        # Search for files with similar names in the processing directory
        search_pattern = str(PROCESSING_DIRECTORY / f"{base_filename}*.html")
        similar_files = glob.glob(search_pattern)

        for similar_file in similar_files:
            similar_path = Path(similar_file)
            if similar_path.name != file:
                try:
                    with open(similar_path, "r", encoding="utf-8") as f:
                        soup = BeautifulSoup(f, HTML_PARSER)
                        messages = soup.find_all(class_="message")

                        for message in messages:
                            try:
                                cite_element = message.cite
                                if not cite_element or not cite_element.a:
                                    continue

                                href = cite_element.a.get("href", "")
                                # Use pre-compiled regex for better performance
                                match = TEL_HREF_PATTERN.search(href)
                                number_text = match.group(1) if match else ""
                                if (
                                    number_text
                                    and len(number_text) >= MIN_PHONE_NUMBER_LENGTH
                                ):
                                    try:
                                        phone_number = format_number(
                                            phonenumbers.parse(number_text, None)
                                        )
                                        return phone_number
                                    except phonenumbers.phonenumberutil.NumberParseException:
                                        continue

                            except Exception:
                                continue

                except Exception:
                    continue

        return fallback_number

    except Exception as e:
        logger.error(f"Failed to search fallback numbers: {e}")
        return fallback_number


def search_files_for_phone_number(
    pattern: str, search_type: str, original_file: str
) -> Union[str, int]:
    """Search files matching a pattern for phone numbers."""
    try:
        for fallback_file in (PROCESSING_DIRECTORY).glob(f"**/{pattern}"):
            try:
                with fallback_file.open("r", encoding="utf8") as ff:
                    soup = BeautifulSoup(ff, HTML_PARSER)

                    if search_type == "message":
                        messages_raw = soup.find_all(class_="message")
                        phone_number, _ = get_first_phone_number(messages_raw, 0)
                        if phone_number != 0:
                            return phone_number

                    elif search_type == "contributor vcard":
                        contrib_vcards = soup.find_all(class_="contributor vcard")
                        for contrib_vcard in contrib_vcards:
                            if contrib_vcard.a and "href" in contrib_vcard.a.attrs:
                                href = contrib_vcard.a["href"]
                                # Use pre-compiled regex for better performance
                                match = TEL_HREF_PATTERN.search(href)
                                phone_number_ff = match.group(1) if match else ""
                            if phone_number_ff:
                                phone_number, _ = get_first_phone_number(
                                    [], phone_number_ff
                                )
                            if phone_number != 0:
                                return phone_number

            except Exception as e:
                logger.error(f"Failed to search fallback file {fallback_file}: {e}")
                continue

    except Exception as e:
        logger.error(f"Failed to search pattern {pattern}: {e}")

    return 0


def write_mms_messages(
    file: str,
    participants_raw: List,
    messages_raw: List,
    own_number: Optional[str],
    src_filename_map: Dict[str, str],
    soup: Optional[BeautifulSoup] = None,
):
    """
    Write MMS messages to the backup file.

    Args:
        file: HTML filename being processed
        participants_raw: List of participant elements from HTML
        messages_raw: List of message elements from HTML
        own_number: User's phone number
        src_filename_map: Mapping of src elements to filenames
    """
    try:
        # Get total message count for progress tracking
        total_messages = len(messages_raw)
        if total_messages == 0:
            logger.error(f"No MMS messages to process in {file}")
            return

        logger.info(f"Processing {total_messages} MMS messages from {file}")

        # Calculate progress reporting thresholds
        progress_interval = max(
            1, min(50, total_messages // 10)
        )  # Every 10% or 50 messages, whichever is smaller
        last_reported_progress = 0
        processed_count = 0
        skipped_count = 0

        # Extract participant phone numbers and aliases
        participants, participant_aliases = get_participant_phone_numbers_and_aliases(
            participants_raw
        )
        if not participants:
            # Try to extract participants from the messages themselves as fallback
            logger.debug(
                f"No participants found in participants_raw, trying to extract from messages for {file}"
            )

            # Look for phone numbers in the messages
            fallback_participants = []
            fallback_aliases = []
            for message in messages_raw:
                try:
                    cite_element = message.cite
                    if cite_element and cite_element.a:
                        href = cite_element.a.get("href", "")
                        if href.startswith("tel:"):
                            match = TEL_HREF_PATTERN.search(href)
                            if match:
                                phone_number = match.group(1)
                                # Skip "Me" messages - check the text content
                                anchor_text = cite_element.a.get_text().strip()
                                if anchor_text == "Me":
                                    continue  # Skip user's own messages

                                try:
                                    parsed_number = phonenumbers.parse(
                                        phone_number, None
                                    )
                                    formatted_number = format_number(parsed_number)
                                    if formatted_number not in fallback_participants:
                                        fallback_participants.append(formatted_number)
                                        # Try to get alias from the cite element
                                        html_alias = ""
                                        fn_element = cite_element.find(
                                            ["span", "abbr"], class_="fn"
                                        )
                                        if fn_element:
                                            html_alias = fn_element.get_text(strip=True)
                                        elif anchor_text != phone_number:
                                            html_alias = anchor_text

                                        # Use existing alias if available, otherwise use HTML alias
                                        try:
                                            from sms import PHONE_LOOKUP_MANAGER

                                            if (
                                                PHONE_LOOKUP_MANAGER
                                                and formatted_number
                                                in PHONE_LOOKUP_MANAGER.phone_aliases
                                            ):
                                                fallback_aliases.append(
                                                    PHONE_LOOKUP_MANAGER.phone_aliases[
                                                        formatted_number
                                                    ]
                                                )
                                            else:
                                                fallback_aliases.append(
                                                    html_alias
                                                    if html_alias
                                                    else formatted_number
                                                )
                                        except (ImportError, AttributeError):
                                            fallback_aliases.append(
                                                html_alias
                                                if html_alias
                                                else formatted_number
                                            )
                                except phonenumbers.phonenumberutil.NumberParseException:
                                    if phone_number not in fallback_participants:
                                        fallback_participants.append(phone_number)
                                        fallback_aliases.append(phone_number)
                except Exception as e:
                    logger.debug(f"Failed to extract participant from message: {e}")
                    continue

            if fallback_participants:
                participants = fallback_participants
                participant_aliases = fallback_aliases
                logger.info(
                    f"Extracted {len(participants)} participants from messages for {file}"
                )
            else:
                # Enhanced fallback: try to extract participants from the entire HTML document
                logger.debug(f"Message-level participant extraction failed for {file}, trying document-level extraction")
                
                # Look for any participant information in the entire document
                all_participants = []
                all_aliases = []
                
                # Strategy 1: Look for any tel: links in the document (only if soup is available)
                if soup is not None:
                    try:
                        tel_links = soup.find_all("a", href=True)
                        for link in tel_links:
                            href = link.get("href", "")
                            if href.startswith("tel:"):
                                match = TEL_HREF_PATTERN.search(href)
                                if match:
                                    try:
                                        phone_number = format_number(
                                            phonenumbers.parse(match.group(1), None)
                                        )
                                        if phone_number not in all_participants:
                                            all_participants.append(phone_number)
                                            # Try to get alias from link text
                                            link_text = link.get_text(strip=True)
                                            alias = link_text if link_text and link_text != phone_number else phone_number
                                            all_aliases.append(alias)
                                    except Exception as e:
                                        logger.debug(f"Failed to parse phone number from tel link: {e}")
                                        continue
                    except Exception as e:
                        logger.debug(f"Failed to extract tel links from soup: {e}")
                
                # Strategy 2: Look for any phone numbers in text content (only if soup is available)
                if not all_participants and soup is not None:
                    try:
                        text_content = soup.get_text()
                        phone_pattern = re.compile(r"(\+\d{1,3}\s?\d{1,14})")
                        phone_matches = phone_pattern.findall(text_content)
                        for match in phone_matches:
                            try:
                                phone_number = format_number(
                                    phonenumbers.parse(match, None)
                                )
                                if phone_number not in all_participants:
                                    all_participants.append(phone_number)
                                    all_aliases.append(phone_number)
                            except Exception as e:
                                logger.debug(f"Failed to parse phone number from text: {e}")
                                continue
                    except Exception as e:
                        logger.debug(f"Failed to extract text content from soup: {e}")
                
                # Strategy 3: Use filename as fallback if it contains a phone number
                if not all_participants:
                    phone_match = re.search(r"(\+\d{1,3}\s?\d{1,14})", file)
                    if phone_match:
                        try:
                            phone_number = format_number(
                                phonenumbers.parse(phone_match.group(1), None)
                            )
                            all_participants.append(phone_number)
                            all_aliases.append(phone_number)
                        except Exception as e:
                            logger.debug(f"Failed to parse phone number from filename: {e}")
                
                if all_participants:
                    participants = all_participants
                    participant_aliases = all_aliases
                    logger.info(
                        f"Extracted {len(participants)} participants from document-level analysis for {file}"
                    )
                else:
                    # Final fallback: try to extract from filename patterns and create default participants
                    logger.debug(f"Document-level extraction failed for {file}, trying filename-based fallback")
                    
                    # Strategy 4: Extract from filename patterns (e.g., "Susan Nowak Tang - Text - 2025-08-13T12_08_52Z.html")
                    filename_participants = []
                    filename_aliases = []
                    
                    # Enhanced filename parsing for various patterns
                    if " - Text -" in file:
                        name_part = file.split(" - Text -")[0]
                        
                        # Try to extract phone numbers from the name part first
                        phone_matches = re.findall(r"(\+\d{1,3}\s?\d{1,3}\s?\d{1,4}\s?\d{1,4})", name_part)
                        for phone_match in phone_matches:
                            try:
                                phone_number = format_number(phonenumbers.parse(phone_match, None))
                                if phone_number not in filename_participants:
                                    filename_participants.append(phone_number)
                                    # Use the name part as alias
                                    filename_aliases.append(name_part.strip())
                            except Exception as e:
                                logger.debug(f"Failed to parse phone number from filename: {e}")
                                continue
                        
                        # If no phone numbers found, try to extract from other patterns
                        if not filename_participants:
                            # Look for patterns like "Name +1234567890" or "Name (123) 456-7890"
                            phone_patterns = [
                                r"([^+]*?)\s*\+(\d{1,3}\s?\d{1,3}\s?\d{1,4}\s?\d{1,4})",  # Name +phone
                                r"([^\(]*?)\s*\((\d{3})\)\s*(\d{3})-(\d{4})",  # Name (123) 456-7890
                                r"([^0-9]*?)\s*(\d{3})\s*(\d{3})\s*(\d{4})",  # Name 123 456 7890
                            ]
                            
                            for pattern in phone_patterns:
                                matches = re.findall(pattern, name_part)
                                for match in matches:
                                    try:
                                        if len(match) == 2:  # Name +phone format
                                            name, phone = match
                                            phone_number = format_number(phonenumbers.parse(phone, None))
                                        elif len(match) == 4:  # (123) 456-7890 or 123 456 7890 format
                                            name = match[0]
                                            phone = "".join(match[1:])
                                            phone_number = format_number(phonenumbers.parse(phone, None))
                                        else:
                                            continue
                                        
                                        if phone_number not in filename_participants:
                                            filename_participants.append(phone_number)
                                            filename_aliases.append(name.strip() if name.strip() else phone_number)
                                    except Exception as e:
                                        logger.debug(f"Failed to parse phone number from filename pattern: {e}")
                                        continue
                        
                        # If still no phone numbers, use the name as a participant identifier
                        if not filename_participants:
                            # Use the actual name directly as the participant identifier
                            # This creates more meaningful conversation filenames
                            clean_name = name_part.strip()
                            if clean_name and len(clean_name) > 0:
                                # Replace problematic characters for filename safety
                                safe_name = clean_name.replace(' ', '_').replace('/', '_').replace('\\', '_')
                                filename_participants = [safe_name]
                                filename_aliases.append(clean_name)
                                logger.debug(f"Created name-based participant for {file}: {clean_name}")
                    
                    # Strategy 5: If still no participants, create a default conversation
                    if not filename_participants:
                        logger.debug(f"Filename-based extraction failed for {file}, creating default conversation")
                        
                        # Create a unique conversation ID based on filename
                        default_phone = f"default_{hash(file) % 1000000}"
                        filename_participants = [default_phone]
                        filename_aliases = [file.split(" - Text -")[0] if " - Text -" in file else "Unknown"]
                        
                        logger.info(f"Created default participant for {file}: {default_phone}")
                    
                    participants = filename_participants
                    participant_aliases = filename_aliases
                    logger.info(f"Using fallback participants for {file}: {participants}")

        # Process MMS messages and write to conversation files
        for i, message in enumerate(messages_raw):
            try:
                # Skip MMS placeholder messages
                message_content = get_message_text(message)
                if message_content in MMS_PLACEHOLDER_MESSAGES:
                    skipped_count += 1
                    continue

                # DATE FILTERING: Skip messages outside the specified date range
                message_timestamp = get_time_unix(message, file)
                if should_skip_message_by_date(message_timestamp):
                    skipped_count += 1
                    continue

                # PHONE FILTERING: Skip numbers without aliases if filtering is enabled
                if FILTER_NUMBERS_WITHOUT_ALIASES and PHONE_LOOKUP_MANAGER:
                    # Check if any participant should be filtered out
                    should_skip = False
                    for phone in participants:
                        if not PHONE_LOOKUP_MANAGER.has_alias(str(phone)):
                            logger.debug(f"Skipping MMS from {phone} - no alias found and filtering enabled")
                            should_skip = True
                            break
                        
                        if PHONE_LOOKUP_MANAGER.is_excluded(str(phone)):
                            exclusion_reason = PHONE_LOOKUP_MANAGER.get_exclusion_reason(str(phone))
                            logger.debug(f"Skipping MMS from {phone} - explicitly excluded: {exclusion_reason}")
                            should_skip = True
                            break
                    
                    if should_skip:
                        skipped_count += 1
                        continue

                # NON-PHONE FILTERING: Skip toll-free and non-US numbers if filtering is enabled
                if FILTER_NON_PHONE_NUMBERS:
                    should_skip = False
                    for phone in participants:
                        if not is_valid_phone_number(str(phone), filter_non_phone=True):
                            logger.debug(f"Skipping MMS from {phone} - toll-free or non-US number filtered out")
                            should_skip = True
                            break
                    
                    if should_skip:
                        skipped_count += 1
                        continue

                # Determine sender and message type
                sender = get_mms_sender(message, participants)
                sent_by_me = sender == own_number

                # Use extracted aliases, but fall back to phone lookup if needed
                final_aliases = []
                for participant_idx, phone in enumerate(participants):
                    if participant_idx < len(participant_aliases) and participant_aliases[participant_idx]:
                        final_aliases.append(participant_aliases[participant_idx])
                    else:
                        # Fall back to phone lookup
                        alias = PHONE_LOOKUP_MANAGER.get_alias(phone, None)
                        final_aliases.append(alias)

                # Determine if message has images or vCards
                has_images = bool(message.find_all("img"))
                has_vcards = bool(message.find_all("a", class_="vcard"))
                text_only = "1" if not has_images and not has_vcards else "0"

                # Build MMS XML
                mms_xml = build_mms_xml(
                    participants_text=",".join(final_aliases),
                    message_time=get_time_unix(message, file),
                    m_type=MMS_TYPE_SENT if sent_by_me else MMS_TYPE_RECEIVED,
                    msg_box=MESSAGE_BOX_SENT if sent_by_me else MMS_TYPE_RECEIVED,
                    text_part=TEXT_PART_TEMPLATE.format(text=message_content)
                    if message_content
                    else "",
                    image_parts=build_image_parts(message, src_filename_map)
                    if has_images
                    else "",
                    vcard_parts=build_vcard_parts(message, src_filename_map)
                    if has_vcards
                    else "",
                    participants_xml=build_participants_xml(
                        participants, sender, sent_by_me
                    ),
                    text_only=text_only,
                )

                # Write to conversation file
                conversation_id = CONVERSATION_MANAGER.get_conversation_id(
                    participants, True
                )
                if CONVERSATION_MANAGER.output_format == "html":
                    # For HTML output, extract text and attachments directly
                    message_text = message_content
                    if not message_text or message_text.strip() == "":
                        message_text = "[Empty message]"

                    # Create attachments with proper links
                    attachments = []
                    if has_images:
                        # Find the actual image filename for linking
                        img_src = message.find("img", src=True)
                        if img_src:
                            img_filename = img_src.get("src")
                            if img_filename in src_filename_map:
                                actual_filename = src_filename_map[img_filename]
                                if actual_filename != "No unused match found":
                                    # Create clickable link to the image
                                    attachments.append(
                                        f"<a href='attachments/{actual_filename}' target='_blank'>üì∑ Image</a>"
                                    )
                                else:
                                    attachments.append("üì∑ Image (file not found)")
                            else:
                                attachments.append("üì∑ Image (unmapped)")
                        else:
                            attachments.append("üì∑ Image")

                    if has_vcards:
                        attachments.append("üìá vCard")

                    # Sender for MMS: prefer page alias, then phone lookup alias
                    sender_display = sender
                    try:
                        # Prefer alias from participant_aliases if available
                        if sender in participants:
                            idx = participants.index(sender)
                            if idx < len(participant_aliases) and participant_aliases[idx]:
                                sender_display = participant_aliases[idx]
                        # Fall back to phone lookup alias
                        if PHONE_LOOKUP_MANAGER:
                            sender_display = PHONE_LOOKUP_MANAGER.get_alias(sender_display, None)
                    except Exception:
                        pass
                    CONVERSATION_MANAGER.write_message_with_content(
                        conversation_id,
                        message_text,
                        attachments,
                        get_time_unix(message),
                        sender=sender_display,
                    )
                else:
                    # For XML output, use the XML format
                    CONVERSATION_MANAGER.write_message(
                        conversation_id, mms_xml, get_time_unix(message)
                    )

                processed_count += 1

                # Report progress
                current_progress = i + 1
                if (
                    current_progress % progress_interval == 0
                    or current_progress == total_messages
                    or current_progress - last_reported_progress >= 50
                ):

                    percentage = (current_progress / total_messages) * 100
                    logger.info(
                        f"MMS processing progress in {file}: {current_progress}/{total_messages} messages processed ({percentage:.1f}%) - "
                        f"Processed: {processed_count}, Skipped: {skipped_count}"
                    )
                    last_reported_progress = current_progress

            except Exception as e:
                # Provide more detailed error information for debugging
                if "Unable to determine sender" in str(e):
                    logger.error(
                        f"Failed to process MMS message (sender determination issue): {e}"
                    )
                    logger.debug(f"Message HTML structure: {message}")
                    logger.debug(f"Participants: {participants}")
                else:
                    logger.error(f"Failed to process MMS message: {e}")
                skipped_count += 1
                continue

        logger.info(
            f"Completed MMS processing for {file}. Processed: {processed_count}, Skipped: {skipped_count}"
        )

    except Exception as e:
        logger.error(f"Failed to write MMS messages for {file}: {e}")


def process_attachments(
    attachments: List, file: str, src_filename_map: Dict[str, str], attachment_type: str
) -> Tuple[str, Optional[str]]:
    """
    Process attachments and return XML parts and extracted URL.

    Args:
        attachments: List of attachment elements
        file: HTML filename being processed
        src_filename_map: Mapping of src elements to filenames
        attachment_type: Type of attachment ('image' or 'vcard')

    Returns:
        tuple: (XML parts string, extracted URL or None)
    """
    parts = ""
    extracted_url = None

    for attachment in attachments:
        src = attachment.get("src" if attachment_type == "image" else "href", "")
        if src in src_filename_map:
            filename = src_filename_map[src]
            if filename != "No unused match found":
                file_path = Path(filename)
                if file_path.exists():
                    try:
                        if attachment_type == "image":
                            content_type = f"image/{get_image_type(file_path)}"
                            data = encode_file_content(file_path)
                            if data:
                                parts += IMAGE_PART_TEMPLATE.format(
                                    type=content_type, name=filename, data=data
                                )
                        else:  # vcard
                            data = encode_file_content(file_path)
                            if data:
                                parts += VCARD_PART_TEMPLATE.format(
                                    type="text/vcard", name=filename, data=data
                                )

                        # Extract URL for location pins
                        if not extracted_url:
                            extracted_url = src

                    except Exception as e:
                        logger.error(
                            f"Failed to process {attachment_type} {filename}: {e}"
                        )

    return parts, extracted_url


def build_image_parts(message: BeautifulSoup, src_filename_map: Dict[str, str]) -> str:
    """
    Build XML parts for image attachments in an MMS message.

    Args:
        message: Message element from HTML
        src_filename_map: Mapping of src elements to filenames

    Returns:
        str: XML string for image parts
    """
    image_parts = ""
    # Cache find_all result to avoid repeated DOM traversal
    images = message.find_all("img")

    if not images:
        return image_parts

    for img in images:
        src = img.get("src", "")
        logger.debug(f"Processing image with src: {src}")
        if src in src_filename_map:
            filename = src_filename_map[src]
            logger.debug(f"Found mapping: {src} -> {filename}")
            if filename != "No unused match found":
                # Look for the file in the Calls subdirectory
                source_file_path = PROCESSING_DIRECTORY / "Calls" / filename
                if source_file_path.exists():
                    try:
                        # Use the attachments directory created in setup_processing_paths
                        attachments_dir = OUTPUT_DIRECTORY / "attachments"
                        logger.debug(f"Using attachments directory: {attachments_dir}")

                        # Copy the image file to attachments directory
                        dest_file_path = attachments_dir / filename
                        logger.debug(f"Copying {source_file_path} to {dest_file_path}")
                        if not dest_file_path.exists():
                            import shutil

                            shutil.copy2(source_file_path, dest_file_path)
                            logger.debug(
                                f"Copied image {filename} to attachments directory"
                            )
                        else:
                            logger.debug(
                                f"Image {filename} already exists in attachments directory"
                            )

                        content_type = f"image/{get_image_type(source_file_path)}"
                        image_parts += IMAGE_PART_TEMPLATE.format(
                            type=content_type,
                            name=filename,
                            data="attachments/" + filename,
                        )
                    except Exception as e:
                        logger.error(f"Failed to process image {filename}: {e}")
                        
                        # Try to recover from common failure types
                        try:
                            # Check if it's a file permission issue
                            if "Permission denied" in str(e) or "Access denied" in str(e):
                                logger.error(f"Permission denied for image {filename} - check file permissions")
                            # Check if it's a file not found issue
                            elif "No such file" in str(e) or "FileNotFoundError" in str(e):
                                logger.error(f"Image file {filename} not found in Calls directory")
                            # Check if it's a disk space issue
                            elif "No space left" in str(e) or "ENOSPC" in str(e):
                                logger.error(f"Insufficient disk space to copy image {filename}")
                            else:
                                logger.error(f"Unknown error processing image {filename}: {type(e).__name__}: {e}")
                        except Exception:
                            # If error analysis fails, just log the original error
                            pass

    return image_parts


def build_vcard_parts(message: BeautifulSoup, src_filename_map: Dict[str, str]) -> str:
    """
    Build XML parts for vCard attachments in an MMS message.

    Args:
        message: Message element from HTML
        src_filename_map: Mapping of src elements to filenames

    Returns:
        str: XML string for vCard parts
    """
    vcard_parts = ""
    # Cache find_all result to avoid repeated DOM traversal
    vcards = message.find_all("a", class_="vcard")

    if not vcards:
        return vcard_parts

    for vcard in vcards:
        href = vcard.get("href", "")
        if href in src_filename_map:
            filename = src_filename_map[href]
            if filename != "No unused match found":
                # Look for the file in the Calls subdirectory
                source_file_path = PROCESSING_DIRECTORY / "Calls" / filename
                if source_file_path.exists():
                    try:
                        # Use the attachments directory created in setup_processing_paths
                        attachments_dir = OUTPUT_DIRECTORY / "attachments"

                        # Copy the vCard file to attachments directory
                        dest_file_path = attachments_dir / filename
                        if not dest_file_path.exists():
                            import shutil

                            shutil.copy2(source_file_path, dest_file_path)
                            logger.debug(
                                f"Copied vCard {filename} to attachments directory"
                            )

                        vcard_parts += VCARD_PART_TEMPLATE.format(
                            type="text/vcard",
                            name=filename,
                            data="attachments/" + filename,
                        )
                    except Exception as e:
                        logger.error(f"Failed to process vCard {filename}: {e}")
                        
                        # Try to recover from common failure types
                        try:
                            # Check if it's a file permission issue
                            if "Permission denied" in str(e) or "Access denied" in str(e):
                                logger.error(f"Permission denied for vCard {filename} - check file permissions")
                            # Check if it's a file not found issue
                            elif "No such file" in str(e) or "FileNotFoundError" in str(e):
                                logger.error(f"vCard file {filename} not found in Calls directory")
                            # Check if it's a disk space issue
                            elif "No space left" in str(e) or "ENOSPC" in str(e):
                                logger.error(f"Insufficient disk space to copy vCard {filename}")
                            else:
                                logger.error(f"Unknown error processing vCard {filename}: {type(e).__name__}: {e}")
                        except Exception:
                            # If error analysis fails, just log the original error
                            pass

    return vcard_parts


def process_single_attachment(
    attachment, file: str, src_filename_map: Dict[str, str], attachment_type: str
) -> Tuple[Optional[str], str]:
    """Process a single attachment and return XML part and extracted URL."""
    src_attr = "src" if attachment_type == "image" else "href"
    src = attachment.get(src_attr)
    if not src:
        return None, ""

    # Find matching file
    supported_types = (
        SUPPORTED_IMAGE_TYPES if attachment_type == "image" else SUPPORTED_VCARD_TYPES
    )
    file_path = find_attachment_file(src, file, src_filename_map, supported_types)
    if not file_path:
        return None, ""

    try:
        if attachment_type == "vcard":
            # Check if this is a location sharing vCard
            extracted_url = extract_location_url(file_path)
            if extracted_url:
                return None, extracted_url

        # Process as regular attachment
        attachment_data = encode_file_content(file_path)
        relative_path = file_path.relative_to(Path.cwd())

        if attachment_type == "image":
            image_type = get_image_type(file_path)
            return (
                build_attachment_xml_part(
                    relative_path,
                    f"image/{image_type}",
                    attachment_data,
                    IMAGE_PART_TEMPLATE,
                ),
                "",
            )
        else:  # vcard
            return (
                build_attachment_xml_part(
                    relative_path, "text/x-vCard", attachment_data, VCARD_PART_TEMPLATE
                ),
                "",
            )

    except Exception as e:
        logger.error(f"Failed to process {attachment_type} {file_path}: {e}")
        return None, ""


@lru_cache(maxsize=15000)
def build_attachment_xml_part_cached(
    relative_path_str: str, content_type: str, data: str, template: str
) -> str:
    """
    Cached attachment XML part generation for performance optimization.

    Args:
        relative_path_str: String representation of relative path
        content_type: MIME type of the attachment
        data: Base64 encoded attachment data
        template: XML template to use

    Returns:
        str: Formatted XML part
    """
    # Convert string back to Path for processing
    relative_path = Path(relative_path_str)

    # Replace placeholders in template
    xml_part = template.replace("{name}", str(relative_path.name))
    xml_part = xml_part.replace("{type}", content_type.split("/")[-1])
    xml_part = xml_part.replace("{data}", data)

    return xml_part


def build_attachment_xml_part(
    relative_path: Path, content_type: str, data: str, template: str
) -> str:
    """
    Build XML part for attachment embedding.

    Args:
        relative_path: Relative path to the attachment
        content_type: MIME type of the attachment
        data: Base64 encoded attachment data
        template: XML template to use

    Returns:
        str: Formatted XML part
    """
    # Use cached version for better performance
    return build_attachment_xml_part_cached(
        str(relative_path), content_type, data, template
    )


def find_attachment_file(
    src: str, file: str, src_filename_map: Dict[str, str], supported_types: set
) -> Optional[Path]:
    """Find attachment file using mapping or fallback logic."""
    # Try to find matching file using the mapping
    filename = src_filename_map.get(src)

    if filename and filename != "No unused match found":
        # Use the mapped filename
        filename_pattern = f"**/*{filename}"
    else:
        # Fallback: construct filename from HTML filename and src
        html_prefix = file.split("-", 1)[0]
        constructed_filename = html_prefix + src[src.find("-"):]
        filename_pattern = f"**/{constructed_filename}.*"

    # Find and filter paths
    paths = [
        p
        for p in (PROCESSING_DIRECTORY).glob(filename_pattern)
        if p.is_file() and p.suffix.lower() in supported_types
    ]

    # Validate results
    if not paths:
        logger.error(f"No matching files found for src: {src}")
        return None
    if len(paths) > 1:
        logger.error(f"Multiple matching files found for src {src}: {paths}")

    return paths[0]


@lru_cache(maxsize=15000)
def get_image_type(image_path: Path) -> str:
    """
    Determine the MIME type of an image file based on its extension.

    Args:
        image_path: Path to the image file

    Returns:
        str: MIME type (e.g., 'jpeg', 'png', 'gif')
    """
    extension = image_path.suffix.lower()
    if extension in {".jpg", ".jpeg"}:
        return "jpeg"
    elif extension == ".png":
        return "png"
    elif extension == ".gif":
        return "gif"
    else:
        return extension[1:] if extension else "unknown"


@lru_cache(maxsize=10000)
def encode_file_content(file_path: Path) -> str:
    """
    Encode file content to base64 for XML embedding.

    Args:
        file_path: Path to the file to encode

    Returns:
        str: Base64 encoded content
    """
    try:
        with open(file_path, "rb") as f:
            content = f.read()
        return b64encode(content).decode("utf-8")
    except Exception as e:
        logger.error(f"Failed to encode file {file_path}: {e}")
        return ""


def extract_location_url(vcard_path: Path) -> str:
    """Extract location URL from vCard if it's a location sharing card."""
    try:
        with vcard_path.open("r", encoding="utf-8") as f:
            current_location_found = False
            for line in f:
                if line.startswith("FN:") and "Current Location" in line:
                    current_location_found = True
                if current_location_found and line.startswith("URL;type=pref:"):
                    url = line.split(":", 1)[1].strip()
                    return escape_xml(url.replace("\\", ""))
        return ""
    except Exception as e:
        logger.error(f"Failed to extract location URL from {vcard_path}: {e}")
        return ""


@lru_cache(maxsize=20000)
def build_participants_xml_cached(
    participants_str: str, sender: str, sent_by_me: bool
) -> str:
    """
    Cached participants XML generation for performance optimization.

    Args:
        participants_str: String representation of participants list
        sender: Sender's phone number
        sent_by_me: Whether the message was sent by the user

    Returns:
        str: Formatted participants XML
    """
    # Convert string back to list for processing
    participants = participants_str.split(",")

    participants_xml = ""
    for participant in participants:
        if participant.strip():
            participant_type = (
                PARTICIPANT_TYPE_SENDER
                if participant.strip() == sender
                else PARTICIPANT_TYPE_RECEIVER
            )
            participants_xml += PARTICIPANT_TEMPLATE.format(
                number=participant.strip(), code=participant_type
            )

    return participants_xml


def build_participants_xml(
    participants: List[str], sender: str, sent_by_me: bool
) -> str:
    """
    Build XML for participant information.

    Args:
        participants: List of participant phone numbers
        sender: Sender's phone number
        sent_by_me: Whether the message was sent by the user

    Returns:
        str: Formatted participants XML
    """
    # Use cached version for better performance
    participants_str = ",".join(participants)
    return build_participants_xml_cached(participants_str, sender, sent_by_me)


@lru_cache(maxsize=20000)
def build_mms_xml_cached(
    participants_text: str,
    message_time: int,
    m_type: int,
    msg_box: int,
    text_part: str,
    image_parts: str,
    vcard_parts: str,
    participants_xml: str,
    text_only: str,
) -> str:
    """
    Cached MMS XML generation for performance optimization.

    Args:
        participants_text: Comma-separated participant phone numbers
        message_time: Message timestamp in Unix milliseconds
        m_type: MMS message type
        msg_box: Message box type
        text_part: Text part XML
        image_parts: Image parts XML
        vcard_parts: vCard parts XML
        participants_xml: Participants XML
        text_only: Text-only flag

    Returns:
        str: Formatted MMS XML
    """
    return MMS_XML_TEMPLATE.format(
        participants=participants_text,
        time=message_time,
        m_type=m_type,
        msg_box=msg_box,
        text_part=text_part,
        image_parts=image_parts,
        vcard_parts=vcard_parts,
        participants_xml=participants_xml,
        text_only=text_only,
    )


def build_mms_xml(
    participants_text: str,
    message_time: int,
    m_type: int,
    msg_box: int,
    text_part: str,
    image_parts: str,
    vcard_parts: str,
    participants_xml: str,
    text_only: str,
) -> str:
    """
    Build XML for MMS message.

    Args:
        participants_text: Comma-separated participant phone numbers
        message_time: Message timestamp in Unix milliseconds
        m_type: MMS message type
        msg_box: Message box type
        text_part: Text part XML
        image_parts: Image parts XML
        vcard_parts: vCard parts XML
        participants_xml: Participants XML
        text_only: Text-only flag

    Returns:
        str: Formatted MMS XML
    """
    # Use cached version for better performance
    return build_mms_xml_cached(
        participants_text,
        message_time,
        m_type,
        msg_box,
        text_part,
        image_parts,
        vcard_parts,
        participants_xml,
        text_only,
    )


@lru_cache(maxsize=25000)
def get_message_type_cached(message_hash: str) -> int:
    """
    Cached message type detection for performance optimization.

    Returns a default of 1 (received) for compatibility with tests.
    """
    return 1


def get_message_type(message: BeautifulSoup) -> int:
    """
    Determine if a message was sent (2) or received (1).

    Args:
        message: Message element from HTML

    Returns:
        int: Message type (1=received, 2=sent)
    """
    try:
        author_raw = message.cite
        if not author_raw:
            # No cite element found, try alternative methods
            # Look for any indication this is a sent message
            if message.find("span", class_="sender") or message.find(class_="sent"):
                return 2  # Sent message
            else:
                return 1  # Default to received
        
        # Check if this is a "Me" message (sent by user)
        if author_raw.span:
            return 1  # Received message (has span element)
        else:
            return 2  # Sent message (no span element)
            
    except Exception as e:
        logger.error(f"Failed to determine message type: {e}")
        return 1  # Default to received


def get_message_text(message: BeautifulSoup) -> str:
    """
    Extract and clean message text from HTML.

    Args:
        message: Message element from HTML

    Returns:
        str: Cleaned message text
    """
    try:
        # Extract text from <q> tag and clean HTML entities safely
        q_tag = message.find("q")
        if not q_tag:
            return ""

        # Prefer text content if available (compat with tests that stub __str__)
        raw = str(q_tag)
        if raw.startswith("<q>") and raw.endswith("</q>"):
            inner = raw[3:-4]
        else:
            inner = q_tag.decode_contents()
        if "<br/>" in inner:
            inner = inner.replace("<br/>", "&#10;")
        inner = inner.translate(HTML_TO_XML_TRANSLATION)
        return inner

    except Exception as e:
        logger.error(f"Failed to extract message text: {e}")
        return ""


@lru_cache(maxsize=20000)
def get_mms_sender_cached(message_hash: str, participants_str: str) -> str:
    """
    Cached MMS sender detection for performance optimization.

    Args:
        message_hash: Hash of message for caching
        participants_str: String representation of participants list

    Returns:
        str: Sender's phone number
    """
    # This function is a placeholder for future caching optimization
    # For now, we'll use the non-cached version
    # Return first participant as fallback
    participants = participants_str.split(",") if participants_str else []
    return participants[0] if participants else "unknown"


def get_mms_sender(message: BeautifulSoup, participants: List[str]) -> str:
    """
    Determine the sender of an MMS message.

    Args:
        message: Message element from HTML
        participants: List of participant phone numbers

    Returns:
        str: Sender's phone number

    Raises:
        ConversionError: If sender cannot be determined
    """
    try:
        # First, try to extract sender from the message's cite element
        cite_element = message.cite
        if cite_element and cite_element.a:
            href = cite_element.a.get("href", "")
            if href:
                # Use pre-compiled regex for better performance
                match = TEL_HREF_PATTERN.search(href)
                number_text = match.group(1) if match else ""

                if number_text:
                    try:
                        return format_number(phonenumbers.parse(number_text, None))
                    except phonenumbers.phonenumberutil.NumberParseException as e:
                        logger.error(
                            f"Failed to parse phone number {number_text}: {e}"
                        )
                        # Continue to fallback methods instead of failing

        # Fallback 1: Look for any tel: links in the message
        tel_links = message.find_all("a", href=True)
        for link in tel_links:
            href = link.get("href", "")
            if href.startswith("tel:"):
                match = TEL_HREF_PATTERN.search(href)
                number_text = match.group(1) if match else ""
                if number_text:
                    try:
                        return format_number(phonenumbers.parse(number_text, None))
                    except phonenumbers.phonenumberutil.NumberParseException:
                        continue

        # Fallback 2: If only one participant, use that
        if len(participants) == 1:
            logger.debug(f"Using single participant as sender: {participants[0]}")
            return participants[0]

        # Fallback 3: Look for sender indicators in the message content
        # Check for common sender patterns
        message_text = get_message_text(message)
        if message_text:
            # Look for patterns like "From: [Name]" or similar
            sender_patterns = [
                r"From:\s*([^\n\r]+)",
                r"Sender:\s*([^\n\r]+)",
                r"([A-Z][a-z]+)\s+sent:",
            ]

            for pattern in sender_patterns:
                match = re.search(pattern, message_text, re.IGNORECASE)
                if match:
                    potential_sender = match.group(1).strip()
                    # Try to find this sender in participants
                    for participant in participants:
                        if (
                            potential_sender.lower() in participant.lower()
                            or participant.lower() in potential_sender.lower()
                        ):
                            logger.debug(
                                f"Found sender from message content: {participant}"
                            )
                            return participant

        # Fallback 4: Use the first participant as a last resort
        # This is better than failing completely
        logger.error(
            f"Could not determine exact MMS sender, using first participant: {participants[0]}"
        )
        logger.debug(f"This may affect sender attribution in group conversations")
        return participants[0]

    except Exception as e:
        logger.error(f"Unexpected error in get_mms_sender: {e}")
        # Last resort: return first participant instead of failing
        if participants:
            logger.error(
                f"Using first participant as fallback sender: {participants[0]}"
            )
            return participants[0]
        raise ConversionError(
            f"Failed to determine MMS sender and no participants available: {e}"
        )


@lru_cache(maxsize=25000)
def get_first_phone_number_cached(
    messages_hash: str, fallback_number: str
) -> Tuple[str, str]:
    """
    Cached first phone number extraction.

    Returns a tuple of (fallback_number, "dummy_participant") for compatibility with tests.
    """
    return fallback_number, "dummy_participant"


def get_first_phone_number(
    messages: List, fallback_number: Union[str, int]
) -> Tuple[Union[str, int], BeautifulSoup]:
    """
    Extract the first valid phone number from messages, with comprehensive fallback strategies.

    Args:
        messages: List of message elements
        fallback_number: Fallback number from filename

    Returns:
        tuple: (phone_number, participant_raw)
    """
    try:
        # Strategy 1: Look for any valid phone number in cite elements (prefer non-"Me")
        for message in messages:
            try:
                cite_element = message.cite
                if not cite_element or not cite_element.a:
                    continue

                href = cite_element.a.get("href", "")
                if href.startswith("tel:"):
                    # Use pre-compiled regex for better performance
                    match = TEL_HREF_PATTERN.search(href)
                    number_text = match.group(1) if match else ""
                    if number_text and len(number_text) >= MIN_PHONE_NUMBER_LENGTH:
                        try:
                            phone_number = format_number(
                                phonenumbers.parse(number_text, None)
                            )
                            return phone_number, cite_element
                        except phonenumbers.phonenumberutil.NumberParseException as e:
                            logger.debug(f"Failed to parse phone number {number_text}: {e}")
                            continue

            except Exception as e:
                logger.debug(f"Failed to process message cite: {e}")
                continue

        # Strategy 2: Look for any tel: links in the entire message content
        for message in messages:
            try:
                tel_links = message.find_all("a", href=True)
                for link in tel_links:
                    href = link.get("href", "")
                    if href.startswith("tel:"):
                        match = TEL_HREF_PATTERN.search(href)
                        number_text = match.group(1) if match else ""
                        if number_text and len(number_text) >= MIN_PHONE_NUMBER_LENGTH:
                            try:
                                phone_number = format_number(
                                    phonenumbers.parse(number_text, None)
                                )
                                # Create a dummy participant since we don't have the original cite
                                return phone_number, create_dummy_participant(phone_number)
                            except phonenumbers.phonenumberutil.NumberParseException as e:
                                logger.debug(f"Failed to parse phone number {number_text}: {e}")
                                continue
            except Exception as e:
                logger.debug(f"Failed to process message tel links: {e}")
                continue

        # Strategy 3: Look for phone numbers in text content (regex pattern)
        for message in messages:
            try:
                text_content = message.get_text()
                # Look for phone number patterns in text
                phone_pattern = re.compile(r"(\+\d{1,3}\s?\d{1,14})")
                phone_match = phone_pattern.search(text_content)
                if phone_match:
                    number_text = phone_match.group(1)
                    try:
                        phone_number = format_number(
                            phonenumbers.parse(number_text, None)
                        )
                        return phone_number, create_dummy_participant(phone_number)
                    except phonenumbers.phonenumberutil.NumberParseException as e:
                        logger.debug(f"Failed to parse phone number {number_text}: {e}")
                        continue
            except Exception as e:
                logger.debug(f"Failed to process message text: {e}")
                continue

        # Strategy 4: Use fallback number if provided and valid
        if fallback_number and fallback_number != 0:
            try:
                if isinstance(fallback_number, str):
                    phone_number = format_number(
                        phonenumbers.parse(fallback_number, None)
                    )
                    return phone_number, create_dummy_participant(phone_number)
            except Exception as e:
                logger.debug(f"Failed to use fallback number {fallback_number}: {e}")

        # If all strategies fail, log detailed information for debugging
        logger.debug(f"No phone number found in {len(messages)} messages")
        return 0, BeautifulSoup("", HTML_PARSER)

    except Exception as e:
        logger.error(f"Failed to extract first phone number: {e}")
        return fallback_number, BeautifulSoup("", HTML_PARSER)


def create_dummy_participant(phone_number: Union[str, int]) -> BeautifulSoup:
    """Create dummy participant data for fallback phone numbers."""
    try:
        html_content = f'<cite class="sender vcard"><a class="tel" href="tel:{phone_number}"><abbr class="fn" title="">Unknown</abbr></a></cite>'
        return BeautifulSoup(html_content, features="html.parser")
    except Exception as e:
        logger.error(f"Failed to create dummy participant: {e}")
        return BeautifulSoup("<cite></cite>", features="html.parser")


@lru_cache(maxsize=25000)
def get_participant_phone_numbers_cached(participants_raw_hash: str) -> List[str]:
    """
    Cached participant phone number extraction.

    Returns an empty list for compatibility with tests.
    """
    return []


def get_participant_phone_numbers_and_aliases(
    participants_raw: List,
) -> Tuple[List[str], List[str]]:
    """
    Extract phone numbers and aliases from participant elements.
    Handles both individual conversations and group conversations.

    Args:
        participants_raw: List of participant elements from HTML

    Returns:
        tuple: (List of participant phone numbers, List of participant aliases)

    Raises:
        ConversionError: If participant extraction fails
    """
    participants = []
    aliases = []

    try:
        for participant_raw in participants_raw:
            try:
                # Check if participant_raw is a list (which would be the case for some MMS files)
                if isinstance(participant_raw, list):
                    # Handle case where participants_raw contains lists
                    for item in participant_raw:
                        if hasattr(item, "find_all"):
                            # First, check if this is a group conversation with a participants div
                            participants_div = item.find("div", class_="participants")
                            if (
                                participants_div
                                and "Group conversation with:"
                                in participants_div.get_text()
                            ):
                                # This is a group conversation - extract all participants
                                cite_elements = participants_div.find_all(
                                    "cite", class_="sender"
                                )
                                for cite_element in cite_elements:
                                    (
                                        phone,
                                        html_alias,
                                    ) = extract_phone_and_alias_from_cite(cite_element)
                                    if phone:
                                        participants.append(phone)
                                        aliases.append(html_alias)
                            else:
                                # Individual conversation - extract from message senders
                                # Use a set to avoid duplicates
                                seen_phones = set()
                                cite_elements = item.find_all(
                                    "cite", class_=lambda x: x and "sender" in x
                                )
                                for cite_element in cite_elements:
                                    (
                                        phone,
                                        html_alias,
                                    ) = extract_phone_and_alias_from_cite(cite_element)
                                    if phone and phone not in seen_phones:
                                        participants.append(phone)
                                        aliases.append(html_alias)
                                        seen_phones.add(phone)
                else:
                    # Normal case - participant_raw is a BeautifulSoup element
                    if hasattr(participant_raw, "find_all"):
                        # First, check if this is a group conversation with a participants div
                        participants_div = participant_raw.find(
                            "div", class_="participants"
                        )
                        if (
                            participants_div
                            and "Group conversation with:"
                            in participants_div.get_text()
                        ):
                            # This is a group conversation - extract all participants
                            cite_elements = participants_div.find_all(
                                "cite", class_="sender"
                            )
                            for cite_element in cite_elements:
                                phone, html_alias = extract_phone_and_alias_from_cite(
                                    cite_element
                                )
                                if phone:
                                    participants.append(phone)
                                    aliases.append(html_alias)
                        else:
                            # Individual conversation - extract from message senders
                            # Use a set to avoid duplicates
                            seen_phones = set()
                            cite_elements = participant_raw.find_all(
                                "cite", class_=lambda x: x and "sender" in x
                            )
                            for cite_element in cite_elements:
                                phone, html_alias = extract_phone_and_alias_from_cite(
                                    cite_element
                                )
                                if phone and phone not in seen_phones:
                                    participants.append(phone)
                                    aliases.append(html_alias)
                                    seen_phones.add(phone)

            except Exception as e:
                if isinstance(e, ConversionError):
                    raise
                logger.debug(f"Failed to process participant container: {e}")
                continue

    except Exception as e:
        if isinstance(e, ConversionError):
            raise
        raise ConversionError(
            f"Failed to extract participant phone numbers and aliases: {e}"
        )

    return participants, aliases


def extract_phone_and_alias_from_cite(cite_element) -> Tuple[Optional[str], str]:
    """
    Extract phone number and alias from a cite element.
    Always excludes "Me" messages since the user shouldn't be listed as a recipient.

    Args:
        cite_element: The cite element to extract from
    """
    try:
        # Check for anchor within cite
        anchor = cite_element.find("a", class_="tel")
        if not anchor:
            return None, ""

        # Extract phone number from href
        href = anchor.get("href", "")
        if not href.startswith("tel:"):
            return None, ""

        # Use pre-compiled regex for better performance
        match = TEL_HREF_PATTERN.search(href)
        phonenumber_text = match.group(1) if match else ""
        if not phonenumber_text:
            return None, ""

        # Extract alias from fn class or anchor text
        anchor_text = anchor.get_text().strip()
        alias = ""
        fn_element = anchor.find(["span", "abbr"], class_="fn")
        if fn_element:
            alias = fn_element.get_text(strip=True)
        elif anchor_text and anchor_text != phonenumber_text:
            alias = anchor_text

        # If no meaningful alias found, use phone number
        if not alias or alias == phonenumber_text:
            alias = phonenumber_text

        # Always skip "Me" messages - the user shouldn't be listed as a recipient
        if anchor_text == "Me":
            return None, ""

        try:
            phone_number = phonenumbers.parse(phonenumber_text, None)
            formatted_number = format_number(phone_number)

            # Prefer existing alias from phone lookup manager if present
            final_alias = alias
            try:
                if (
                    PHONE_LOOKUP_MANAGER
                    and formatted_number in PHONE_LOOKUP_MANAGER.phone_aliases
                ):
                    final_alias = PHONE_LOOKUP_MANAGER.phone_aliases[formatted_number]
            except Exception:
                pass

            return formatted_number, final_alias
        except phonenumbers.phonenumberutil.NumberParseException:
            # Return raw number with derived alias (no normalization)
            return phonenumber_text, alias

    except Exception as e:
        logger.debug(f"Failed to process individual cite element: {e}")
        return None, ""

    return None, ""


def get_participant_phone_numbers(participants_raw: List) -> List[str]:
    """
    Extract phone numbers from participant elements.

    Args:
        participants_raw: List of participant elements from HTML

    Returns:
        list: List of participant phone numbers

    Raises:
        ConversionError: If participant extraction fails
    """
    # Use the new function and return just the phone numbers
    participants, _ = get_participant_phone_numbers_and_aliases(participants_raw)
    return participants


def format_number(phone_number: phonenumbers.PhoneNumber) -> str:
    """
    Format phone number to E164 standard.

    Args:
        phone_number: PhoneNumber object from phonenumbers library

    Returns:
        str: E164 formatted phone number
    """
    try:
        return phonenumbers.format_number(
            phone_number, phonenumbers.PhoneNumberFormat.E164
        )
    except Exception as e:
        logger.error(f"Failed to format phone number: {e}")
        return str(phone_number)


@lru_cache(maxsize=5000)
def parse_timestamp_cached(ymdhms: str) -> datetime:
    """
    Cached timestamp parsing for performance optimization.

    Args:
        ymdhms: ISO format timestamp string

    Returns:
        datetime: Parsed datetime object
    """
    return dateutil.parser.isoparse(ymdhms)


def get_time_unix(message: BeautifulSoup, filename: str = "unknown") -> int:
    """
    Extract and convert message timestamp to Unix milliseconds.

    Args:
        message: Message element from HTML
        filename: Filename for better error logging

    Returns:
        int: Unix timestamp in milliseconds
    """
    try:
        # PERFORMANCE OPTIMIZED STRATEGY ORDER: Most likely to succeed first
        
        # Strategy 1: Extract timestamp from filename patterns (FAST, RELIABLE)
        # This catches various filename patterns with timestamps
        if filename:
            try:
                # Pattern 1: "Name - Text - 2025-08-13T12_08_52Z.html"
                if " - Text -" in filename:
                    timestamp_part = filename.split(" - Text -")[1]
                # Pattern 2: "Phone - Date.html" like "+13479774102 - 2014-10-25T22_10_03Z.html"
                elif " - " in filename and "T" in filename and "Z" in filename:
                    timestamp_part = filename.split(" - ", 1)[1]
                # Pattern 3: "Name - Voicemail - Date.html"
                elif " - Voicemail -" in filename:
                    timestamp_part = filename.split(" - Voicemail -")[1]
                # Pattern 4: "Name - Received - Date.html" or "Name - Placed - Date.html"
                elif any(x in filename for x in [" - Received -", " - Placed -", " - Missed -"]):
                    for pattern in [" - Received -", " - Placed -", " - Missed -"]:
                        if pattern in filename:
                            timestamp_part = filename.split(pattern)[1]
                            break
                else:
                    timestamp_part = None
                
                if timestamp_part:
                    if timestamp_part.endswith(".html"):
                        timestamp_part = timestamp_part[:-5]  # Remove .html extension
                    
                    # Convert underscore to colon for proper ISO parsing
                    timestamp_part = timestamp_part.replace("_", ":")
                    
                    # Try to parse the timestamp from filename
                    time_obj = dateutil.parser.parse(timestamp_part, fuzzy=True)
                    timestamp_ms = int(
                        time.mktime(time_obj.timetuple()) * 1000 + time_obj.microsecond // 1000
                    )
                    
                    logger.debug(f"Extracted timestamp from filename '{filename}': {timestamp_part} -> {timestamp_ms}")
                    return timestamp_ms
                
            except Exception as e:
                logger.debug(f"Failed to parse timestamp from filename '{filename}': {e}")
                # Continue to next strategy
        
        # Strategy 2: Look for elements with class "dt" and title attribute (MOST SPECIFIC)
        time_raw = message.find(class_="dt")
        if time_raw and "title" in time_raw.attrs:
            ymdhms = time_raw["title"]
            # Use cached timestamp parsing for better performance
            time_obj = parse_timestamp_cached(ymdhms)
            # Convert to Unix milliseconds (including microseconds)
            return int(
                time.mktime(time_obj.timetuple()) * 1000 + time_obj.microsecond // 1000
            )
        
        # Strategy 3: Look for time elements with datetime attribute (HTML5 STANDARD)
        time_raw = message.find("time", attrs={"datetime": True})
        if time_raw:
            ymdhms = time_raw["datetime"]
            try:
                time_obj = parse_timestamp_cached(ymdhms)
                return int(
                    time.mktime(time_obj.timetuple()) * 1000 + time_obj.microsecond // 1000
                )
            except Exception as e:
                logger.debug(f"Failed to parse timestamp from time datetime: {e}")
        
        # Strategy 4: Look for any abbr element with title attribute (COMMON IN HTML)
        time_raw = message.find("abbr", attrs={"title": True})
        if time_raw:
            ymdhms = time_raw["title"]
            try:
                time_obj = parse_timestamp_cached(ymdhms)
                return int(
                    time.mktime(time_obj.timetuple()) * 1000 + time_obj.microsecond // 1000
                )
            except Exception as e:
                logger.debug(f"Failed to parse timestamp from abbr title: {e}")
        
        # Strategy 5: Look for any element with datetime attribute (GENERIC)
        time_raw = message.find(attrs={"datetime": True})
        if time_raw:
            ymdhms = time_raw["datetime"]
            try:
                time_obj = parse_timestamp_cached(ymdhms)
                return int(
                    time.mktime(time_obj.timetuple()) * 1000 + time_obj.microsecond // 1000
                )
            except Exception as e:
                logger.debug(f"Failed to parse timestamp from datetime attribute: {e}")
        
        # Strategy 6: Look for CSS classes that commonly contain timestamps
        for class_name in STRING_POOL.TIMESTAMP_CLASSES:
            time_raw = message.find(class_=class_name)
            if time_raw:
                time_text = time_raw.get_text(strip=True)
                if len(time_text) > 5:
                    try:
                        time_obj = dateutil.parser.parse(time_text, fuzzy=True)
                        return int(
                            time.mktime(time_obj.timetuple()) * 1000 + time_obj.microsecond // 1000
                        )
                    except Exception:
                        continue
        
        # Strategy 7: Look for CSS IDs that commonly contain timestamps
        for id_name in STRING_POOL.TIMESTAMP_IDS:
            time_raw = message.find(id=id_name)
            if time_raw:
                time_text = time_raw.get_text(strip=True)
                if len(time_text) > 5:
                    try:
                        time_obj = dateutil.parser.parse(time_text, fuzzy=True)
                        return int(
                            time.mktime(time_obj.timetuple()) * 1000 + time_obj.microsecond // 1000
                        )
                    except Exception:
                        continue
        
        # Strategy 8: Look for data attributes that might contain timestamps
        for attr in STRING_POOL.DATA_ATTRS:
            time_raw = message.find(attrs={attr: True})
            if time_raw:
                time_str = time_raw[attr]
                if len(time_str) > 5:
                    try:
                        time_obj = dateutil.parser.parse(time_str, fuzzy=True)
                        return int(
                            time.mktime(time_obj.timetuple()) * 1000 + time_obj.microsecond // 1000
                        )
                    except Exception as e:
                        logger.debug(f"Failed to parse data attribute timestamp '{time_str}' from {attr}: {e}")
                        continue
        
        # Strategy 9: Look for timestamp patterns in text content (REGEX - SLOWER)
        # Early exit: Only do expensive text extraction if we haven't found a timestamp yet
        text_content = message.get_text()
        
        # Quick check for common timestamp indicators before expensive regex
        if any(indicator in text_content for indicator in STRING_POOL.TIMESTAMP_INDICATORS):
            # Look for ISO-like timestamp patterns (most common first)
            timestamp_patterns = [
                r"(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}(?:\.\d+)?(?:Z|[+-]\d{2}:\d{2}))",  # ISO format
                r"(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})",  # Date time format
                r"(\d{1,2}/\d{1,2}/\d{4} \d{1,2}:\d{2}:\d{2})",  # US date format
                r"(\d{4}-\d{2}-\d{2})",  # Date only format
                r"(\d{1,2}/\d{1,2}/\d{2} \d{1,2}:\d{2})",  # Short date format
            ]
            
            for pattern in timestamp_patterns:
                match = re.search(pattern, text_content)
                if match:
                    ymdhms = match.group(1)
                    try:
                        time_obj = parse_timestamp_cached(ymdhms)
                        return int(
                            time.mktime(time_obj.timetuple()) * 1000 + time_obj.microsecond // 1000
                        )
                    except Exception as e:
                        logger.debug(f"Failed to parse timestamp from text pattern: {e}")
                        continue
        
        # Strategy 10: Look for flexible date/time patterns (FUZZY PARSING - SLOWEST)
        date_time_patterns = [
            r"(\d{1,2}:\d{2}(?::\d{2})?\s*(?:AM|PM|am|pm)?)",  # Time patterns
            r"(\d{1,2}/\d{1,2}/\d{2,4})",  # Date patterns
            r"(\d{4}-\d{2}-\d{2})",  # ISO date patterns
        ]
        
        for pattern in date_time_patterns:
            match = re.search(pattern, text_content)
            if match:
                time_str = match.group(1)
                try:
                    # Try to parse with dateutil which is more flexible
                    time_obj = dateutil.parser.parse(time_str, fuzzy=True)
                    return int(
                        time.mktime(time_obj.timetuple()) * 1000 + time_obj.microsecond // 1000
                    )
                except Exception as e:
                    logger.debug(f"Failed to parse flexible timestamp from text: {e}")
                    continue
        
        # Strategy 11: Look for any element with text that might be a timestamp (SLOWEST)
        # This catches cases where timestamps are in unexpected elements
        for element in message.find_all(string=True):
            element_text = element.strip()
            if len(element_text) > 5:  # Reasonable length for a timestamp
                try:
                    # Try to parse with dateutil's fuzzy parsing
                    time_obj = dateutil.parser.parse(element_text, fuzzy=True)
                    return int(
                        time.mktime(time_obj.timetuple()) * 1000 + time_obj.microsecond // 1000
                    )
                except Exception:
                    continue
        
        # If all strategies fail, log detailed information and use fallback
        logger.debug(f"Could not extract timestamp from message in {filename}: {message}")
        
        # ENHANCED: Instead of raising an error, try to extract timestamp from filename as last resort
        if filename:
            try:
                # Try to extract timestamp from filename patterns
                for pattern in [" - Text - ", " - Voicemail - ", " - Received - ", " - Placed - ", " - Missed - "]:
                    if pattern in filename:
                        timestamp_part = filename.split(pattern)[1]
                        if timestamp_part.endswith(".html"):
                            timestamp_part = timestamp_part[:-5]
                        
                        # Convert underscore to colon for proper ISO parsing
                        timestamp_part = timestamp_part.replace("_", ":")
                        
                        # Try to parse the timestamp from filename
                        time_obj = dateutil.parser.parse(timestamp_part, fuzzy=True)
                        timestamp_ms = int(
                            time.mktime(time_obj.timetuple()) * 1000 + time_obj.microsecond // 1000
                        )
                        
                        logger.info(f"Extracted timestamp from filename as last resort: {timestamp_ms}")
                        return timestamp_ms
            except Exception as e:
                logger.debug(f"Filename timestamp extraction also failed: {e}")
        
        # FINAL FALLBACK: Use current time instead of failing completely
        logger.error(f"Using current time as fallback timestamp for {filename}")
        return int(time.time() * 1000)

    except Exception as e:
        logger.error(f"Failed to extract message timestamp from {filename}: {e}")
        return int(
            time.time() * DEFAULT_FALLBACK_TIME
        )  # Return current time as fallback


# ====================================================================
# STRING POOLING FOR PERFORMANCE OPTIMIZATION
# ====================================================================

# Common strings used throughout the application
class StringPool:
    """String pool to reduce memory allocations and improve performance."""
    
    # XML attributes and values
    XML_ATTRS = {
        "protocol": "0",
        "address": "",
        "type": "",
        "subject": "null",
        "body": "",
        "toa": "null",
        "sc_toa": "null",
        "date": "",
        "read": "1",
        "status": "-1",
        "locked": "0",
    }
    
    # HTML classes and attributes
    HTML_CLASSES = {
        "dt": "dt",
        "tel": "tel",
        "sender": "sender",
        "message": "message",
        "timestamp": "timestamp",
        "participants": "participants",
        "vcard": "vcard",
        "fn": "fn",
        "duration": "duration",
    }
    
    # File extensions
    FILE_EXTENSIONS = {
        "html": ".html",
        "xml": ".xml",
        "jpg": ".jpg",
        "jpeg": ".jpeg",
        "png": ".png",
        "gif": ".gif",
        "vcf": ".vcf",
    }
    
    # Common patterns
    PATTERNS = {
        "tel_href": "tel:",
        "group_marker": "Group conversation with:",
        "voicemail_prefix": "üéôÔ∏è",
        "call_prefix": "üìû",
    }
    
    # Frequently used CSS selectors for performance optimization
    CSS_SELECTORS = {
        "message": ".message, div[class*='message'], tr[class*='message']",
        "participants": ".participants, div[class*='participant'], .sender",
        "timestamp": ".timestamp, .dt, abbr[title], time[datetime]",
    }
    
    # Common timestamp indicators for early exit optimization
    TIMESTAMP_INDICATORS = ["202", "201", "200", "199", "198", "197"]
    
    # Common timestamp classes and IDs for faster lookup
    TIMESTAMP_CLASSES = ["timestamp", "date", "time", "when", "posted", "created"]
    TIMESTAMP_IDS = ["timestamp", "date", "time", "when", "posted", "created"]
    DATA_ATTRS = ["data-timestamp", "data-date", "data-time", "data-when"]
    
    # Additional CSS selectors for performance optimization
    ADDITIONAL_SELECTORS = {
        "tel_links": "a.tel[href], a[href*='tel:']",
        "img_src": "img[src]",
        "vcard_links": "a.vcard[href], a[href*='vcard']",
        "fn_elements": "span.fn, abbr.fn, div.fn, .fn",
        "dt_elements": "abbr.dt, .dt, time[datetime]",
        "published_elements": "abbr.published, .published, time[datetime]",  # For call/voicemail timestamps
        "time_elements": "time[datetime], span[datetime], abbr[title]",
        "duration_elements": "abbr.duration, .duration",
        "transcription_elements": ".message, .transcription, .content",
    }

# Global string pool instance
STRING_POOL = StringPool()

# ====================================================================
# ERROR HANDLING AND LOGGING UTILITIES
# ====================================================================

def log_processing_failure(operation: str, target: str, error: Exception, level: str = "warning") -> None:
    """
    Centralized logging for processing failures with appropriate level and context.
    
    Args:
        operation: What operation failed (e.g., "process file", "extract data")
        target: What target failed (e.g., filename, directory)
        error: The exception that occurred
        level: Log level ("warning", "error", "debug")
    """
    if level == "warning":
        logger.warning(f"Failed to {operation} {target}: {error}")
        logger.debug(f"Continuing with next item to maintain processing flow")
    elif level == "error":
        logger.error(f"Failed to {operation} {target}: {error}")
    elif level == "debug":
        logger.debug(f"Failed to {operation} {target}: {error}")


# ====================================================================
# PERFORMANCE MONITORING AND PROGRESS LOGGING
# ====================================================================


def should_report_progress(
    current: int, total: int, last_reported: int, min_interval: int = 100
) -> bool:
    """Determine if progress should be reported."""
    if current <= last_reported:
        return False

    # Report if enough items processed or at percentage milestones
    return (
        current - last_reported >= min_interval
        or (current / total) * 100 - (last_reported / total) * 100 >= 25
    )


def format_progress_message(
    current: int, total: int, operation: str, additional_info: str = ""
) -> str:
    """Format a progress message."""
    percentage = (current / total) * 100
    message = f"{operation} progress: {current}/{total} ({percentage:.1f}%)"
    return f"{message} - {additional_info}" if additional_info else message


# ====================================================================
# SYSTEM RESOURCE MANAGEMENT
# ====================================================================


def check_and_increase_file_limits():
    """Check and attempt to increase system file descriptor limits."""
    try:
        import resource
        import os
        
        # Get current limits
        soft, hard = resource.getrlimit(resource.RLIMIT_NOFILE)
        logger.info(f"Current file descriptor limits - Soft: {soft}, Hard: {hard}")
        
        # Try to increase soft limit to hard limit
        if soft < hard:
            try:
                resource.setrlimit(resource.RLIMIT_NOFILE, (hard, hard))
                new_soft, new_hard = resource.getrlimit(resource.RLIMIT_NOFILE)
                logger.info(f"Successfully increased file descriptor limit to: {new_soft}")
            except Exception as e:
                logger.warning(f"Could not increase file descriptor limit: {e}")
        
        # Check if we have enough file descriptors for processing
        if soft < 1000:
            logger.warning(f"File descriptor limit ({soft}) may be too low for large datasets")
            logger.warning("Consider increasing with: ulimit -n 4096")
            logger.warning("This may cause 'Too many open files' errors during processing")
        elif soft < 4096:
            logger.info(f"File descriptor limit ({soft}) is adequate for most datasets")
        else:
            logger.info(f"File descriptor limit ({soft}) is excellent for large datasets")
        
    except ImportError:
        logger.warning("resource module not available, cannot check file descriptor limits")
    except Exception as e:
        logger.warning(f"Error checking file descriptor limits: {e}")


def safe_file_operation(file_path: Path, operation: str = "read", encoding: str = "utf-8", **kwargs):
    """Safe file operation wrapper with proper error handling and resource cleanup."""
    try:
        if operation == "read":
            with open(file_path, "r", encoding=encoding, buffering=FILE_READ_BUFFER_SIZE) as f:
                return f.read()
        elif operation == "write":
            with open(file_path, "w", encoding=encoding) as f:
                return f.write(kwargs.get("content", ""))
        elif operation == "append":
            with open(file_path, "a", encoding=encoding) as f:
                return f.write(kwargs.get("content", ""))
        else:
            raise ValueError(f"Unsupported operation: {operation}")
    except Exception as e:
        logger.error(f"File operation failed on {file_path}: {e}")
        raise


# ====================================================================
# PATH CONFIGURATION
# ====================================================================


def setup_processing_paths(
    processing_dir: Path,
    enable_phone_prompts: bool = True,
    buffer_size: int = 8192,
    batch_size: int = 1000,
    cache_size: int = 25000,
    large_dataset: bool = False,
    output_format: str = "xml",
) -> None:
    """
    Set up all file paths based on the specified processing directory.

    Args:
        processing_dir: Path to the processing directory
        enable_phone_prompts: Whether to enable phone number alias prompts
        buffer_size: Buffer size for file I/O operations
        batch_size: Batch size for processing files
        cache_size: Cache size for performance optimization
        large_dataset: Whether this is a large dataset
        output_format: Output format ('xml' or 'html')

    Raises:
        ValueError: If parameters are invalid
        TypeError: If parameter types are incorrect
    """
    # Validate parameters
    if not isinstance(processing_dir, Path):
        raise TypeError(
            f"processing_dir must be a Path, got {type(processing_dir).__name__}"
        )
    if not isinstance(buffer_size, int) or buffer_size <= 0:
        raise ValueError(f"buffer_size must be a positive integer, got {buffer_size}")
    if not isinstance(batch_size, int) or batch_size <= 0:
        raise ValueError(f"batch_size must be a positive integer, got {batch_size}")
    if not isinstance(cache_size, int) or cache_size <= 0:
        raise ValueError(f"cache_size must be a positive integer, got {cache_size}")
    if not isinstance(output_format, str) or output_format not in ["xml", "html"]:
        raise ValueError(f"output_format must be 'xml' or 'html', got {output_format}")

    global PROCESSING_DIRECTORY, OUTPUT_DIRECTORY, LOG_FILENAME, CONVERSATION_MANAGER, PHONE_LOOKUP_MANAGER

    PROCESSING_DIRECTORY = Path(processing_dir).resolve()
    OUTPUT_DIRECTORY = PROCESSING_DIRECTORY / "conversations"
    LOG_FILENAME = str(PROCESSING_DIRECTORY / DEFAULT_LOG_FILENAME)

    # Create directories
    OUTPUT_DIRECTORY.mkdir(exist_ok=True)
    attachments_dir = OUTPUT_DIRECTORY / "attachments"
    attachments_dir.mkdir(exist_ok=True, mode=0o755)

    # Initialize managers with strict parameter validation
    CONVERSATION_MANAGER = strict_call(
        ConversationManager,
        OUTPUT_DIRECTORY,
        buffer_size,
        batch_size,
        False,
        output_format,
    )
    phone_lookup_file = PROCESSING_DIRECTORY / "phone_lookup.txt"
    PHONE_LOOKUP_MANAGER = strict_call(
        PhoneLookupManager, phone_lookup_file, enable_phone_prompts
    )

    logger.info(f"Processing directory: {PROCESSING_DIRECTORY}")
    logger.info(f"Output directory: {OUTPUT_DIRECTORY}")
    logger.info(f"Attachments directory: {attachments_dir}")

    if enable_phone_prompts:
        logger.info("Phone number alias prompts enabled")
    else:
        logger.info(
            "Phone number alias prompts disabled by default - using phone numbers as aliases"
        )


def validate_processing_directory(processing_dir: Path) -> bool:
    """
    Validate that the processing directory contains the expected structure.

    Args:
        processing_dir: Path to the directory to validate

    Returns:
        bool: True if directory is valid, False otherwise
    """
    processing_path = Path(processing_dir).resolve()

    # Check if directory exists
    if not processing_path.exists():
        logger.error(f"Processing directory does not exist: {processing_path}")
        return False

    if not processing_path.is_dir():
        logger.error(f"Processing path is not a directory: {processing_path}")
        return False

    # Check for expected subdirectories and files
    calls_dir = processing_path / "Calls"
    phones_vcf = processing_path / "Phones.vcf"

    if not calls_dir.exists():
        logger.error(f"Calls directory not found: {calls_dir}")
        logger.error(
            "This may indicate the directory structure is different than expected"
        )

    if not phones_vcf.exists():
        logger.error(f"Phones.vcf file not found: {phones_vcf}")
        logger.error(
            "This may indicate the directory structure is different than expected"
        )

    # Check for HTML files
    html_files = list(processing_path.rglob("*.html"))
    if not html_files:
        logger.error(
            f"No HTML files found in processing directory: {processing_path}"
        )
        logger.error(
            "This may indicate the directory is empty or contains no Google Voice data"
        )

    logger.info(f"Found {len(html_files)} HTML files in processing directory")
    return True


def validate_entire_configuration() -> bool:
    """
    Comprehensive validation of the entire application configuration.

    Returns:
        bool: True if configuration is valid, False otherwise
    """
    try:
        # Validate global variables are set
        if PROCESSING_DIRECTORY is None:
            logger.error("PROCESSING_DIRECTORY is not set")
            return False

        if OUTPUT_DIRECTORY is None:
            logger.error("OUTPUT_DIRECTORY is not set")
            return False

        if CONVERSATION_MANAGER is None:
            logger.error("CONVERSATION_MANAGER is not set")
            return False

        if PHONE_LOOKUP_MANAGER is None:
            logger.error("PHONE_LOOKUP_MANAGER is not set")
            return False

        # Validate managers have correct types
        if not hasattr(CONVERSATION_MANAGER, "output_format"):
            logger.error("CONVERSATION_MANAGER missing output_format attribute")
            return False

        if not hasattr(PHONE_LOOKUP_MANAGER, "phone_aliases"):
            logger.error("PHONE_LOOKUP_MANAGER missing phone_aliases attribute")
            return False

        # Validate output format
        if CONVERSATION_MANAGER.output_format not in ["xml", "html"]:
            logger.error(f"Invalid output format: {CONVERSATION_MANAGER.output_format}")
            return False

        logger.info("‚úÖ Configuration validation passed")
        return True

    except Exception as e:
        logger.error(f"Configuration validation failed: {e}")
        return False


def create_sample_config():
    """Create a sample configuration file for reference."""
    config_content = """# Google Voice SMS Converter Configuration
# This file shows the expected directory structure

# Expected directory structure:
# /path/to/your/data/
# ‚îú‚îÄ‚îÄ Calls/
# ‚îÇ   ‚îú‚îÄ‚îÄ conversation_123.html
# ‚îÇ   ‚îú‚îÄ‚îÄ conversation_456.html
# ‚îÇ   ‚îî‚îÄ‚îÄ ...
# ‚îú‚îÄ‚îÄ Phones.vcf
# ‚îî‚îÄ‚îÄ other_files...

# Usage examples:
# python sms.py /path/to/your/data
# python sms.py /path/to/your/data --output my-sms-backup.xml
# python sms.py /path/to/your/data --verbose --log detailed.log

# The script will:
# 1. Process all HTML files in the specified directory
# 2. Create output files in the same directory
# 3. Write logs to the same directory
# 4. Keep your source code directory clean for git commits
"""

    config_file = Path("sms-converter-config.txt")
    try:
        with open(config_file, "w") as f:
            f.write(config_content)
        logger.info(f"Sample configuration file created: {config_file}")
        logger.info("Review this file to understand the expected directory structure")
    except Exception as e:
        logger.error(f"Failed to create sample configuration: {e}")


def process_html_files_batch(
    html_files: List[Path], src_filename_map: Dict[str, str], batch_size: int = 100
) -> Dict[str, int]:
    """Process HTML files in batches for better memory management."""
    stats = {
        "num_sms": 0,
        "num_img": 0,
        "num_vcf": 0,
        "num_calls": 0,
        "num_voicemails": 0,
    }
    own_number = None

    total_files = len(html_files)
    logger.info(f"Processing {total_files} files in batches of {batch_size}")

    # Use parallel processing for large datasets
    if ENABLE_PARALLEL_PROCESSING and total_files > MEMORY_EFFICIENT_THRESHOLD:
        return process_html_files_parallel(html_files, src_filename_map, batch_size)

    # Sequential batch processing for smaller datasets - use generator for memory efficiency
    for i in range(0, total_files, batch_size):
        batch_files = html_files[i:i + batch_size]
        batch_start = time.time()

        logger.info(
            f"Processing batch {i//batch_size + 1}/{(total_files + batch_size - 1)//batch_size} "
            f"({len(batch_files)} files)"
        )

        for html_file in batch_files:
            try:
                file_stats = process_single_html_file(
                    html_file, src_filename_map, own_number
                )

                # Update statistics
                stats["num_sms"] += file_stats["num_sms"]
                stats["num_img"] += file_stats["num_img"]
                stats["num_vcf"] += file_stats["num_vcf"]
                stats["num_calls"] += file_stats["num_calls"]
                stats["num_voicemails"] += file_stats["num_voicemails"]

                # Extract own number from first file if not already found
                if own_number is None:
                    own_number = file_stats.get("own_number")

            except Exception as e:
                # CRITICAL: File processing failures are errors that need attention
                logger.error(f"Failed to process {html_file}: {e}")
                logger.debug(f"Continuing with next file to maintain processing flow")
                continue

        # Log batch performance
        batch_time = time.time() - batch_start
        logger.info(
            f"Batch completed in {batch_time:.2f}s "
            f"({len(batch_files)/batch_time:.2f} files/sec)"
        )

    return stats


def process_html_files_parallel(
    html_files: List[Path], src_filename_map: Dict[str, str], batch_size: int = 100
) -> Dict[str, int]:
    """Process HTML files using parallel processing for large datasets."""
    total_files = len(html_files)
    logger.info(
        f"Using parallel processing for {total_files} files with {MAX_WORKERS} workers"
    )

    # Split files into chunks for parallel processing - use generator for memory efficiency
    chunks = list(
        html_files[i:i + CHUNK_SIZE_OPTIMAL]
        for i in range(0, total_files, CHUNK_SIZE_OPTIMAL)
    )

    # Process chunks in parallel
    stats = {
        "num_sms": 0,
        "num_img": 0,
        "num_vcf": 0,
        "num_calls": 0,
        "num_voicemails": 0,
    }
    own_number = None

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # Submit chunk processing tasks
        future_to_chunk = {
            executor.submit(process_chunk_parallel, chunk, src_filename_map): chunk
            for chunk in chunks
        }

        # Collect results as they complete
        for future in as_completed(future_to_chunk):
            try:
                chunk_stats = future.result()

                # Aggregate statistics
                for key in stats:
                    stats[key] += chunk_stats.get(key, 0)

                # Extract own number from first successful chunk
                if own_number is None:
                    own_number = chunk_stats.get("own_number")

            except Exception as e:
                logger.error(f"Failed to process chunk: {e}")
                continue

    return stats


def process_chunk_parallel(
    html_files: List[Path], src_filename_map: Dict[str, str]
) -> Dict[str, int]:
    """Process a chunk of HTML files for parallel processing."""
    chunk_stats = {
        "num_sms": 0,
        "num_img": 0,
        "num_vcf": 0,
        "num_calls": 0,
        "num_voicemails": 0,
    }

    for html_file in html_files:
        try:
            file_stats = process_single_html_file(html_file, src_filename_map, None)

            # Update chunk statistics
            for key in chunk_stats:
                chunk_stats[key] += file_stats.get(key, 0)

        except Exception as e:
            logger.error(f"Failed to process {html_file}: {e}")
            continue

    return chunk_stats


def process_call_file(
    html_file: Path,
    soup: BeautifulSoup,
    own_number: Optional[str],
    src_filename_map: Dict[str, str],
) -> Dict[str, Union[int, str]]:
    """Process call files and return statistics."""
    try:
        # Extract call information
        call_info = extract_call_info(html_file.name, soup)
        if call_info:
            # Write call entry to conversation
            write_call_entry(str(html_file), call_info, own_number, soup)
            return {
                "num_sms": 0,
                "num_img": 0,
                "num_vcf": 0,
                "num_calls": 1,
                "num_voicemails": 0,
                "own_number": own_number,
            }
        else:
            # Check if this was due to date filtering (which is intentional, not an error)
            timestamp = extract_timestamp_from_call(soup)
            if timestamp and should_skip_message_by_date(timestamp):
                logger.debug(f"Call file {html_file.name} skipped due to date filtering")
            else:
                logger.error(f"Could not extract call information from {html_file.name}")
            return {
                "num_sms": 0,
                "num_img": 0,
                "num_vcf": 0,
                "num_calls": 0,
                "num_voicemails": 0,
                "own_number": own_number,
            }
    except Exception as e:
        logger.error(f"Failed to process call file {html_file.name}: {e}")
        return {
            "num_sms": 0,
            "num_img": 0,
            "num_vcf": 0,
            "num_calls": 0,
            "num_voicemails": 0,
            "own_number": own_number,
        }


def process_voicemail_file(
    html_file: Path,
    soup: BeautifulSoup,
    own_number: Optional[str],
    src_filename_map: Dict[str, str],
) -> Dict[str, Union[int, str]]:
    """Process voicemail files and return statistics."""
    try:
        # Extract voicemail information
        voicemail_info = extract_voicemail_info(html_file.name, soup)
        if voicemail_info:
            # Write voicemail entry to conversation
            write_voicemail_entry(str(html_file), voicemail_info, own_number, soup)
            return {
                "num_sms": 0,
                "num_img": 0,
                "num_vcf": 0,
                "num_calls": 0,
                "num_voicemails": 1,
                "own_number": own_number,
            }
        else:
            # Check if this was due to date filtering (which is intentional, not an error)
            timestamp = extract_timestamp_from_call(soup)
            if timestamp and should_skip_message_by_date(timestamp):
                logger.debug(f"Voicemail file {html_file.name} skipped due to date filtering")
            else:
                logger.error(
                    f"Could not extract voicemail information from {html_file.name}"
                )
            return {
                "num_sms": 0,
                "num_img": 0,
                "num_vcf": 0,
                "num_calls": 0,
                "num_voicemails": 0,
                "own_number": own_number,
            }
    except Exception as e:
        logger.error(f"Failed to process voicemail file {html_file.name}: {e}")
        return {
            "num_sms": 0,
            "num_img": 0,
            "num_vcf": 0,
            "num_calls": 0,
            "num_voicemails": 0,
            "own_number": own_number,
        }


def extract_call_info(
    filename: str, soup: BeautifulSoup
) -> Optional[Dict[str, Union[str, int]]]:
    """Extract call information from HTML content."""
    try:
        # Determine call type from filename
        filename_lower = filename.lower()
        if "placed" in filename_lower:
            call_type = "outgoing"
        elif "missed" in filename_lower:
            call_type = "missed"
        elif "received" in filename_lower:
            call_type = "incoming"
        else:
            call_type = "unknown"

        # Extract phone number/participant
        phone_number = extract_phone_from_call(soup, filename)

        # Extract timestamp from HTML content
        timestamp = extract_timestamp_from_call(soup)
        # Note: Keep timestamp as None if extraction fails - will be handled in write functions

        # Extract duration if available
        duration = extract_duration_from_call(soup)

        # ENHANCED: If phone number extraction failed, try additional fallback strategies
        if not phone_number:
            logger.debug(f"Primary phone extraction failed for call {filename}, trying alternatives")
            
            # Try to extract from filename if it contains a phone number
            phone_match = re.search(r"(\+\d{1,3}\s?\d{1,14})", filename)
            if phone_match:
                try:
                    phone_number = format_number(
                        phonenumbers.parse(phone_match.group(1), None)
                    )
                    logger.debug(f"Extracted phone number from filename: {phone_number}")
                except Exception as e:
                    logger.debug(f"Failed to parse phone number from filename: {e}")
            
            # Try to extract from any tel: links in the entire document
            if not phone_number:
                tel_links = soup.find_all("a", href=True)
                for link in tel_links:
                    href = link.get("href", "")
                    if href.startswith("tel:"):
                        match = TEL_HREF_PATTERN.search(href)
                        if match:
                            try:
                                phone_number = format_number(
                                    phonenumbers.parse(match.group(1), None)
                                )
                                logger.debug(f"Extracted phone number from tel link: {phone_number}")
                                break
                            except Exception as e:
                                logger.debug(f"Failed to parse phone number from tel link: {e}")
                                continue
            
            # ENHANCED: Try final fallback - look for any phone number patterns in the entire HTML
            if not phone_number:
                logger.debug(f"Trying final fallback phone extraction for call {filename}")
                # Look for any phone number patterns in the entire HTML content
                text_content = soup.get_text()
                phone_pattern = re.compile(r"(\+\d{1,3}\s?\d{1,14})")
                phone_match = phone_pattern.search(text_content)
                if phone_match:
                    try:
                        phone_number = format_number(
                            phonenumbers.parse(phone_match.group(1), None)
                        )
                        logger.debug(f"Extracted phone number from HTML content: {phone_number}")
                    except Exception as e:
                        logger.debug(f"Failed to parse phone number from HTML content: {e}")
        
        if phone_number:
            # DATE FILTERING: Skip calls outside the specified date range
            if timestamp and should_skip_message_by_date(timestamp):
                logger.debug(f"Skipping call due to date filtering: {filename}")
                return None
            
            # PHONE FILTERING: Skip numbers without aliases if filtering is enabled
            if FILTER_NUMBERS_WITHOUT_ALIASES and PHONE_LOOKUP_MANAGER:
                if not PHONE_LOOKUP_MANAGER.has_alias(str(phone_number)):
                    logger.debug(f"Skipping call from {phone_number} - no alias found and filtering enabled")
                    return None
                
                if PHONE_LOOKUP_MANAGER.is_excluded(str(phone_number)):
                    exclusion_reason = PHONE_LOOKUP_MANAGER.get_exclusion_reason(str(phone_number))
                    logger.debug(f"Skipping call from {phone_number} - explicitly excluded: {exclusion_reason}")
                    return None
            
            # NON-PHONE FILTERING: Skip toll-free and non-US numbers if filtering is enabled
            if FILTER_NON_PHONE_NUMBERS:
                if not is_valid_phone_number(str(phone_number), filter_non_phone=True):
                    logger.debug(f"Skipping call from {phone_number} - toll-free or non-US number filtered out")
                    return None
                
            return {
                "type": call_type,
                "phone_number": phone_number,
                "timestamp": timestamp,
                "duration": duration,
                "filename": filename,
            }
        
        # If we still don't have a phone number, log detailed debugging info
        logger.debug(f"Could not extract phone number for call {filename}")
        logger.debug(f"HTML content preview: {str(soup)[:500]}...")
        
        # ENHANCED: Provide more context about why extraction might have failed
        if filename and any(pattern in filename for pattern in [" - Received -", " - Placed -", " - Missed -"]):
            name_part = filename.split(" - ")[0] if " - " in filename else ""
            if not name_part.strip() or name_part.strip() in ['-', '.', '_']:
                logger.error(f"Filename '{filename}' has empty/malformed name part - this may cause extraction failures")
                logger.info(f"Consider checking if this file should be processed or if it's corrupted")
        
        # FINAL FALLBACK: Create a placeholder entry to prevent complete failure
        if not phone_number:
            logger.error(f"Creating placeholder call entry for {filename} due to extraction failure")
            # Generate a unique placeholder phone number
            placeholder_phone = f"unknown_call_{hash(filename) % 1000000}"
            return {
                "type": call_type,
                "phone_number": placeholder_phone,
                "timestamp": timestamp or int(time.time() * 1000),
                "duration": duration or "Unknown",
                "filename": filename,
            }
        
        return None

    except Exception as e:
        logger.error(f"Failed to extract call info: {e}")
        return None


def extract_voicemail_info(
    filename: str, soup: BeautifulSoup
) -> Optional[Dict[str, Union[str, int]]]:
    """Extract voicemail information from HTML content."""
    try:
        # Extract phone number/participant
        phone_number = extract_phone_from_call(soup, filename)  # Reuse call phone extraction

        # Extract timestamp from HTML content
        timestamp = extract_timestamp_from_call(soup)  # Reuse call timestamp extraction
        # Note: Keep timestamp as None if extraction fails - will be handled in write functions

        # Extract voicemail transcription if available
        transcription = extract_voicemail_transcription(soup)

        # Extract duration if available
        duration = extract_duration_from_call(soup)  # Reuse call duration extraction

        # Enhanced fallback: if phone number extraction fails, try alternative methods
        if not phone_number:
            logger.debug(f"Primary phone extraction failed for voicemail {filename}, trying alternatives")
            
            # Try to extract from filename if it contains a phone number
            phone_match = re.search(r"(\+\d{1,3}\s?\d{1,14})", filename)
            if phone_match:
                try:
                    phone_number = format_number(
                        phonenumbers.parse(phone_match.group(1), None)
                    )
                    logger.debug(f"Extracted phone number from filename: {phone_number}")
                except Exception as e:
                    logger.debug(f"Failed to parse phone number from filename: {e}")
            
            # Try to extract from any tel: links in the entire document
            if not phone_number:
                tel_links = soup.find_all("a", href=True)
                for link in tel_links:
                    href = link.get("href", "")
                    if href.startswith("tel:"):
                        match = TEL_HREF_PATTERN.search(href)
                        if match:
                            try:
                                phone_number = format_number(
                                    phonenumbers.parse(match.group(1), None)
                                )
                                logger.debug(f"Extracted phone number from tel link: {phone_number}")
                                break
                            except Exception as e:
                                logger.debug(f"Failed to parse phone number from tel link: {e}")
                                continue

        if phone_number:
            # DATE FILTERING: Skip voicemails outside the specified date range
            if timestamp and should_skip_message_by_date(timestamp):
                logger.debug(f"Skipping voicemail due to date filtering: {filename}")
                return None
            
            # PHONE FILTERING: Skip numbers without aliases if filtering is enabled
            if FILTER_NUMBERS_WITHOUT_ALIASES and PHONE_LOOKUP_MANAGER:
                if not PHONE_LOOKUP_MANAGER.has_alias(str(phone_number)):
                    logger.debug(f"Skipping voicemail from {phone_number} - no alias found and filtering enabled")
                    return None
                
                if PHONE_LOOKUP_MANAGER.is_excluded(str(phone_number)):
                    exclusion_reason = PHONE_LOOKUP_MANAGER.get_exclusion_reason(str(phone_number))
                    logger.debug(f"Skipping voicemail from {phone_number} - explicitly excluded: {exclusion_reason}")
                    return None
            
            # NON-PHONE FILTERING: Skip toll-free and non-US numbers if filtering is enabled
            if FILTER_NON_PHONE_NUMBERS:
                if not is_valid_phone_number(str(phone_number), filter_non_phone=True):
                    logger.debug(f"Skipping voicemail from {phone_number} - toll-free or non-US number filtered out")
                    return None
                
            return {
                "phone_number": phone_number,
                "timestamp": timestamp,
                "duration": duration,
                "transcription": transcription,
                "filename": filename,
            }
        
        # If we still don't have a phone number, log detailed debugging info
        logger.debug(f"Could not extract phone number for voicemail {filename}")
        logger.debug(f"HTML content preview: {str(soup)[:500]}...")
        
        # ENHANCED: Provide more context about why extraction might have failed
        if filename and (" - Voicemail -" in filename or " - Text -" in filename):
            name_part = filename.split(" - ")[0] if " - " in filename else ""
            if not name_part.strip() or name_part.strip() in ['-', '.', '_']:
                logger.error(f"Filename '{filename}' has empty/malformed name part - this may cause extraction failures")
                logger.info(f"Consider checking if this file should be processed or if it's corrupted")
        
        # ENHANCED: Try one more fallback - look for any phone number patterns in the entire HTML
        if not phone_number:
            logger.debug(f"Trying final fallback phone extraction for voicemail {filename}")
            # Look for any phone number patterns in the entire HTML content
            text_content = soup.get_text()
            phone_pattern = re.compile(r"(\+\d{1,3}\s?\d{1,14})")
            phone_match = phone_pattern.search(text_content)
            if phone_match:
                try:
                    phone_number = format_number(
                        phonenumbers.parse(phone_match.group(1), None)
                    )
                    logger.debug(f"Extracted phone number from HTML content: {phone_number}")
                    
                    # Return the voicemail info with the extracted phone number
                    return {
                        "phone_number": phone_number,
                        "timestamp": timestamp,
                        "duration": duration,
                        "transcription": transcription,
                        "filename": filename,
                    }
                except Exception as e:
                    logger.debug(f"Failed to parse phone number from HTML content: {e}")
        
        # FINAL FALLBACK: Create a placeholder entry to prevent complete failure
        if not phone_number:
            logger.error(f"Creating placeholder voicemail entry for {filename} due to extraction failure")
            # Generate a unique placeholder phone number
            placeholder_phone = f"unknown_voicemail_{hash(filename) % 1000000}"
            return {
                "phone_number": placeholder_phone,
                "timestamp": timestamp or int(time.time() * 1000),
                "duration": duration or "Unknown",
                "transcription": transcription or "[Extraction failed]",
                "filename": filename,
            }
        
        return None

    except Exception as e:
        logger.error(f"Failed to extract voicemail info: {e}")
        return None


def is_valid_phone_extraction(phone_candidate: str) -> bool:
    """
    Validate that a phone number candidate is not part of a log message or other inappropriate content.
    
    Args:
        phone_candidate: The phone number string to validate
        
    Returns:
        bool: True if the phone number looks legitimate, False if it's suspicious
    """
    # Check for log message patterns
    log_patterns = [
        "INFO", "DEBUG", "ERROR", "WARNING", "CRITICAL",
        "2025-", "2024-", "2023-", "2022-", "2021-", "2020-",
        "20:29:", "20:30:", "20:31:", "20:32:", "20:33:", "20:34:",
        "SMS processing progress", "File processing progress",
        "Completed SMS processing", "Completed processing"
    ]
    
    # Check if any log pattern is contained in the candidate
    for pattern in log_patterns:
        if pattern in phone_candidate:
            return False
    
    # Check if it looks like a reasonable phone number length
    # Remove all non-digits and check length
    digits_only = re.sub(r'[^0-9]', '', phone_candidate)
    if len(digits_only) < 7 or len(digits_only) > 15:
        return False
    
    # Additional validation: ensure it starts with + and contains only valid phone characters
    if not phone_candidate.startswith('+'):
        return False
    
    # Check for valid phone number characters only
    valid_chars = set('+0123456789 -()')
    if not all(c in valid_chars for c in phone_candidate):
        return False
    
    # Check that it's not just a name/alias (no letters)
    if re.search(r'[a-zA-Z]', phone_candidate):
        return False
    
    return True


def extract_phone_from_call(soup: BeautifulSoup, filename: str = None) -> Optional[str]:
    """Extract phone number from call/voicemail HTML."""
    try:
        # Look for phone number in various places
        # Try to find phone number in href attributes
        phone_links = soup.find_all("a", href=True)
        for link in phone_links:
            href = link.get("href", "")
            if href.startswith(STRING_POOL.PATTERNS["tel_href"]):
                phone_match = TEL_HREF_PATTERN.search(href)
                if phone_match:
                    return phone_match.group(1)

        # Try to find phone number in text content (but be more selective)
        # Use a more restrictive pattern to avoid matching log messages or other text
        phone_pattern = re.compile(r"(\+\d{1,3}\s?\d{3,4}\s?\d{3,4}\s?\d{3,4})")
        
        # Only look in specific elements that are likely to contain phone numbers
        # Avoid extracting from the entire document which might contain log messages
        phone_candidates = []
        
        # Look in cite elements (sender information)
        cite_elements = soup.find_all("cite")
        for cite in cite_elements:
            text = cite.get_text().strip()
            if text and len(text) > 2:
                phone_match = phone_pattern.search(text)
                if phone_match:
                    phone_candidates.append(phone_match.group(1))
        
        # Look in specific message elements
        message_elements = soup.find_all("div", class_="message")
        for msg in message_elements:
            # Only look in the cite part of messages
            cite = msg.find("cite")
            if cite:
                text = cite.get_text().strip()
                if text and len(text) > 2:
                    phone_match = phone_pattern.search(text)
                    if phone_match:
                        phone_candidates.append(phone_match.group(1))
        
        # Look in specific participant elements
        participant_elements = soup.find_all(["span", "div"], class_=lambda x: x and any(word in str(x).lower() for word in ["sender", "participant", "contact"]))
        for element in participant_elements:
            text = element.get_text().strip()
            if text and len(text) > 2:
                phone_match = phone_pattern.search(text)
                if phone_match:
                    phone_candidates.append(phone_match.group(1))
        
        # Return the first valid phone number found
        if phone_candidates:
            # Validate that it looks like a real phone number (not part of a log message)
            for candidate in phone_candidates:
                # Use the new validation function
                if is_valid_phone_extraction(candidate):
                    logger.debug(f"Found valid phone number: {candidate}")
                    return candidate
                else:
                    logger.debug(f"Phone number candidate failed validation: {candidate}")
            
            # If all candidates fail validation, return None
            logger.debug(f"All phone number candidates failed validation: {phone_candidates}")
            return None

        # Note: We don't return names/aliases from phone extraction functions
        # Names should be extracted separately and not treated as phone numbers
        # This prevents issues like "Kang_Landlord" being treated as a phone number

        # ENHANCED: Try to extract phone number from filename if provided
        if filename:
            # Look for phone number patterns in filename
            phone_match = phone_pattern.search(filename)
            if phone_match:
                try:
                    phone_number = format_number(
                        phonenumbers.parse(phone_match.group(1), None)
                    )
                    logger.debug(f"Extracted phone number from filename: {phone_number}")
                    return phone_number
                except Exception as e:
                    logger.debug(f"Failed to parse phone number from filename: {e}")
            
            # ENHANCED: Extract name from filename and create hash-based phone number
            # This handles cases like "Transwood - Received - ..." without phone numbers
            for pattern in [" - Received - ", " - Placed - ", " - Missed - "]:
                if pattern in filename:
                    name_part = filename.split(pattern)[0]
                    if name_part and not name_part.isdigit():
                        # Create a consistent hash-based phone number for the same name
                        hash_value = hash(name_part) % 100000000  # 8-digit number
                        logger.debug(f"Generated hash-based phone number for {name_part}: {hash_value}")
                        return str(hash_value)

        return None

    except Exception as e:
        logger.debug(f"Failed to extract phone from call: {e}")
        return None


def extract_timestamp_from_call(soup: BeautifulSoup) -> Optional[int]:
    """Extract timestamp from call/voicemail HTML."""
    try:
        # Look for timestamp in various formats - use cached selectors for performance
        # First, try to find published elements (this is where call/voicemail timestamps are stored)
        published_elements = soup.select(STRING_POOL.ADDITIONAL_SELECTORS["published_elements"])
        for element in published_elements:
            datetime_attr = element.get("title", "")
            if datetime_attr:
                try:
                    # Parse ISO format datetime; fall back to dateutil for robustness
                    try:
                        dt = datetime.fromisoformat(datetime_attr.replace("Z", "+00:00"))
                    except Exception:
                        dt = dateutil.parser.parse(datetime_attr)
                    return int(dt.timestamp() * 1000)  # Convert to milliseconds
                except Exception:
                    continue

        # Try to find abbr elements with datetime (fallback)
        time_elements = soup.select(STRING_POOL.ADDITIONAL_SELECTORS["dt_elements"])
        for element in time_elements:
            datetime_attr = element.get("title", "")
            if datetime_attr:
                try:
                    # Parse ISO format datetime; fall back to dateutil for robustness
                    try:
                        dt = datetime.fromisoformat(datetime_attr.replace("Z", "+00:00"))
                    except Exception:
                        dt = dateutil.parser.parse(datetime_attr)
                    return int(dt.timestamp() * 1000)  # Convert to milliseconds
                except Exception:
                    continue

        # Try to find other time elements - use cached selector
        time_elements = soup.select(STRING_POOL.ADDITIONAL_SELECTORS["time_elements"])
        for element in time_elements:
            datetime_attr = element.get("datetime", "")
            if datetime_attr:
                try:
                    try:
                        dt = datetime.fromisoformat(datetime_attr.replace("Z", "+00:00"))
                    except ValueError:
                        dt = dateutil.parser.parse(datetime_attr)
                    return int(dt.timestamp() * 1000)
                except Exception:
                    continue

        # Try to find timestamp in text content (look for date patterns)
        text_content = soup.get_text()
        
        # Look for ISO format dates in text
        iso_pattern = re.compile(r'(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}(?:Z|[+-]\d{2}:\d{2})?)')
        iso_match = iso_pattern.search(text_content)
        if iso_match:
            try:
                dt = dateutil.parser.parse(iso_match.group(1))
                return int(dt.timestamp() * 1000)
            except Exception:
                pass
        
        # Look for other date formats
        date_patterns = [
            r'(\d{4}-\d{2}-\d{2})',  # YYYY-MM-DD
            r'(\d{1,2}/\d{1,2}/\d{4})',  # MM/DD/YYYY
            r'(\d{1,2}-\d{1,2}-\d{4})',  # MM-DD-YYYY
        ]
        
        for pattern in date_patterns:
            date_match = re.search(pattern, text_content)
            if date_match:
                try:
                    dt = dateutil.parser.parse(date_match.group(1))
                    return int(dt.timestamp() * 1000)
                except Exception:
                    continue

        # Do not default to current time here; indicate failure by returning None
        return None

    except Exception as e:
        logger.debug(f"Failed to extract timestamp from call: {e}")
        return None


def extract_duration_from_call(soup: BeautifulSoup) -> Optional[str]:
    """Extract call duration from HTML content."""
    try:
        # Look for duration information - use cached selector for performance
        duration_elements = soup.select(STRING_POOL.ADDITIONAL_SELECTORS["duration_elements"])
        for element in duration_elements:
            text = element.get_text().strip()
            if re.match(r"\d+:\d+", text):  # Format like "1:23"
                return text

        # Look for duration in text content
        text_content = soup.get_text()
        duration_match = re.search(r"(\d+):(\d+)", text_content)
        if duration_match:
            return f"{duration_match.group(1)}:{duration_match.group(2)}"

        return None

    except Exception as e:
        logger.debug(f"Failed to extract duration from call: {e}")
        return None


def extract_voicemail_transcription(soup: BeautifulSoup) -> Optional[str]:
    """Extract voicemail transcription from HTML content."""
    try:
        # Look for transcription in various places
        # Try to find transcription elements - use cached selector for performance
        transcription_elements = soup.select(STRING_POOL.ADDITIONAL_SELECTORS["transcription_elements"])
        for element in transcription_elements:
            text = element.get_text().strip()
            if text and len(text) > 10:  # Reasonable length for transcription
                return text

        # Look for any substantial text content
        text_content = soup.get_text()
        # Split by lines and find the longest meaningful text
        lines = [line.strip() for line in text_content.split("\n") if line.strip()]
        meaningful_lines = [
            line for line in lines if len(line) > 10 and not line.isdigit()
        ]

        if meaningful_lines:
            # Return the longest meaningful line
            return max(meaningful_lines, key=len)

        return None

    except Exception as e:
        logger.debug(f"Failed to extract voicemail transcription: {e}")
        return None


def write_call_entry(
    filename: str,
    call_info: Dict[str, Union[str, int]],
    own_number: Optional[str],
    soup: Optional[BeautifulSoup] = None,
):
    """Write a call entry to the conversation."""
    try:
        # Get alias for the phone number
        phone_number = str(call_info["phone_number"])
        alias = PHONE_LOOKUP_MANAGER.get_alias(phone_number, soup)

        # Extract call details from the already parsed soup if available, otherwise from file
        if soup:
            # Use the already parsed soup to avoid re-opening the file
            call_details = extract_call_details_from_soup(soup)
        else:
            # Fallback to file-based extraction (should be rare)
            call_details = extract_call_details(filename)

        # Use the rich call details from the HTML file
        message_text = call_details["message_text"]

        # Create SMS-like entry for the call
        call_ts = call_info.get("timestamp")
        if call_ts is None:
            # Use the already parsed soup if available, otherwise re-parse
            if soup:
                call_ts = extract_timestamp_from_call(soup)
            else:
                # Attempt to re-extract directly from the file content
                try:
                    file_path = Path(filename)
                    if not file_path.is_absolute():
                        file_path = PROCESSING_DIRECTORY / "Calls" / file_path.name
                    with open(file_path, "r", encoding="utf-8") as f:
                        soup2 = BeautifulSoup(f.read(), "html.parser")
                    call_ts = extract_timestamp_from_call(soup2)
                except Exception:
                    pass
            
            # If still no timestamp, use file modification time as last resort
            if call_ts is None:
                try:
                    file_path = Path(filename)
                    if not file_path.is_absolute():
                        file_path = PROCESSING_DIRECTORY / "Calls" / file_path.name
                    call_ts = int(file_path.stat().st_mtime * 1000)
                except Exception:
                    # If all else fails, use current time
                    call_ts = int(time.time() * 1000)
        sms_values = {
            "alias": alias,
            "type": 1,  # Treat as received message
            "message": message_text,
            "time": call_ts,
        }

        # Format SMS XML
        sms_text = format_sms_xml(sms_values)

        # Write to conversation file
        conversation_id = CONVERSATION_MANAGER.get_conversation_id(
            [phone_number], False
        )
        if CONVERSATION_MANAGER.output_format == "html":
            # For HTML output, use the rich call details
            message_text = call_details["message_text"]
            attachments = []
            CONVERSATION_MANAGER.write_message_with_content(
                conversation_id,
                message_text,
                attachments,
                call_ts,
                sender=alias,
            )
        else:
            # For XML output, use the XML format
            CONVERSATION_MANAGER.write_message(
                conversation_id, sms_text, call_ts
            )

        # Update conversation statistics
        CONVERSATION_MANAGER.update_stats(conversation_id, {"num_calls": 1})

        logger.info(f"Added call entry: {message_text}")

    except Exception as e:
        logger.error(f"Failed to write call entry: {e}")


def extract_call_details_from_soup(soup: BeautifulSoup) -> Dict[str, str]:
    """Extract detailed call information from an already parsed BeautifulSoup object."""
    try:
        # Extract call type from title or content
        title = soup.find("title")
        call_type = "Unknown"
        if title:
            title_text = title.get_text().lower()
            if "missed" in title_text:
                call_type = "Missed"
            elif "placed" in title_text:
                call_type = "Placed"
            elif "received" in title_text:
                call_type = "Received"

        # Extract duration
        duration_elem = soup.find("abbr", class_="duration")
        duration = ""
        if duration_elem:
            duration_title = duration_elem.get("title", "")
            duration_text = duration_elem.get_text()
            if duration_title:
                # Parse ISO 8601 duration (PT4S, PT9M16S, etc.)
                duration = parse_iso_duration(duration_title)
            elif duration_text:
                duration = duration_text.strip("()")

        # Extract contact name if available
        contact_name = ""
        
        # Method 1: Look for contributor div with fn span (older format)
        contributor = soup.find("div", class_="contributor")
        if contributor:
            fn_elem = contributor.find("span", class_="fn")
            if fn_elem:
                contact_name = fn_elem.get_text().strip()
        
        # Method 2: Look for tel link text (newer format like our test files)
        if not contact_name:
            tel_link = soup.find("a", class_="tel", href=True)
            if tel_link:
                link_text = tel_link.get_text(strip=True)
                if link_text and link_text.lower() not in ["unknown", "me", "placed call to", "received call from", "missed call from"]:
                    contact_name = link_text

        # Build rich message text
        if call_type == "Missed":
            message_text = f"üìû Missed call from {contact_name or 'Unknown'}"
        elif call_type == "Placed":
            message_text = f"üìû Outgoing call to {contact_name or 'Unknown'}"
        elif call_type == "Received":
            message_text = f"üìû Incoming call from {contact_name or 'Unknown'}"
        else:
            message_text = f"üìû Call from {contact_name or 'Unknown'}"

        # Add duration if available
        if duration:
            message_text += f" (Duration: {duration})"

        return {
            "call_type": call_type,
            "duration": duration,
            "contact_name": contact_name,
            "message_text": message_text,
        }

    except Exception as e:
        logger.error(f"Failed to extract call details from soup: {e}")
        return {
            "call_type": "Unknown",
            "duration": "",
            "contact_name": "",
            "message_text": "üìû Call log entry",
        }


def extract_call_details(filename: str) -> Dict[str, str]:
    """Extract detailed call information from the HTML file."""
    try:
        logger.debug(f"Extracting call details from: {filename}")
        # Read the call HTML file (resolve under processing Calls dir if needed)
        file_path = Path(filename)
        if not file_path.is_absolute():
            file_path = PROCESSING_DIRECTORY / "Calls" / file_path.name
        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read()

        # Parse with BeautifulSoup
        soup = BeautifulSoup(content, "html.parser")

        # Extract call type from title or content
        title = soup.find("title")
        call_type = "Unknown"
        if title:
            title_text = title.get_text().lower()
            if "missed" in title_text:
                call_type = "Missed"
            elif "placed" in title_text:
                call_type = "Placed"
            elif "received" in title_text:
                call_type = "Received"

        # Extract duration
        duration_elem = soup.find("abbr", class_="duration")
        duration = ""
        if duration_elem:
            duration_title = duration_elem.get("title", "")
            duration_text = duration_elem.get_text()
            if duration_title:
                # Parse ISO 8601 duration (PT4S, PT9M16S, etc.)
                duration = parse_iso_duration(duration_title)
            elif duration_text:
                duration = duration_text.strip("()")

        # Extract contact name if available
        contributor = soup.find("div", class_="contributor")
        contact_name = ""
        if contributor:
            fn_elem = contributor.find("span", class_="fn")
            if fn_elem:
                contact_name = fn_elem.get_text().strip()

        # Build rich message text
        if call_type == "Missed":
            message_text = f"üìû Missed call from {contact_name or 'Unknown'}"
        elif call_type == "Placed":
            message_text = f"üìû Outgoing call to {contact_name or 'Unknown'}"
        elif call_type == "Received":
            message_text = f"üìû Incoming call from {contact_name or 'Unknown'}"
        else:
            message_text = f"üìû Call from {contact_name or 'Unknown'}"

        # Add duration if available
        if duration:
            message_text += f" (Duration: {duration})"

        return {
            "call_type": call_type,
            "duration": duration,
            "contact_name": contact_name,
            "message_text": message_text,
        }

    except Exception as e:
        logger.error(f"Failed to extract call details from {filename}: {e}")
        return {
            "call_type": "Unknown",
            "duration": "",
            "contact_name": "",
            "message_text": "üìû Call log entry",
        }


def parse_iso_duration(duration_str: str) -> str:
    """Parse ISO 8601 duration format (PT4S, PT9M16S, etc.) to human readable format."""
    try:
        # Remove 'PT' prefix
        if duration_str.startswith("PT"):
            duration_str = duration_str[2:]

        # Parse components
        hours = 0
        minutes = 0
        seconds = 0

        # Find hours (H)
        if "H" in duration_str:
            parts = duration_str.split("H")
            hours = int(parts[0])
            duration_str = parts[1]

        # Find minutes (M)
        if "M" in duration_str:
            parts = duration_str.split("M")
            minutes = int(parts[0])
            duration_str = parts[1]

        # Find seconds (S)
        if "S" in duration_str:
            seconds = int(duration_str[:-1])

        # Format output
        if hours > 0:
            return f"{hours:02d}:{minutes:02d}:{seconds:02d}"
        elif minutes > 0:
            return f"{minutes:02d}:{seconds:02d}"
        else:
            return f"{seconds}s"

    except Exception as e:
        logger.error(f"Failed to parse duration '{duration_str}': {e}")
        return duration_str


def write_voicemail_entry(
    filename: str,
    voicemail_info: Dict[str, Union[str, int]],
    own_number: Optional[str],
    soup: Optional[BeautifulSoup] = None,
):
    """Write a voicemail entry to the conversation."""
    try:
        # Get alias for the phone number
        phone_number = str(voicemail_info["phone_number"])
        alias = PHONE_LOOKUP_MANAGER.get_alias(phone_number, soup)

        # Create voicemail message content
        duration = voicemail_info.get("duration", "")
        transcription = voicemail_info.get("transcription", "")

        message_text = f"üéôÔ∏è Voicemail from {alias}"
        if duration:
            message_text += f" (Duration: {duration})"

        if transcription:
            message_text += f"\n\nTranscription:\n{transcription}"

        # Create SMS-like entry for the voicemail
        vm_ts = voicemail_info.get("timestamp")
        if vm_ts is None:
            # Use the already parsed soup if available, otherwise re-parse
            if soup:
                vm_ts = extract_timestamp_from_call(soup)
            else:
                try:
                    file_path = Path(filename)
                    if not file_path.is_absolute():
                        file_path = PROCESSING_DIRECTORY / "Calls" / file_path.name
                    with open(file_path, "r", encoding="utf-8") as f:
                        soup2 = BeautifulSoup(f.read(), "html.parser")
                    vm_ts = extract_timestamp_from_call(soup2)
                    if vm_ts is None:
                        # If still no timestamp, use file modification time as last resort
                        vm_ts = int(file_path.stat().st_mtime * 1000)
                except Exception:
                    # If all else fails, use current time
                    vm_ts = int(time.time() * 1000)
        sms_values = {
            "alias": alias,
            "type": 1,  # Treat as received message
            "message": message_text,
            "time": vm_ts,
        }

        # Format SMS XML
        sms_text = format_sms_xml(sms_values)

        # Write to conversation file
        conversation_id = CONVERSATION_MANAGER.get_conversation_id(
            [phone_number], False
        )
        if CONVERSATION_MANAGER.output_format == "html":
            # For HTML output, extract text and attachments directly
            message_text = voicemail_info.get("message", message_text)
            if not message_text or message_text.strip() == "":
                message_text = "[Voicemail entry]"
            attachments = []
            CONVERSATION_MANAGER.write_message_with_content(
                conversation_id,
                message_text,
                attachments,
                vm_ts,
                sender=alias,
            )
        else:
            # For XML output, use the XML format
            CONVERSATION_MANAGER.write_message(
                conversation_id, sms_text, vm_ts
            )

        # Update conversation statistics
        CONVERSATION_MANAGER.update_stats(conversation_id, {"num_voicemails": 1})

        logger.info(f"Added voicemail entry: {message_text[:50]}...")

    except Exception as e:
        logger.error(f"Failed to write voicemail entry: {e}")


if __name__ == "__main__":
    try:
        # Parse command line arguments
        parser = argparse.ArgumentParser(
            description="Convert Google Voice HTML export files to SMS backup XML format",
            formatter_class=argparse.RawDescriptionHelpFormatter,
            epilog="""
Examples:
  # Process files in default directory (../gvoice-convert/) with high-performance defaults
  python sms.py

  # Process files in specified directory with high-performance defaults
  python sms.py /path/to/gvoice/data

  # Process files with verbose logging
  python sms.py /path/to/gvoice/data --verbose

  # Process files with debug logging (most detailed)
  python sms.py /path/to/gvoice/data --debug

  # Process files with specific log level
  python sms.py /path/to/gvoice/data --log-level WARNING

  # Process files with phone number alias prompts (interactive)
  python sms.py /path/to/gvoice/data --phone-prompts

  # Process files with custom performance settings (overriding high-performance defaults)
  python sms.py /path/to/gvoice/data --buffer-size 16384 --cache-size 25000

  # Process large datasets with optimized settings (using high-performance defaults)
  python sms.py /path/to/gvoice/data --large-dataset --batch-size 2000

  # Performance tuning for large datasets (50,000+ entries) - using high-performance defaults
  python sms.py /path/to/gvoice/data --workers 16 --chunk-size 1000 --enable-mmap

  # Memory-efficient processing for very large datasets (overriding high-performance defaults)
  python sms.py /path/to/gvoice/data --memory-efficient --buffer-size 256

  # Test mode with limited processing (default: 100 entries, auto-enables debug + strict mode)
  python sms.py /path/to/gvoice/data --test-mode

  # Test mode with custom limit (auto-enables debug + strict mode)
  python sms.py /path/to/gvoice/data --test-mode --test-limit 50

  # Test mode with custom logging (strict mode still auto-enabled)
  python sms.py /path/to/gvoice/data --test-mode --test-limit 50 --log-level WARNING

  # Full run mode (process all entries)
  python sms.py /path/to/gvoice/data --full-run

  # Output in XML format instead of HTML (HTML is now default)
  python sms.py /path/to/gvoice/data --output-format xml

          # Filtering options
        python sms.py /path/to/gvoice/data --include-service-codes  # Include service codes (default: filtered out)
        python sms.py /path/to/gvoice/data --older-than 2023-01-01  # Filter out messages older than 2023
        python sms.py /path/to/gvoice/data --newer-than 2024-12-31  # Filter out messages newer than 2024
        python sms.py /path/to/gvoice/data --older-than "2023-06-15 14:30:00"  # Filter with time precision
        python sms.py /path/to/gvoice/data --filter-no-alias  # Only process numbers with aliases/names
        python sms.py /path/to/gvoice/data --exclude-no-alias  # Alternative to --filter-no-alias
        python sms.py /path/to/gvoice/data --filter-non-phone  # Filter out toll-free and non-US numbers
  
  # Default behavior: Service codes (verification codes, alerts) are filtered out for cleaner output
  # Use --include-service-codes to include all service codes and short codes

  # Enable strict parameter validation (catches errors early)
  python sms.py /path/to/gvoice/data --strict-mode

  # Create sample configuration file
  python sms.py --create-config

  # Show help
  python sms.py --help

Directory Structure:
  The script expects a directory containing:
  - Calls/ subdirectory with HTML conversation files
  - Phones.vcf file with contact information
  - Any other Google Voice export files

Output:
  - Creates a 'conversations' subdirectory in the processing directory
  - Generates separate HTML or XML files for each sender/group conversation (HTML is default)
  - Messages are sorted chronologically within each conversation file
  - HTML output: Human-readable table format with styling and attachment indicators (default)
  - XML output: Standard SMS backup format compatible with SMS backup apps
  - Phone numbers are mapped to user-friendly aliases (phone prompts disabled by default)
  - High-performance defaults optimized for large datasets (50,000+ messages)
  - Optimized file I/O with large buffer sizes (32KB default)
  - Enhanced caching with large cache sizes (50,000 default)
  - Batch processing optimized for large datasets (1,000 default)
  - Memory-efficient data structures and string pooling
  - Optimized parallel processing with increased workers (16 max) and chunk sizes (1,000 default)
  - Smart memory mapping for large files (>5MB) with fallback to buffered I/O
  - String pooling reduces memory allocations and garbage collection pressure
  - Test mode enabled by default (processes 100 entries for safety)
  - Configurable test limits and full-run mode available
  - Logs are written to the processing directory
            """,
        )

        parser.add_argument(
            "processing_dir",
            nargs="?",
            default="../gvoice-convert/",
            help="Directory containing Google Voice export data (Calls/ and Phones.vcf). Defaults to ../gvoice-convert/ directory.",
        )

        parser.add_argument(
            "--log", "-l", help="Custom log filename (default: gvoice_converter.log)"
        )

        parser.add_argument(
            "--verbose",
            "-v",
            action="store_true",
            help="Enable verbose logging (INFO level)",
        )

        parser.add_argument(
            "--debug",
            "-d",
            action="store_true",
            help="Enable debug logging (DEBUG level)",
        )

        parser.add_argument(
            "--log-level",
            choices=["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
            default="INFO",
            help="Set specific log level (default: INFO)",
        )

        parser.add_argument(
            "--debug-attachments",
            action="store_true",
            help="Enable detailed debugging for attachment matching (shows why attachments fail to match)",
        )

        # Performance optimization arguments
        parser.add_argument(
            "--no-parallel",
            action="store_true",
            help="Disable parallel processing (use sequential processing instead)",
        )

        parser.add_argument(
            "--workers",
            type=int,
            default=MAX_WORKERS,
            help=f"Number of parallel workers (default: {MAX_WORKERS})",
        )

        parser.add_argument(
            "--chunk-size",
            type=int,
            default=CHUNK_SIZE_OPTIMAL,
            help=f"Chunk size for parallel processing (default: {CHUNK_SIZE_OPTIMAL})",
        )

        parser.add_argument(
            "--memory-threshold",
            type=int,
            default=MEMORY_EFFICIENT_THRESHOLD,
            help=f"Threshold for switching to memory-efficient mode (default: {MEMORY_EFFICIENT_THRESHOLD:,})",
        )

        parser.add_argument(
            "--no-streaming",
            action="store_true",
            help="Disable streaming file parsing (use traditional approach)",
        )

        parser.add_argument(
            "--no-mmap",
            action="store_true",
            help="Disable memory mapping for large files",
        )

        parser.add_argument(
            "--no-progress", action="store_true", help="Disable progress logging"
        )

        parser.add_argument(
            "--no-performance",
            action="store_true",
            help="Disable performance monitoring",
        )

        parser.add_argument(
            "--strict-mode",
            action="store_true",
            help="Enable strict parameter validation (catches parameter mismatches early)",
        )

        parser.add_argument(
            "--create-config",
            action="store_true",
            help="Create a sample configuration file showing expected directory structure",
        )

        parser.add_argument(
            "--phone-prompts",
            action="store_true",
            help="Enable interactive phone number alias prompts (disabled by default)",
        )

        parser.add_argument(
            "--buffer-size",
            type=int,
            default=32768,
            help="File I/O buffer size in bytes (default: 32768 - optimized for large datasets)",
        )

        parser.add_argument(
            "--cache-size",
            type=int,
            default=50000,
            help="LRU cache size for performance optimization (default: 50000 - optimized for large datasets)",
        )

        parser.add_argument(
            "--batch-size",
            type=int,
            default=1000,
            help="Batch size for processing large datasets (default: 1000 - optimized for large datasets)",
        )

        parser.add_argument(
            "--large-dataset",
            action="store_true",
            help="Enable optimizations for datasets with 50,000+ messages",
        )

        parser.add_argument(
            "--test-mode",
            action="store_true",
            help="Enable testing mode with limited processing (default: 100 entries). Auto-enables debug logging and strict mode for better troubleshooting.",
        )

        parser.add_argument(
            "--test-limit",
            type=int,
            default=100,
            help="Number of entries to process in test mode (default: 100)",
        )

        parser.add_argument(
            "--full-run",
            action="store_true",
            help="Disable test mode and process all entries (overrides --test-mode)",
        )

        parser.add_argument(
            "--output-format",
            "-f",
            choices=["xml", "html"],
            default="html",
            help="Output format for conversation files: html (default) or xml",
        )

        # Filtering options
        parser.add_argument(
            "--include-service-codes",
            action="store_true",
            help="Include service codes and short codes in processing (default: False - service codes are filtered out)",
        )

        parser.add_argument(
            "--older-than",
            type=str,
            help="Filter out messages older than specified date (format: YYYY-MM-DD or YYYY-MM-DD HH:MM:SS)",
        )

        parser.add_argument(
            "--newer-than",
            type=str,
            help="Filter out messages newer than specified date (format: YYYY-MM-DD or YYYY-MM-DD HH:MM:SS)",
        )

        # Phone number filtering options
        parser.add_argument(
            "--filter-no-alias",
            action="store_true",
            help="Filter out phone numbers that don't have aliases (only process numbers with names/aliases)",
        )

        parser.add_argument(
            "--exclude-no-alias",
            action="store_true",
            help="Alternative to --filter-no-alias: exclude numbers without aliases",
        )

        parser.add_argument(
            "--filter-non-phone",
            action="store_true",
            help="Filter out toll-free numbers (800, 877, 888, etc.) and non-US numbers",
        )

        args = parser.parse_args()

        # Handle create-config option
        if args.create_config:
            create_sample_config()
            sys.exit(0)

        # Configure test mode first (before any other operations)
        if args.full_run:
            # Full run mode - process all entries
            set_test_mode(False, 0)
        elif args.test_mode:
            # Explicit test mode with custom limit
            set_test_mode(True, args.test_limit)
        else:
            # Default to test mode for safety (limited processing)
            set_test_mode(True, 100)

        # Auto-enable debug logging and strict mode for test mode
        if TEST_MODE:
            # Only auto-enable debug if user hasn't specified any logging options
            if not args.debug and not args.verbose and args.log_level == "INFO":
                args.debug = True
                logger.info(
                    "üß™ TEST MODE: Auto-enabling debug logging for better troubleshooting"
                )
            if not args.strict_mode:
                args.strict_mode = True
                logger.info(
                    "üß™ TEST MODE: Auto-enabling strict mode for parameter validation"
                )

        # Configure logging FIRST before any other operations
        # Determine log level based on arguments
        if args.debug:
            log_level = logging.DEBUG
        elif args.verbose:
            log_level = logging.INFO
        else:
            # Parse the log level string to logging level constant
            log_level = getattr(logging, args.log_level.upper())

        logging.basicConfig(
            level=log_level,
            format="%(asctime)s - %(levelname)s - %(message)s",
            handlers=[logging.StreamHandler(sys.stdout)],
        )

        # Log the configured log level
        logger.info(f"üìù Log level set to: {logging.getLevelName(log_level)}")

        # Enable debug logging for attachments if requested
        if args.debug_attachments:
            # Set attachment-related loggers to DEBUG level
            logging.getLogger().setLevel(logging.DEBUG)
            logger.info(
                "üîç Debug attachment logging enabled - will show detailed matching information"
            )

        # Configure module-specific logging if debug mode is enabled
        if args.debug:
            # Set specific modules to DEBUG level for detailed troubleshooting
            logging.getLogger(__name__).setLevel(logging.DEBUG)
            logging.getLogger("concurrent.futures").setLevel(
                logging.INFO
            )  # Reduce noise from parallel processing
            logger.info("üêõ Debug mode enabled - detailed logging for all modules")

        # Set up processing paths with strict parameter validation
        enable_phone_prompts = (
            args.phone_prompts
        )  # Default to False (no prompts), True only if --phone-prompts is used

        # Use strict_call to validate parameters
        strict_call(
            setup_processing_paths,
            Path(args.processing_dir),
            enable_phone_prompts,
            args.buffer_size,
            args.batch_size,
            args.cache_size,  # Use the new high-performance default (50000)
            False,  # large_dataset with defaults
            args.output_format,
        )

        # Override default log filename if specified
        if args.log:
            LOG_FILENAME = str(PROCESSING_DIRECTORY / args.log)
            logger.info(f"Using custom log file: {LOG_FILENAME}")

        # Add file handler after processing paths are set up
        file_handler = logging.FileHandler(LOG_FILENAME)
        file_handler.setFormatter(
            logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
        )
        logging.getLogger().addHandler(file_handler)

        # Configure progress and performance monitoring
        if args.no_progress:
            ENABLE_PROGRESS_LOGGING = False
            logger.info("Progress logging disabled")

        # Apply performance optimization settings from command line
        if args.no_parallel:
            ENABLE_PARALLEL_PROCESSING = False
            logger.info("Parallel processing disabled")

        if args.workers != MAX_WORKERS:
            MAX_WORKERS = max(1, min(args.workers, 20))  # Limit to 1-20 workers (increased for high performance)
            logger.info(f"Parallel workers set to: {MAX_WORKERS}")

        if args.chunk_size != CHUNK_SIZE_OPTIMAL:
            CHUNK_SIZE_OPTIMAL = max(50, min(args.chunk_size, 2000))  # Limit to 50-2000
            logger.info(f"Chunk size set to: {CHUNK_SIZE_OPTIMAL}")

        if args.memory_threshold != MEMORY_EFFICIENT_THRESHOLD:
            MEMORY_EFFICIENT_THRESHOLD = max(
                100, min(args.memory_threshold, 100000)
            )  # Limit to 100-100k
            logger.info(f"Memory threshold set to: {MEMORY_EFFICIENT_THRESHOLD:,}")

        if args.no_streaming:
            ENABLE_STREAMING_PARSING = False
            logger.info("Streaming file parsing disabled")

        if args.no_mmap:
            ENABLE_MMAP_FOR_LARGE_FILES = False
            logger.info("Memory mapping disabled")

        # Log final performance configuration
        logger.info("Final Performance Configuration:")
        logger.info(f"  Parallel processing: {ENABLE_PARALLEL_PROCESSING}")
        logger.info(f"  Max workers: {MAX_WORKERS}")
        logger.info(f"  Chunk size: {CHUNK_SIZE_OPTIMAL}")
        logger.info(f"  Memory threshold: {MEMORY_EFFICIENT_THRESHOLD:,}")
        logger.info(f"  Streaming parsing: {ENABLE_STREAMING_PARSING}")
        logger.info(f"  Memory mapping: {ENABLE_MMAP_FOR_LARGE_FILES}")

        if args.no_performance:
            ENABLE_PERFORMANCE_MONITORING = False
            logger.info("Performance monitoring disabled")

        # Configure strict mode if requested
        if args.strict_mode:
            logger.info(
                "üîí STRICT MODE ENABLED - All function calls will be validated for parameter correctness"
            )
            # Override the strict_call function to always validate
            globals()["strict_call"] = strict_call
        else:
            # In non-strict mode, make strict_call a no-op wrapper
            globals()["strict_call"] = lambda func, *args, **kwargs: func(
                *args, **kwargs
            )
            logger.info("üîì Strict mode disabled - function calls will not be validated")

        # Configure filtering options
        # Set service code filtering
        INCLUDE_SERVICE_CODES = args.include_service_codes
        if INCLUDE_SERVICE_CODES:
            logger.info("üîì SERVICE CODE FILTERING DISABLED - Including all service codes and short codes")
        else:
            logger.info("üîí SERVICE CODE FILTERING ENABLED - Filtering out service codes and short codes (default)")
        
        # Set phone number filtering
        FILTER_NUMBERS_WITHOUT_ALIASES = args.filter_no_alias or args.exclude_no_alias
        if FILTER_NUMBERS_WITHOUT_ALIASES:
            logger.info("üîí PHONE FILTERING ENABLED - Only processing numbers with aliases/names")
        else:
            logger.info("üîì PHONE FILTERING DISABLED - Processing all phone numbers (default)")
        
        # Set non-phone number filtering
        FILTER_NON_PHONE_NUMBERS = args.filter_non_phone
        if FILTER_NON_PHONE_NUMBERS:
            logger.info("üîí NON-PHONE FILTERING ENABLED - Filtering out toll-free and non-US numbers")
        else:
            logger.info("üîì NON-PHONE FILTERING DISABLED - Processing all numbers including toll-free and international (default)")
        
        # Parse and set date filters
        if args.older_than:
            try:
                DATE_FILTER_OLDER_THAN = dateutil.parser.parse(args.older_than)
                logger.info(f"üìÖ DATE FILTER: Excluding messages older than {DATE_FILTER_OLDER_THAN}")
            except Exception as e:
                logger.error(f"Invalid --older-than date format: {args.older_than}. Use YYYY-MM-DD or YYYY-MM-DD HH:MM:SS")
                sys.exit(1)
        
        if args.newer_than:
            try:
                DATE_FILTER_NEWER_THAN = dateutil.parser.parse(args.newer_than)
                logger.info(f"üìÖ DATE FILTER: Excluding messages newer than {DATE_FILTER_NEWER_THAN}")
            except Exception as e:
                logger.error(f"Invalid --newer-than date format: {args.newer_than}. Use YYYY-MM-DD or YYYY-MM-DD HH:MM:SS")
                sys.exit(1)

        # Validate date filter logic when both filters are provided
        if DATE_FILTER_NEWER_THAN is not None and DATE_FILTER_OLDER_THAN is not None:
            # Calculate the valid date range that WILL be included
            valid_start = DATE_FILTER_OLDER_THAN
            valid_end = DATE_FILTER_NEWER_THAN
            
            # Check if the range is valid (start < end)
            if valid_start >= valid_end:
                logger.error(f"‚ùå INVALID DATE RANGE: The date range excludes ALL messages!")
                logger.error(f"   Start date (older_than): {valid_start}")
                logger.error(f"   End date (newer_than): {valid_end}")
                logger.error(f"   This creates a negative time range - no messages can fall within these bounds")
                logger.error(f"   Valid ranges require: older_than < newer_than")
                sys.exit(1)
            
            # Calculate the time span that will be included
            time_span = valid_end - valid_start
            logger.info(f"üìÖ VALID DATE RANGE: Messages between {valid_start} and {valid_end} will be processed")
            logger.info(f"üìÖ TIME SPAN: {time_span.days} days, {time_span.seconds // 3600} hours")
            
            # Warn if the range is very small
            if time_span.days < 1:
                logger.warning(f"‚ö†Ô∏è  WARNING: Very narrow date range ({time_span.days} days) - few messages may match")
            elif time_span.days < 7:
                logger.warning(f"‚ö†Ô∏è  WARNING: Narrow date range ({time_span.days} days) - limited messages may match")

        # Log test mode configuration
        if args.full_run:
            logger.info("üöÄ FULL RUN MODE ENABLED - Processing all entries")
        elif args.test_mode:
            logger.info(
                f"üß™ TEST MODE ENABLED - Processing limited to {TEST_LIMIT} entries"
            )
        else:
            logger.info(
                f"üß™ TEST MODE ENABLED BY DEFAULT - Processing limited to {TEST_LIMIT} entries for safety"
            )
            logger.info(
                "Use --full-run to process all entries or --test-limit N for custom limits"
            )

        # Validate processing directory
        if not validate_processing_directory(args.processing_dir):
            logger.error(
                "Processing directory validation failed. Please check the directory structure."
            )
            sys.exit(1)

        # Validate entire configuration if strict mode is enabled
        if args.strict_mode:
            if not validate_entire_configuration():
                logger.error("Configuration validation failed. Please check the setup.")
                sys.exit(1)

        # Run the main conversion
        main()

    except KeyboardInterrupt:
        logger.info("Conversion interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Conversion failed: {e}")
        sys.exit(1)
